{"paragraphs":[{"text":"%md\n# About\n**Lab:** Processing Streaming Data\n**Objective:** Read and process streaming data from a set of files.\n**File locations:**\n    Test data (local): /home/devuser/data/telco/activations_stream/\n    Test script: /home/devuser/bin/devsh/scripts/streamtest-file.sh\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Processing Streaming Data\n<br  /><strong>Objective:</strong> Read and process streaming data from a set of files.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Test data (local): $DEVDATA/activations_stream/\nTest script: $DEVSH/scripts/streamtest-file.sh\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820025921_-1040301881","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:23607"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820025922_2024528163","id":"20181201-044336_178705192","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23608"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591820025923_26673817","id":"20181126-093358_358613711","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23609"},{"text":"%md\n### Display Streaming Data to the Console\n\nIn this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple -- it does not transform the data, and simply outputs the data it receives \"as-is.\"\"","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Streaming Data to the Console</h3>\n<p>In this section, you will read data from a file-based stream and display the results to the\n<br  />console. The query in this section is very simple &ndash; it does not transform the data, and\n<br  />simply outputs the data it receives &ldquo;as-is.&ldquo;&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025924_-13929649","id":"20200426-194011_597040821","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23610"},{"title":"1 - Review the test data you will be using in this exercise","text":"%md\nIt contains information about device activations on Loudacre's cellular network in JSON format.\nThe data is in `/home/devuser/data/telco/activations_stream/`.\nYou will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>It contains information about device activations on Loudacre's cellular network in JSON format.\n<br  />The data is in <code>$DEVDATA/activations_stream/</code>.\n<br  />You will use these files later to simulate a stream of JSON data by running a script\n<br  />that copies the files in one at a time.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025925_-1454797975","id":"20200426-194056_111072930","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23611"},{"title":"2 - In a terminal, set up a directory to contain the data files that Spark will read","text":"%md\nSet the file permissions to allow your application to access the files.\nDo not copy any data into the directory yet.\n\n```\nmkdir -p /tmp/telco-streaming\nchmod +wr /tmp/telco-streaming\n```\n\n**Note:** The directory from which Spark will load data must exist before the you create the DataFrame based on the data.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025925_113161620","id":"20200426-194111_1243893916","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23612"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\nIf you currenty have a Spark shell running in a terminal session, exit it.\n\nStart a new Python or Scala Spark shell running on the cluster.\n\n```\nspark-shell --master local[2]\n```\n\nEnter the code in the following paragraphs into your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currently have a Spark shell running in a terminal session, exit it.\n<br  />The course exercise environment's resources are not sufficient to run a streaming\n<br  />application, so start a new Python or Scala Spark shell running locally instead of on\n<br  />the cluster.</p>\n<pre><code>$ pyspark --master local[2]\n</code></pre>\n<!-- -->\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820025926_-989936261","id":"20200426-194110_1702903882","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23613"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\nval inputDir = \"file:/tmp/telco-streaming/\"\n\nimport org.apache.spark.sql.types._\n\nval activationsSchema = StructType( List(\nStructField(\"acct_num\", IntegerType),\nStructField(\"dev_id\", StringType),\nStructField(\"phone\", StringType),\nStructField(\"model\", StringType)))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025926_-1614038469","id":"20200426-194109_271714022","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23614"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nval activationsDF = spark.readStream.\nschema(activationsSchema).\njson(\"file:/tmp/telco-streaming/\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025927_-1291933850","id":"20200426-194108_1321483966","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23615"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025927_1337071176","id":"20200426-194106_1144996716","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23616"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025928_-864567909","id":"20200426-194105_131508129","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23617"},{"title":"8 - Start a streaming query","text":"%md\nStart a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.\n\nSet the `truncate` option so that you will be able to see all the data in each record.\n\n```\nval activationsQuery = activationsDF.writeStream.\noutputMode(\"append\").\nformat(\"console\").option(\"truncate\",\"false\").\nstart\n```\n\nThe query will not display any output yet, because no files are available to read yet.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays results to the console. Use append mode to\n<br  />display the first several records in each new input stream micro-batch.</p>\n<p>Set the <code>truncate</code> option so that you will be able to see all the data in each record.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025928_2013380910","id":"20200426-194103_454623960","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23618"},{"title":"9 - Begin streaming data","text":"%md\nOpen a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.\n\n```\n/home/devuser/bin/devsh/scripts/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```\n\nThe script will display the names of the files as it copies them.\n\n**Note:** Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will ignore any files that were previously copied.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new terminal window. Run a test script to copy the test data files into the\n<br  />streaming directory at a rate of one per second.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-file.sh \\\n$DEVDATA/activations_stream/ /tmp/devsh-streaming\n</code></pre>\n<p>The script will display the names of the files as it copies them.</p>\n<p><strong>Note:</strong> Spark keeps track of files that have been previously read by each query. If you\n<br  />need to re-run the script later to test the same query, Spark will ignore any files that\n<br  />were previously copied.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025929_-476366287","id":"20200426-194101_1798043560","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23619"},{"title":"10 - Confirm that the query is displaying data","text":"%md\nReturn to your Spark shell and confirm that the query is displaying data from each batch.\nNote that the first batch processed and displayed will always be empty.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>!!! Edit needed - &ldquo;displaying console output displaying data from each batch&rdquo; !!!</h4>\n<p>Return to your Spark shell and confirm that the query is displaying console output\n<br  />displaying data from each batch.\n<br  />Note that the first batch processed and displayed will always be empty.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025929_654657707","id":"20200426-195419_757368827","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23620"},{"title":"11 - Stopping the stream","text":"%md\nWhen you are done, stop the stream by entering \n\n```\nactivationsQuery.stop\n```\n\n**Note:** You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the stream by entering activationsQuery.stop().</p>\n<p><strong>Note:</strong> You will not see a prompt while the shell is displaying output, but you can\n<br  />enter commands in the shell anyway.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025930_-323099201","id":"20200426-195419_1156531890","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23621"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025931_507002712","id":"20200426-195418_1358698244","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23622"},{"text":"%md\n### Display Aggregated Streaming Data to the Console\n\nIn this section, you will perform a simple aggregation -- counting devices by model -- and display the results to the console.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Aggregated Streaming Data to the Console</h3>\n<p>In this section, you will perform a simple aggregation &ndash; counting devices by model &ndash; and display the results to the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025931_2126389401","id":"20200426-195418_1158851408","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23623"},{"title":"13 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove data from the\n<br  />streaming directory copied in the last section.</p>\n<pre><code>$ rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820025932_341161154","id":"20200426-195417_1588990866","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23624"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created above, create a second DataFrame containing the count of each device model.\n\n```\nval activationCountDF = activationsDF.\ngroupBy(\"model\").count\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025932_615707979","id":"20200426-195417_926762270","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23625"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\nStart a streaming query that displays the model name and count for activated devices. Use `complete` mode so that the data will be aggregated across all the data in all the batches received so far for each interval, rather than just each individual batch.\n\n```\nval activationCountQuery = activationCountDF.\nwriteStream.outputMode(\"complete\").\nformat(\"console\").start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025932_1651147384","id":"20200426-195415_1492878269","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23626"},{"title":"16 - Run the streamtest-file.sh script again","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` script you ran in the previous section.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025933_1673270011","id":"20200426-195414_562992384","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23627"},{"title":"17 - Confirm that complete output mode is activated","text":"%md\nIn your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you used `complete` output mode.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025933_1901243789","id":"20200426-195413_572889012","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23628"},{"title":"18 - Stop the query and terminate the test script","text":"%md\nWhen you are done, stop the `activationCountQuery` query and terminate the test script.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025934_943194090","id":"20200426-195412_2056098062","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23629"},{"text":"%md\n### Output Data to Files\n\nIn this section, youw ill run a query that outputs to a set of files.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Output Data to Files</h3>\n<p>In this section, youw ill run a query that outputs to a set of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025935_-317883240","id":"20200426-194100_1694382703","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23630"},{"title":"19 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025935_-472271199","id":"20200426-201404_1653661450","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23631"},{"title":"20 - Create a new streaming DataFrame","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created previously, create a new streaming DataFrame with just the rows for the \"Titanic 1000\" devices and the `dev_id` and `acct_num` columns.\n\n```\nval titanic1000DF = activationsDF.\nwhere(\"model = 'Titanic 1000'\").\nselect(\"dev_id\",\"acct_num\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025935_1524057154","id":"20200426-201404_1292067866","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23632"},{"title":"21 - Start a query that saves to HDFS","text":"%md\nStart a query that saves the data in the streaming DataFrame you just created to the `/user/zeppelin/titanic1000/` directory in HDFS. Set the query to trigger every three seconds.\n\n**a.** Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.\n\n```\nspark.conf.set(\n\"spark.sql.streaming.checkpointLocation\",\n\"/tmp/streaming-checkpoint\")\n```\n\n**b.** Start the query\n\n```\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval titanic1000Query = titanic1000DF.\nwriteStream.trigger(ProcessingTime(\"3 seconds\")).\noutputMode(\"append\").format(\"csv\").\noption(\"path\",\"/user/zeppelin/titanic1000/\").\nstart\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a query that saves the data in the streaming DataFrame you just created to the <code>/user/zeppelin/titanic1000/`</code> directory in HDFS. Set the query to trigger every three seconds.</p>\n<p><strong>a.</strong> Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025936_387230671","id":"20200426-201402_528992491","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23633"},{"title":"22 - Re-run the stream test script","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` test script.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code>\n<br  />test script.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025937_691627742","id":"20200426-202548_611263496","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23634"},{"title":"23 - Confirm the target directory is receiving the files","text":"%md\nOpen a new (third) terminal window and list the contents of the HDFS target directory: `/user/zeppelin/titanic1000`. Take note of the number of files.\n\nList the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming microbatch received.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new (third) terminal window and list the contents of the HDFS target\n<br  />directory: <code>/devsh_loudacre/titanic1000</code>. Take note of the number of files.</p>\n<p>List the contents again after a few seconds and notice that the number of files is\n<br  />growing as Spark adds new data files to the directory for each incoming microbatch\n<br  />received.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820025937_-2011568342","id":"20200426-202547_2080286423","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23635"},{"title":"24 - Stopping the test","text":"%md\nWhen you are done, stop the query, terminate the test script, and exit your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025938_-163702493","id":"20200426-202547_1010479452","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23636"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820025938_-112895768","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23637"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820025939_-1673951511","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23638"},{"text":"%md\n### Display Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025939_-1899704674","id":"20200429-213425_223954687","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23639"},{"title":"1   - Review the test data you will be using in this exercise","text":"%sh\nhead /home/devuser/data/telco/activations_stream/part-00000-ad954932-b2d7-426d-a7c1-429cfc1ebb33-c000.json","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"acct_num\":128931,\"dev_id\":\"247abaa7-6c85-4f2c-a96f-c11450250506\",\"phone\":\"6190399439\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":97620,\"dev_id\":\"16512c4a-b3d3-4f92-97b7-80f9df6a487e\",\"phone\":\"7072821950\",\"model\":\"Sorrento F24L\"}\n{\"acct_num\":74940,\"dev_id\":\"361cbf31-cc6c-4233-ad4e-28d10ab09568\",\"phone\":\"5623783063\",\"model\":\"MeeToo 3.0\"}\n{\"acct_num\":125327,\"dev_id\":\"6db1bbf9-3903-47f8-b150-388dcfd8d0d3\",\"phone\":\"9168710424\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":32782,\"dev_id\":\"e97f4769-13f7-4a28-9843-827f49e767dd\",\"phone\":\"8056881980\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":94440,\"dev_id\":\"1512a38e-721b-4712-afd4-79fd94ec2b56\",\"phone\":\"9513506516\",\"model\":\"MeeToo 2.0\"}\n{\"acct_num\":4337,\"dev_id\":\"6f495bc4-d6a9-43bb-90fc-650cbd6da073\",\"phone\":\"8056442972\",\"model\":\"Titanic 2200\"}\n{\"acct_num\":28646,\"dev_id\":\"eee8add4-1740-4a39-9f23-c9ee8b4e04ac\",\"phone\":\"5039316268\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":44835,\"dev_id\":\"aed40d19-d5fc-4e11-a04a-f68094b38d73\",\"phone\":\"9289444545\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":121026,\"dev_id\":\"e67cd2c4-7358-4796-81ea-bc8a7bf91212\",\"phone\":\"5419395477\",\"model\":\"Sorrento F32L\"}\n"}]},"apps":[],"jobName":"paragraph_1591820025940_1871283826","id":"20200429-213439_92529216","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23640"},{"title":"2 - Open a terminal and set up a directory to contain the data files that Spark will read","text":"%md\n```\nmkdir -p /tmp/telco-streaming\nchmod +wr /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591820025940_-984824370","id":"20200429-213508_983964336","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23641"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\n```\nspark-shell --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025941_810521461","id":"20200429-213505_44445732","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23642"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\nval inputDir = \"file:/tmp/devsh-streaming/\"\n\n// JSON format files containing device activations\nimport org.apache.spark.sql.types._\nval activationsSchema = StructType( List(\n  StructField(\"acct_num\", IntegerType),\n  StructField(\"dev_id\", StringType),\n  StructField(\"phone\", StringType),\n  StructField(\"model\", StringType)))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025941_1293162145","id":"20200601-192211_252144301","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23643"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nval activationsDF = spark.readStream.schema(activationsSchema).json(inputDir)\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025942_2080620805","id":"20200429-213501_2000175821","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23644"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"%md\n```\nactivationsDF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025942_1329648665","id":"20200429-213459_1552306503","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23645"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"%md\n*** ??? Inspect the DataFrame's schema? ??? ***","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025943_-1671476668","id":"20200429-213458_677198705","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23646"},{"title":"8 - Start a streaming query","text":"%md\n```\nval activationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025943_1482788957","id":"20200429-213458_820871973","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23647"},{"title":"9 - Begin streaming data","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025944_2107755663","id":"20200429-213456_1259144389","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23648"},{"title":"10 - Confirm that the query is displaying data","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025944_-1196050759","id":"20200429-213456_813615217","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23649"},{"title":"11 - Stopping the stream","text":"%md\n```\nactivationsQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025945_-410102207","id":"20200429-213455_307289701","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23650"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025945_-807661154","id":"20200429-213454_100119688","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23651"},{"text":"%md\n### Display Aggregated Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025946_1824376718","id":"20200429-213454_1477948613","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23652"},{"title":"13 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025946_108365247","id":"20200429-213454_63154555","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23653"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\n```\nval activationCountDF = activationsDF.groupBy(\"model\").count()\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025947_-1663178337","id":"20200429-213453_573955489","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23654"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\n```\nval activationCountQuery = activationCountDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025947_2109568122","id":"20200429-213452_810770797","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23655"},{"title":"16 - Run the streamtest-file.sh script again","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025948_1486286274","id":"20200429-213450_1359119850","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23656"},{"title":"17 - Confirm that complete output mode is activated","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025948_-2034271656","id":"20200429-213450_472997628","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23657"},{"title":"18 - Stop the query and terminate the test script","text":"%md\n```\nactivationCountQuery.stop\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025949_1735851732","id":"20200429-213450_463682321","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23658"},{"text":"%md\n### Output Data to Files","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025949_2108010021","id":"20200429-213449_842091323","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23659"},{"title":"19 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n````","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025950_-1292024786","id":"20200429-213449_1526293893","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23660"},{"title":"20 - Create a new streaming DataFrame","text":"%md\n```\nval titanic1000DF = activationsDF.where(\"model = 'Titanic 1000'\").select(\"dev_id\",\"acct_num\")\ntitanic1000DF.printSchema\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025950_286488097","id":"20200429-213448_1192486338","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23661"},{"title":"21 - Start a query that saves to HDFS","text":"%md\n```\nspark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-checkpoint\")\n```\n\n```\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval titanic1000Query = titanic1000DF.writeStream.trigger(ProcessingTime(\"3 seconds\")).outputMode(\"append\").format(\"csv\").option(\"path\",\"/user/zeppelin/titanic1000/\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025951_-1928848358","id":"20200429-213447_402657581","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23662"},{"title":"22 - Re-run the stream test script","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025951_1076538267","id":"20200429-213445_1660540485","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23663"},{"title":"23 - Confirm the target directory is receiving the files","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025952_2003827022","id":"20200429-213445_371430499","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23664"},{"title":"24 - Stopping the test","text":"%md\n```\ntitanic1000Query.stop\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820025952_270854736","id":"20200429-213445_2052407867","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23665"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591820025953_-624941601","id":"20181126-133017_244739700","dateCreated":"2020-06-10T20:13:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23666"}],"name":"ScalaSpark/16-ProcessingStreamingData","id":"2FB1KJ58C","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}