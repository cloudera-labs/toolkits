{"paragraphs":[{"text":"%md\n# About\n**Lab:** Aggregating and Joining Streaming DataFrames\n**Objective:** Practice streaming both full and windowed aggregation transformations.\n**File locations:**\n    Data (local): /home/devuser/data/telco/activations_stream\n    \n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Aggregating and Joining Streaming DataFrames\n<br  /><strong>Objective:</strong> Practice streaming both full and windowed aggregation transformations.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Data (local): $DEVDATA/activations_stream\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820037229_1361139959","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6961"},{"text":"%md\n# Setup\n\nNone","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>None</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037230_25915760","id":"20181201-044336_178705192","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6962"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591820037231_738618599","id":"20181126-093358_358613711","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6963"},{"text":"%md\n### Find the Most Commonly Activated Devices Models\n\nIn this section, you will find the most common device models activated by counting model names and sorting by count. The count will be based on full aggregation of all the data received in the stream.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Find the Most Commonly Activated Devices Models</h3>\n<p>In this section, you will find the most common device models activated by counting\n<br  />model names and sorting by count. The count will be based on full aggregation of all the\n<br  />data received in the stream.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037232_-460035245","id":"20200426-213726_1385705129","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6964"},{"title":"1 - Terminate any Spark shells","text":"%md\nIf you currently have a Spark shell running in a terminal session, exit it.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037232_469481522","id":"20200426-213833_1554281507","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6965"},{"title":"2 - Start a new local Spark shell","text":"%md\nStart a new Python or Scala Spark shell running locally with two threads.\n\n```\nspark-shell --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a new Python or Scala Spark shell running locally with two threads.</p>\n<pre><code>$ pyspark --master local[2]\n</code></pre>\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037233_896832405","id":"20200426-213832_1251923548","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6966"},{"title":"3 - Set the default number of partitions for shuffle operations","text":"%md\nSet the default number of partitions for shuffle operations, which includes aggregation operations.\n\n```\nspark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n```\n\n> The default number of partitions is 200. This is reasonable in a typical production cluster containing many worker hosts. Your exercise environment includes only a single host. If the number of partitions in a DataFrame greatly exceeds the number of worker hosts, you will get very poor performance. For these exercises, you must lower the number of partitions to allow your aggregation operations to complete in a reasonable amount of time.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Set the default number of partitions for shuffle operations, which includes\n<br  />aggregation operations.</p>\n<blockquote><p>The default number of partitions is 200. This is reasonable in a\n<br  />typical production cluster containing many worker hosts. Your\n<br  />exercise environment includes only a single host. If the number of\n<br  />partitions in a DataFrame greatly exceeds the number of worker\n<br  />hosts, you will get very poor performance. For these exercises, you\n<br  />must lower the number of partitions to allow your aggregation\n<br  />operations to complete in a reasonable amount of time.</p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1591820037233_-592023091","id":"20200426-213832_1922966997","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6967"},{"title":"4 - Create a streaming DataFrame","text":"%md\nCreate a streaming DataFrame called `kafkaDF` to receive messages in the `activations` topic.\n\n```\nval kafkaDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", \"localhost:9092\").\noption(\"subscribe\", \"activations\").load\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a streaming DataFrame called <code>kafkaDF</code> to receive messages in the <code>activations</code> topic.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037234_-431146294","id":"20200426-213830_1680734993","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6968"},{"title":"5 - Create a new DataFrame based on kafkaDF","text":"%md\nCreate a new DataFrame based on kafkaDF with an activation column containing the activation message values, parsed from JSON format.\n\n```\nimport org.apache.spark.sql.types._\n\nval activationsSchema = StructType( List(\nStructField(\"acct_num\", IntegerType),\nStructField(\"dev_id\", StringType),\nStructField(\"phone\", StringType),\nStructField(\"model\", StringType)))\n\nval activationsDF = kafkaDF.\nselect(from_json($\"value\".cast(\"string\"),\nactivationsSchema).alias(\"activation\"))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on kafkaDF with an activation column\n<br  />containing the activation message values, parsed from JSON format.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037234_-410641972","id":"20200426-213815_593478837","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6969"},{"title":"6 - Create a streaming DataFrame","text":"%md\nCreate a streaming DataFrame that counts the rows in `activationsDF` by model name and sorts the results in descending order.\n\n```\nval sortedModelCountDF = activationsDF.\ngroupBy($\"activation\"(\"model\")).count.\nsort($\"count\".desc)\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037235_1965493820","id":"20200426-213806_799679800","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6970"},{"title":"7 - Start a query based on the sortedModelCountDF DataFrame","text":"%md\nThe query should\n- Display the complete results to the console\n- Disable truncation of the rows\n- Set a trigger interval of five seconds\n    - This is not required by the query, but will aggregate larger batches of data so that you will be able to analyze the results more easily.\n\n```\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval sortedModelCountQuery = sortedModelCountDF.\nwriteStream.outputMode(\"complete\").format(\"console\").\noption(\"truncate\",\"false\").\ntrigger(ProcessingTime(\"5 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should</p>\n<ul>\n<li>Display the complete results to the console</li>\n<li>Disable truncation of the rows</li>\n<li>Set a trigger interval of five seconds<ul>\n<li>This is not required by the query, but will aggregate larger batches of data so\n<br  />that you will be able to analyze the results more easily.</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591820037235_-1897384419","id":"20200426-213804_1784840007","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6971"},{"title":"8 - After starting the query, switch to a separate terminal session","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037236_175385796","id":"20200426-213803_1348196026","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6972"},{"title":"9 - Create the activations topic for the streaming messages","text":"%md\n(If you created it previously, you will get an exception saying the topic already exists. This is not aproblem and you can proceed with the exercises.)\n\n```\nkafka-topics --create --zookeeper localhost:2181 --partitions 2 --replication-factor 1 --topic activations\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>(If you created it previously, you will get an exception saying the topic already exists. This is not a\n<br  />problem and you can proceed with the exercises.)</p>\n<pre><code>$ kafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 --topic activations\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037236_-1512392432","id":"20200426-213803_553810788","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6973"},{"title":"10 - Test the query","text":"%md\nGenerate Kafka messages to test the query using the provided test script.\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Generate Kafka messages to test the query using the provided test script.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-kafka.sh activations \\\nlocalhost:9092 10 $DEVDATA/activations_stream\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037237_-775957680","id":"20200426-213802_883829322","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6974"},{"title":"11 - Stopping the test script","text":"%md\nLet the script run for about 20 seconds, then stop it using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let the script run for about 20 seconds, then stop it using <code>Ctrl+C</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037237_-332244080","id":"20200426-213802_1039300162","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6975"},{"title":"12 - Observe the results","text":"%md\nReturn to the Spark shell and observe the results displayed on the console. Note that the model counts continue to increase between batches. They will increase indefinitely because the query runs against all the data received in the stream.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to the Spark shell and observe the results displayed on the console. Note\n<br  />that the model counts continue to increase between batches. They will increase\n<br  />indefinitely because the query runs against all the data received in the stream.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037238_-1905959280","id":"20200426-213801_1474504317","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6976"},{"title":"13 - Stop the query","text":"%md\n```\nsortedModelCountQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037238_1155462529","id":"20200426-213801_1490570835","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6977"},{"text":"%md\n### Count Activated Models within a Sliding Window\n\nIn this section, you will count the number of activation events per model every five seconds. Each batch of results will contain the counts for events received during the prior 10 seconds.\n\nNote that aggregating results over a 10 second window would not be a common production use case. In the exercise, you will use this small window duration so that you can analyze the results more easily.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Count Activated Models within a Sliding Window</h3>\n<p>In this section, you will count the number of activation events per model every five\n<br  />seconds. Each batch of results will contain the counts for events received during the\n<br  />prior 10 seconds.</p>\n<p>Note that aggregating results over a 10 second window would not be a common\n<br  />production use case. In the exercise, you will use this small window duration so that\n<br  />you can analyze the results more easily.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037239_-186341267","id":"20200426-213759_1853192695","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6978"},{"title":"14 - Create a DataFrame","text":"%md\nCreate a DataFrame called activationsTimeDF. This DataFrame will be identical to the activationsDF you created above, except that it will contain a timestamp column with the time the activation event occurred. An event time value is required to do aggregations within a window.\n\n```\nval activationsTimeDF = kafkaDF.\nselect($\"timestamp\",from_json($\"value\".cast(\"string\"),\nactivationsSchema).alias(\"activation\"))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037239_-1639336742","id":"20200426-213759_2132726473","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6979"},{"title":"15 - Count activation events","text":"%md\nDefine a new DataFrame that counts activation events that occurred in the 10 seconds prior, updated every five seconds. In order to be able to analyze the query more easily, limit the query to models MeeToo 3.0 and 3.1.\n\n```\nval windowModelCountDF = activationsTimeDF.\nwhere($\"activation\"(\"model\").\nstartsWith(\"MeeToo 3\")).\ngroupBy(window($\"timestamp\", \"10 seconds\", \"5 seconds\"),\n$\"activation\"(\"model\")).\ncount","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a new DataFrame that counts activation events that occurred in the 10\n<br  />seconds prior, updated every five seconds. In order to be able to analyze the query\n<br  />more easily, limit the query to models MeeToo 3.0 and 3.1.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037240_-1482854745","id":"20200426-213758_1015727381","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6980"},{"title":"16 - Start a query based on the windowModelCountDF DataFrame","text":"%md\nThe query should\n\n- Display the complete results to the console\n- Disable truncation of the rows\n- Set a trigger interval of five seconds\n\n```\nval windowModelCountQuery = windowModelCountDF.\nwriteStream.outputMode(\"complete\").\nformat(\"console\").option(\"truncate\",\"false\").\ntrigger(ProcessingTime(\"5 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should</p>\n<ul>\n<li>Display the complete results to the console</li>\n<li>Disable truncation of the rows</li>\n<li>Set a trigger interval of five seconds</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591820037240_-22828338","id":"20200426-213757_1299348615","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6981"},{"title":"17 - Test the query","text":"%md\nIn a separate terminal window, generate test events using the provided test script.\n\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In a separate terminal window, generate test events using the provided test script.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-kafka.sh activations \\\nlocalhost:9092 10 $DEVDATA/activations_stream\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037241_-1539947011","id":"20200426-213756_727522879","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6982"},{"title":"18 - Stopping the test script","text":"%md\nLet the script run for at least 20 seconds, then stop it using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let the script run for at least 20 seconds, then stop it using <code>Ctrl+C</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037241_1048831328","id":"20200426-213755_729951167","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6983"},{"title":"19 - Observe the results","text":"%md\nReturn to the Spark shell and observe the results displayed on the console.\n\nIn particular, take note of the time period values in the `window` column. The value is a pair consisting of a start time and an end time.\n\n- Each period spans a 10-second time period, reflecting the window duration you specified above; for example, a period with start time `2019-06-05 08:12:40` and end time `2019-06-05 08:12:50`.\n- New windows are generated every five seconds. For example, a period with start time `2019-06-05 08:12:40` will be followed by a period starting `2019-06-05 08:12:45` -- five seconds later.\n- Each row in the results shows the number of times a particular device model occurred within a window. That means there will be up to two rows for any one window -- one with the MeeToo 3.0 count and the other with the MeeToo 3.1 count.\n- The events included in consecutive windows overlap. That is, an event that occurred at `2019-06-05 08:12:48` is included in two 10-second windows: the `2019-06-05 08:12:40` and `2019-06-05 08:12:45` windows.\n- Note that the results are unordered; results for consecutive windows may not be displayed consecutively in the output.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to the Spark shell and observe the results displayed on the console.</p>\n<p>In particular, take note of the time period values in the <code>window</code> column. The value\n<br  />is a pair consisting of a start time and an end time.</p>\n<ul>\n<li>Each period spans a 10-second time period, reflecting the window duration you\n<br  />specified above; for example, a period with start time <code>2019-06-05 08:12:40</code>\n<br  />and end time <code>2019-06-05 08:12:50</code>.</li>\n<li>New windows are generated every five seconds. For example, a period with\n<br  />start time <code>2019-06-05 08:12:40</code> will be followed by a period starting\n<br  /><code>2019-06-05 08:12:45</code> &ndash; five seconds later.</li>\n<li>Each row in the results shows the number of times a particular device model\n<br  />occurred within a window. That means there will be up to two rows for any one\n<br  />window &ndash; one with the MeeToo 3.0 count and the other with the MeeToo 3.1\n<br  />count.</li>\n<li>The events included in consecutive windows overlap. That is, an event that\n<br  />occurred at <code>2019-06-05 08:12:48</code> is included in two 10-second windows:\n<br  />the <code>2019-06-05 08:12:40</code> and <code>2019-06-05 08:12:45</code> windows.</li>\n<li>Note that the results are unordered; results for consecutive windows may not be\n<br  />displayed consecutively in the output.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591820037242_-1708315108","id":"20200426-213755_1149864474","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6984"},{"title":"20 - Stop the query","text":"%md\n```\nwindowModelCountQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037242_-2128534584","id":"20200426-213755_784337107","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6985"},{"title":"21 - Run the query using the update output mode","text":"%md\nRerun the query above, except use the `update` output mode.\n\n```\nval windowModelCountQuery2 = windowModelCountDF.\nwriteStream.outputMode(\"update\").\nformat(\"console\").option(\"truncate\",\"false\").\ntrigger(processingTime=\"15 seconds\").start()\n```\n\nNote that the trigger interval is longer than in the last query. This means that each batch will be based on more data, making the results easier to analyze.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Rerun the query above, except use the <code>update</code> output mode.</p>\n<p>Note that the trigger interval is longer than in the last query. This means that each\n<br  />batch will be based on more data, making the results easier to analyze.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037243_-276075956","id":"20200426-213753_1787869932","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6986"},{"title":"22 - Test the query","text":"%md\nGenerate test events as you did above. Let the script run for at least a minute.\n\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Generate test events as you did above. Let the script run for at least a minute.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-kafka.sh activations \\\nlocalhost:9092 10 $DEVDATA/activations_stream\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037243_843315904","id":"20200426-213752_1401656742","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6987"},{"title":"23 - Review the output","text":"%md\nReview the `update` mode output.\n\nThis time only windows with counts that changed between the previous batch and the current one are displayed. That is, if no MeeToo 3.0 devices were activated between `2019-06-05 08:12:40` and `2019-06-05 08:12:50`, then the row containing the MeeToo 3.0 count for that window will not be shown.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the <code>update</code> mode output.</p>\n<p>This time only windows with counts that changed between the previous batch and\n<br  />the current one are displayed. That is, if no MeeToo 3.0 devices were activated\n<br  />between <code>2019-06-05 08:12:40</code> and <code>2019-06-05 08:12:50</code>, then the row\n<br  />containing the MeeToo 3.0 count for that window will not be shown.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037244_-2091997331","id":"20200426-213751_400998628","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6988"},{"title":"24 - Stopping the query","text":"%md\nStop the windowModelCountQuery2 query.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037244_911753242","id":"20200426-213751_2130137713","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6989"},{"text":"%md\n### Join Streaming Activation and Static Account Data\n\nIn this section, you will join static account data in the `devsh.accounts` Hive table with streaming activation data based on the account ID. Only active accounts (those for which the `acct_close_dt` is null) will be included in the results. You will practice using both an inner and an outer join.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Join Streaming Activation and Static Account Data</h3>\n<p>In this section, you will join static account data in the <code>devsh.accounts</code> Hive table\n<br  />with streaming activation data based on the account ID. Only active accounts (those for\n<br  />which the <code>acct_close_dt</code> is null) will be included in the results. You will practice\n<br  />using both an inner and an outer join.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037245_-1365789801","id":"20200428-190617_1622516387","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6990"},{"title":"25 - Define a static DataFrame","text":"%md\nDefine a static DataFrame containing rows where `acct_close_dt` is null.\n\n```\nval accountsStaticDF =\nspark.read.table(\"telco.accounts\")\n\nval activeAccountsStaticDF = accountsStaticDf.\nwhere($\"acct_close_dt\".isNull)\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037245_769806432","id":"20200426-213749_1082815533","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6991"},{"title":"26 - Join the DataFrames","text":"%md\nJoin the static active accounts DataFrame with the `activationsDF` you created in the previous section. Include the account number, first name, last name, account close date, and device ID in the new DataFrame.\n\n```\nval joinedDF = activeAccountsStaticDF.\njoin(activationsDF, activationsDF(\"activation\")\n(\"acct_num\") ===\naccountsStaticDF(\"acct_num\")).\nselect(\"acct_num\",\"first_name\",\"last_name\",\n\"acct_close_dt\",\"activation.dev_id\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Include the account number, first name, last name, account close date, and device ID in the new DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037246_2063835005","id":"20200426-213748_357446253","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6992"},{"title":"27 - Display account data","text":"%md\nStart a query to display account data to the console. Use append output mode and set a one second trigger.\n\n```\nval joinedQuery = joinedDF.\nwriteStream.outputMode(\"append\").\nformat(\"console\").option(\"truncate\",\"false\").\ntrigger(ProcessingTime(\"1 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037246_1096714912","id":"20200426-222959_1833111122","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6993"},{"title":"28 - Test the query","text":"%md\nGenerate test events as you did in the last section. Let the script run for a few seconds.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>??? Solution ???</h3>\n<pre><code>$ $DEVSH/scripts/streamtest-kafka.sh activations \\\nlocalhost:9092 10 $DEVDATA/activations_stream\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591820037247_485039766","id":"20200426-222958_1488462629","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6994"},{"title":"29 - Review the output","text":"%md\nReview the console output in the Spark shell. Note that the joined DataFrame contains only activation data associated with active accounts. Inactive accounts were excluded from the accounts data, and you performed an inner join (the default), so activation records for accounts not in the accounts DataFrame were ignored.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Review the console output in the Spark shell. Note that the joined DataFrame\n<br  />contains only activation data associated with active accounts. Inactive accounts\n<br  />were excluded from the accounts data, and you performed an inner join (the\n<br  />default), so activation records for accounts not in the accounts DataFrame were\n<br  />ignored.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037247_-1562049670","id":"20200426-222957_1006781391","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6995"},{"title":"30 - Stop the query","text":"","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037248_84299672","id":"20200426-222957_336923731","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6996"},{"title":"31 - Perform a right outer join","text":"%md\nRepeat the steps above, but this time perform a right outer join.\n\n```\nval joinedRightDF = activeAccountsStaticDF.\njoin(activationsDF, activationsDF(\"activation\")\n(\"acct_num\") ===\naccountsStaticDF(\"acct_num\"),\"right_outer\").\nselect(\"acct_num\",\"first_name\",\"last_name\",\n\"acct_close_dt\",\"activation.dev_id\")\n\nval joinedRightQuery = joinedRightDF.writeStream.\noutputMode(\"append\").format(\"console\").\noption(\"truncate\",\"false\").\ntrigger(ProcessingTime(\"1 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Repeat the steps above, but this time perform a right outer join.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037248_-1404480214","id":"20200426-235904_525060356","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6997"},{"title":"32 - Test the query and compare the results","text":"%md\nTest the query using the message generation script again and compare the results to the previous query. Note that this time, the output includes rows in which the account data (such as account number) is null. That is because the outer join includes all data from the streaming DataFrame, even when it does not have a matching row in the static DataFrame.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Test the query using the message generation script again and compare the results\n<br  />to the previous query. Note that this time, the output includes rows in which the\n<br  />account data (such as account number) is null. That is because the outer join\n<br  />includes all data from the streaming DataFrame, even when it does not have a\n<br  />matching row in the static DataFrame.</p>\n"}]},"apps":[],"jobName":"paragraph_1591820037249_-902627157","id":"20200426-235902_243047401","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6998"},{"title":"33 - Stop the query","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037249_539396395","id":"20200427-000105_1731816017","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6999"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820037250_1636148095","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7000"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591820037250_-1574076406","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7001"},{"text":"%md\n### Find the Most Commonly Activated Devices Models","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037251_-1928466026","id":"20200429-233703_174600623","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7002"},{"title":"1 - Terminate any Spark shells","text":"","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037251_376153678","id":"20200427-010510_1061909187","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7003"},{"title":"2 - Start a new local Spark shell","text":"%md\n```\nspark-shell --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037252_225750273","id":"20200429-233750_349753213","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7004"},{"title":"3 - Set the default number of partitions for shuffle operations","text":"%md\n```\nspark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037252_-110445161","id":"20200429-233749_462865264","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7005"},{"title":"4 - Create a streaming DataFrame","text":"%md\n```\nval kafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037252_1370462355","id":"20200429-233747_32642663","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7006"},{"title":"5 - Create a new DataFrame based on kafkaDF","text":"%md\n```\nimport org.apache.spark.sql.types._\nval activationsSchema = StructType( List(\n  StructField(\"acct_num\", IntegerType),\n  StructField(\"dev_id\", StringType),\n  StructField(\"phone\", StringType),\n  StructField(\"model\", StringType))) \n\nval activationsDF = kafkaDF.select(from_json($\"value\".cast(\"string\"), activationsSchema).alias(\"activation\"))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037253_-1777772526","id":"20200429-233746_29770565","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7007"},{"title":"6 - Create a streaming DataFrame","text":"%md\n```\nval sortedModelCountDF = activationsDF.groupBy($\"activation\"(\"model\")).count.sort($\"count\".desc)\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037253_242542916","id":"20200429-233742_883449806","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7008"},{"title":"7 - Start a query based on the sortedModelCountDF DataFrame","text":"%md\n```\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\nval sortedModelCountQuery = sortedModelCountDF.writeStream.outputMode(\"complete\").format(\"console\").option(\"truncate\",\"false\").trigger(ProcessingTime(\"5 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037254_1288956586","id":"20200429-233741_579100035","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7009"},{"title":"8 - After starting the query, switch to a separate terminal session","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037255_575893986","id":"20200429-233739_428786167","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7010"},{"title":"9 - Create the activations topic for the streaming session","text":"%md\n```\nkafka-topics --create --zookeeper localhost:2181 --partitions 2 --replication-factor 1 --topic activations\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037255_1328390676","id":"20200429-233739_429679420","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7011"},{"title":"10 - Test the query","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037255_1085033541","id":"20200429-233739_39041954","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7012"},{"title":"11 - Stopping the test script","text":"%md\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037256_-1300685613","id":"20200429-233738_246730740","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7013"},{"title":"12 - Observe the results","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037256_998558039","id":"20200429-233738_1435375277","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7014"},{"title":"13 - Stop the query","text":"%md\n```\nsortedModelCountQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037257_-1686810605","id":"20200429-233738_1286834654","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7015"},{"text":"%md\n### Count Activated Models within a Sliding Window","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Count Activated Models within a Sliding Window</h3>\n"}]},"apps":[],"jobName":"paragraph_1591820037257_-419120504","id":"20200429-233735_232008717","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7016"},{"title":"14 - Create a DataFrame","text":"%md\n```\nval activationsTimeDF = kafkaDF.select($\"timestamp\",from_json($\"value\".cast(\"string\"), activationsSchema).alias(\"activation\"))\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037258_-1450717988","id":"20200429-233735_99641606","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7017"},{"title":"15 - Count activation events","text":"%md\n```\nval windowModelCountDF = activationsTimeDF.where($\"activation\"(\"model\").startsWith(\"MeeToo 3\")).groupBy(window($\"timestamp\", \"10 seconds\", \"5 seconds\"), $\"activation\"(\"model\")).count\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037258_2009274738","id":"20200429-233734_2023151881","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7018"},{"title":"16 - Start a query based on the windowModelCountDF DataFrame","text":"%md\n```\nval windowModelCountQuery = windowModelCountDF.writeStream.outputMode(\"complete\").format(\"console\").option(\"truncate\",\"false\").trigger(ProcessingTime(\"5 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037259_1863128316","id":"20200429-233733_908727488","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7019"},{"title":"17 - Test the query","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037259_-1627891729","id":"20200429-233732_1046935600","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7020"},{"title":"18 - Stopping the test script","text":"%md\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037260_-821405113","id":"20200429-233731_1252515190","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7021"},{"title":"19 - Observe the results","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037260_658916874","id":"20200429-233731_1509625270","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7022"},{"title":"20 - Stop the query","text":"%md\n```\nwindowModelCountQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037261_-1456509601","id":"20200429-233730_693625211","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7023"},{"title":"21 - Run the query using the update output mode","text":"%md\n```\nval windowModelCountQuery2 = windowModelCountDF.writeStream.outputMode(\"update\").format(\"console\").option(\"truncate\",\"false\").trigger(ProcessingTime(\"15 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037261_-984967988","id":"20200429-233729_400595096","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7024"},{"title":"22 - Test the query","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037262_891959069","id":"20200429-233728_1397137973","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7025"},{"title":"23 - Review the output","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037262_1563701779","id":"20200429-233727_2084493876","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7026"},{"title":"24 - Stopping the query","text":"%md\n```\nwindowModelCountQuery2.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037263_-1880124566","id":"20200429-235635_1832562156","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7027"},{"title":"","text":"%md\n### Join Streaming Activation and Static Account Data","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Join Streaming Activation and Static Account Data</h3>\n"}]},"apps":[],"jobName":"paragraph_1591820037263_122785716","id":"20200429-235633_245798890","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7028"},{"title":"25 - Define a static DataFrame","text":"%md\n```\nval accountsStaticDF = spark.read.table(\"telco.accounts\")\n\nval activeAccountsStaticDF = accountsStaticDF.where($\"acct_close_dt\".isNull)\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037264_-2097597766","id":"20200429-235632_1786888256","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7029"},{"title":"26 - Join the DataFrames","text":"%md\n```\nval joinedDF = activeAccountsStaticDF.join(activationsDF, activationsDF(\"activation\")(\"acct_num\") === accountsStaticDF(\"acct_num\")).select(\"acct_num\",\"first_name\",\"last_name\",\"acct_close_dt\",\"activation.dev_id\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037264_1769123468","id":"20200429-235820_1131193567","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7030"},{"title":"27 - Display account data","text":"%md\n```\nval joinedQuery = joinedDF.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\",\"false\").trigger(ProcessingTime(\"1 seconds\")).start\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037265_1993680421","id":"20200427-010747_906422379","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7031"},{"title":"28 - Test the query","text":"%md\n```\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037265_-2027498421","id":"20200427-010654_2032587977","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7032"},{"title":"29 - Review the output","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037266_-2111443016","id":"20200427-010640_1275984685","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7033"},{"title":"30 - Stop the query","text":"%md\n```\njoinedQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037266_-351848900","id":"20200427-010625_1405494394","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7034"},{"title":"31 - Perform a right outer join","text":"%md\n```\nval joinedRightDF = activeAccountsStaticDF.join(activationsDF, activationsDF(\"activation\")(\"acct_num\") === accountsStaticDF(\"acct_num\"),\"right_outer\").select(\"acct_num\",\"first_name\",\"last_name\",\"acct_close_dt\",\"activation.dev_id\")\n\nval joinedRightQuery = joinedRightDF.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\",\"false\").trigger(ProcessingTime(\"1 seconds\")).start","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037267_1143375241","id":"20200427-010555_1058572198","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7035"},{"title":"32 - Test the query and review the results","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037267_2082697950","id":"20200427-010515_1378662300","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7036"},{"title":"33 - Stop the query","text":" %pyspark\njoinedRightQuery.stop\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591820037268_325118216","id":"20200427-010430_261610496","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7037"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591820037268_1469669163","id":"20181126-133017_244739700","dateCreated":"2020-06-10T20:13:57+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7038"}],"name":"ScalaSpark/18-AggregatingAndJoiningStreamingDataFrames","id":"2FB38CY6K","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}