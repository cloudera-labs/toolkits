{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with DataFrames and Schemas\n**Objective:** Practice working with structured account data and mobile device data using DataFrames\n**File locations:**\n- Data files (HDFS): /user/zeppelin/devices.json\n- Hive Tables: telco.accounts\n\n**Successful outcome:** Create and save DataFrames using different types of data sources, and infer and define schemas\n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with DataFrames and Schemas\n<br  /><strong>Objective:</strong> Practice working with structured account data and mobile device data using DataFrames\n<br  /><strong>File locations:</strong></p>\n<ul>\n<li>Data files (HDFS): /devsh_loudacre/devices.json</li>\n<li>Hive Tables: devsh.accounts</li>\n</ul>\n<p><strong>Successful outcome:</strong> Create and save DataFrames using different types of data sources, and infer and define schemas\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819965117_-1762399594","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:186534"},{"text":"%md\n# Setup\nThis exercise requires the `telco.accounts` table, check that it exists and containts data by running the `SELECT` statement below. If the table has not already been created and loaded you can do so by completing exercise `20 - Supplemental: Creating and Loading Hive Database`.\n\n**Important:** This assume you are familiar with DataFrames, you can practice by completing exercise `04 - Exploring DataFrames Using the Apache Spark Shell`.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>This exercise requires the <code>telco.accounts</code> table, check that it exists and containts data by running the <code>SELECT</code> statement below. If the table has not already been created and loaded you can do so by completing exercise <code>20 - Supplemental: Creating and Loading Hive Database</code>.</p>\n<p><strong>Important:</strong> This assume you are familiar with DataFrames, you can practice by completing exercise 04 - Exploring DataFrames Using the Apache Spark Shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965118_1156624288","id":"20181201-044336_178705192","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186535"},{"text":"%jdbc(hive)\nSELECT * FROM telco.accounts LIMIT 5;","user":"sysadmin","dateUpdated":"2020-06-10T20:25:10+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"accounts.acct_num":"string","accounts.acct_create_dt":"string","accounts.acct_close_dt":"string","accounts.first_name":"string","accounts.last_name":"string","accounts.address":"string","accounts.city":"string","accounts.state":"string","accounts.zipcode":"string","accounts.phone_number":"string","accounts.created":"string","accounts.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"accounts.acct_num\taccounts.acct_create_dt\taccounts.acct_close_dt\taccounts.first_name\taccounts.last_name\taccounts.address\taccounts.city\taccounts.state\taccounts.zipcode\taccounts.phone_number\taccounts.created\taccounts.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1591819965118_1553944161","id":"20200602-184825_95278537","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:25:10+0000","dateFinished":"2020-06-10T20:25:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186536"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591819965119_-1828355168","id":"20181126-093358_358613711","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186537"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591819965119_1742025725","id":"20200427-233238_1449553071","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186538"},{"title":"1 - Review the accounts table in the Hive database","text":"%md\nThis exercise uses a DataFrame based on the `accounts` table in the `telco` Hive database. You can review the schema using the `jdbc` interpreter to access Hive.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This exercise uses a DataFrame based on the <code>accounts</code> table in the <code>devsh</code> Hive\n<br  />database. You can review the schema using the Beeline SQL command line to access\n<br  />Hive.</p>\n<p>In a terminal session (not one that is running the Spark shell), enter the following\n<br  />command:</p>\n<pre><code>$ beeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591819965119_2056462511","id":"20200424-193917_1691179324","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186539"},{"text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591819965120_-41570868","id":"20200521-194130_648124297","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186540"},{"title":"2 - Create a new DataFrame","text":"%md\nCreate a new DataFrame using the Hive `telco.accounts` table.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Hive <code>telco.accounts</code> table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965120_-1859442745","id":"20200424-194206_1083041321","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186541"},{"text":"%spark2\nval accountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965120_744147279","id":"20200424-194301_536696070","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186542"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965121_1713282892","id":"20200424-194700_1657078805","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186543"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965121_-1005061359","id":"20200522-201531_943828593","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186544"},{"title":"4 - Create a new DataFrame based on a condition","text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the `/user/zeppelin/accounts_zip94913` HDFS directory. You can do this in a single command, as shown below, or with multiple commands.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":60,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the accounts data where the zip\n<br  />code is 94913, and save the result to CSV files in the <code>/devsh_loudacre/accounts_zip94913</code> HDFS directory. You can do this in a single command, as shown below, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965121_-867397158","id":"20200424-194924_522335325","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186545"},{"text":"%spark2\naccountsDF.where(\"zipcode = 94913\").\nwrite.option(\"header\",\"true\").\ncsv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965122_-1356216318","id":"20200424-195112_623415846","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186546"},{"title":"5 - Confirm the operation was executed correctly","text":"%md\nUse `hdfs` to view the `/user/zeppelin/accounts_zip94913` directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>hdfs</code> in a separate terminal window to view the\n<br  /><code>/devsh_loudacre/accounts_zip94913</code> directory in HDFS and the data in\n<br  />one of the saved files. Confirm that the CSV file includes a header line, and that only\n<br  />records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965122_-1468851561","id":"20200424-195251_938833734","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186547"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965122_801024720","id":"20200522-201740_1472082247","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186548"},{"title":"6 - Create a DataFrame from a CSV","text":"%md\n*Optional:* Try creating a new DataFrame based on the CSV files you created above. Compare the schema of the original `accountsDF` and the new DataFrame. What's different? Try again, this time setting the `inferSchema` option to true and compare again.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try creating a new DataFrame based on the CSV files you created above.\n<br  />Compare the schema of the original <code>accountsDF</code> and the new DataFrame. What's\n<br  />different? Try again, this time setting the <code>inferSchema</code> option to true and\n<br  />compare again.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965123_-789570806","id":"20200424-195331_1399907429","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186549"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965123_-668296794","id":"20200522-201757_1455279920","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186550"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591819965123_1231181693","id":"20200427-234619_2069213136","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186551"},{"title":"7 - Review the data file","text":"%md\nIf you have not done so yet, review the data in the HDFS file `/user/zeppelin/devices.json`.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you have not done so yet, review the data in the HDFS file\n<br  /><code>/devsh_loudacre/devices.json</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965124_513232445","id":"20200424-195454_1861585933","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186552"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965124_-1007597027","id":"20200430-003645_261089763","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186553"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%md\nCreate a new DataFrame based on the `devices.json` file. (This command could take several seconds while it infers the schema.)","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the <code>devices.json</code> file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965124_1684468765","id":"20200424-201503_1618492364","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186554"},{"text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965125_-1786118935","id":"20200424-201433_776130777","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186555"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the `release_dt` column is of type `string`, whereas the data in the column actually represents a timestamp.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that\n<br  />Spark inferred from the JSON file. In particular, note that the <code>release_dt</code> column\n<br  />is of type <code>string</code>, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965125_-606797858","id":"20200424-201330_1854493694","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186556"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965125_-1614752560","id":"20200522-201923_1097540138","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186557"},{"title":"10 - Specify the column types of the DataFrame","text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and types.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start\n<br  />by importing the package with the definitions of necessary classes and types.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965125_-805411259","id":"20200424-201310_819844900","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186558"},{"text":"%spark2\nimport org.apache.spark.sql.types._","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965126_-1592665094","id":"20200424-201240_160502927","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186559"},{"title":"11 - Represent the column definitions with StructField objects","text":"%md\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column should be a timestamp.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, create a collection of <code>StructField</code> objects, which represent column\n<br  />definitions. The <code>release_dt</code> column should be a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965126_-430631988","id":"20200424-201153_1739325405","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186560"},{"text":"%spark2\nval devColumns = List(\nStructField(\"devnum\",LongType),\nStructField(\"make\",StringType),\nStructField(\"model\",StringType),\nStructField(\"release_dt\",TimestampType),\nStructField(\"dev_type\",StringType))","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965126_183908860","id":"20200424-201128_583503462","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186561"},{"title":"12 - Create a schema using the column definition list","text":"%md\nCreate a schema (a `StructType` object) using the column definition list.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965127_-954931644","id":"20200424-201020_2045594003","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186562"},{"text":"%spark2\nval devSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965127_-2065310152","id":"20200424-200751_1167632895","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186563"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%md\nRecreate the `devDF` DataFrame, this time using the new schema.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Recreate the <code>devDF</code> DataFrame, this time using the new schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965127_938055633","id":"20200424-200207_703556537","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186564"},{"text":"%spark2\nval devDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965128_1621837885","id":"20200424-200134_290658193","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186565"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now `timestamp`.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the\n<br  /><code>release_dt</code> column type is now <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965128_439017538","id":"20200424-200027_404284113","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186566"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965128_850927002","id":"20200522-202136_1188742309","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186567"},{"title":"15 - Save the DataFrame in Parquet format","text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. Save the Parquet data files into an HDFS directory called `/user/zeppelin/devices_parquet`.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data in Parquet format,\n<br  />which automatically embeds the schema. Save the Parquet data files into an HDFS\n<br  />directory called <code>/devsh_loudacre/devices_parquet</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965129_-912274278","id":"20200424-195942_1783798900","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186568"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965129_908060723","id":"20200522-202142_98727568","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186569"},{"title":"16 - View the schema of the Parquet file","text":"%md\n*Optional:* Use `parquet-tools` to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```\n    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in Parquet.\n\nFor more information about `parquet-tools`, run `parquet-tools --help`.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> In a separate terminal window, use <code>parquet-tools</code> to view the schema\n<br  />of the saved files. First download the HDFS directory (or an individual file), then run\n<br  />the command.</p>\n<pre><code>$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\n$ parquet-tools schema /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark\n<br  />denotes a timestamp type in Parquet.</p>\n<p>For more information about <code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965129_1800644845","id":"20200424-195815_1058807478","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186570"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965130_294218371","id":"20200522-202202_1659863288","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186571"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%md\nCreate a new DataFrame using the Parquet files you saved in `devices_parquet` and view its schema. Note that Spark is able to correctly infer the timestamp type of the `release_dt` column from Parquet's embedded schema.","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in <code>devices_parquet</code>\n<br  />and view its schema. Note that Spark is able to correctly infer the timestamp type\n<br  />of the <code>release_dt</code> column from Parquet's embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819965130_-1060335467","id":"20200424-195648_983273468","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186572"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965130_-1936018540","id":"20200522-202219_851657177","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186573"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819965131_-1002921606","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186574"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819965131_2123433134","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186575"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591819965131_-606727391","id":"20200428-225106_184286167","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186576"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-10T20:25:24+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591819965132_817224414","id":"20200428-225139_1888690996","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:25:24+0000","dateFinished":"2020-06-10T20:25:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186577"},{"title":"2 - Create a new DataFrame","text":"%spark2\nval accountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-10T20:25:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"accountsDF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1591819965132_-1124821291","id":"20200428-225540_857086049","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:25:29+0000","dateFinished":"2020-06-10T20:25:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186578"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%spark2\naccountsDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-10T20:25:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591819965132_-1206423742","id":"20200428-225533_1026098327","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:25:32+0000","dateFinished":"2020-06-10T20:25:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186579"},{"title":"4 - Create a new DataFrame based on a condition","text":"%spark2\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-10T20:30:14+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591819965133_-1842383352","id":"20200428-225650_1329297880","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:26:52+0000","dateFinished":"2020-06-10T20:26:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186580"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\nhdfs dfs -ls /user/zeppelin/accounts_zip94913\n\necho\nhdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>","user":"sysadmin","dateUpdated":"2020-06-10T20:29:33+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-06-10 20:26 /user/zeppelin/accounts_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-06-10 20:26 /user/zeppelin/accounts_zip94913/part-00000-5b3642ea-e8d9-4332-8f7a-35dedaba9667-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-06-10 20:26 /user/zeppelin/accounts_zip94913/part-00001-5b3642ea-e8d9-4332-8f7a-35dedaba9667-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-06-10 20:26 /user/zeppelin/accounts_zip94913/part-00002-5b3642ea-e8d9-4332-8f7a-35dedaba9667-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-06-10 20:26 /user/zeppelin/accounts_zip94913/part-00003-5b3642ea-e8d9-4332-8f7a-35dedaba9667-c000.csv\n\nbash: -c: line 3: syntax error near unexpected token `newline'\nbash: -c: line 3: `hdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>'\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1591819965133_1416476330","id":"20200428-225649_867963440","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:29:33+0000","dateFinished":"2020-06-10T20:29:35+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:186581"},{"title":"6 - Create a DataFrame from a CSV","text":"%spark2\nval test1DF = spark.read.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\nval test2DF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\n\ntest1DF.printSchema\ntest2DF.printSchema","user":"sysadmin","dateUpdated":"2020-06-10T20:30:40+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\nroot\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\ntest1DF: org.apache.spark.sql.DataFrame = [acct_num: string, acct_create_dt: string ... 10 more fields]\ntest2DF: org.apache.spark.sql.DataFrame = [acct_num: int, acct_create_dt: timestamp ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1591819965133_667182284","id":"20200428-225649_314698363","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:30:40+0000","dateFinished":"2020-06-10T20:30:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186582"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591819965134_-116491323","id":"20200428-225648_2130864594","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186583"},{"title":"7 - Review the data file","text":"%sh\nhdfs dfs -head /user/zeppelin/devices.json","user":"sysadmin","dateUpdated":"2020-06-10T20:30:52+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1591819965134_-1296987820","id":"20200428-225648_313296430","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:30:52+0000","dateFinished":"2020-06-10T20:30:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186584"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%spark2\nval devDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T20:31:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"devDF: org.apache.spark.sql.DataFrame = [dev_type: string, devnum: bigint ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1591819965134_190351408","id":"20200428-225648_1886643259","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:01+0000","dateFinished":"2020-06-10T20:31:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186585"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%spark2\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-10T20:31:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591819965135_1663728202","id":"20200428-225647_1646122487","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:04+0000","dateFinished":"2020-06-10T20:31:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186586"},{"title":"10 - Specify the column types of the DataFrame","text":"%spark2\nimport org.apache.spark.sql.types._","user":"sysadmin","dateUpdated":"2020-06-10T20:31:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\n"}]},"apps":[],"jobName":"paragraph_1591819965135_1095339354","id":"20200428-225647_1682536791","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:06+0000","dateFinished":"2020-06-10T20:31:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186587"},{"title":"11 - Represent the column definitions with StructField objects","text":"%spark2\nval devColumns = List(\n  StructField(\"devnum\",LongType),\n  StructField(\"make\",StringType),\n  StructField(\"model\",StringType),\n  StructField(\"release_dt\",TimestampType),\n  StructField(\"dev_type\",StringType))","user":"sysadmin","dateUpdated":"2020-06-10T20:31:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"devColumns: List[org.apache.spark.sql.types.StructField] = List(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))\n"}]},"apps":[],"jobName":"paragraph_1591819965135_-1145612729","id":"20200428-225646_614334074","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:29+0000","dateFinished":"2020-06-10T20:31:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186588"},{"title":"12 - Create a schema using the column definitions","text":"%spark2\nval devSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-10T20:31:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"devSchema: org.apache.spark.sql.types.StructType = StructType(StructField(devnum,LongType,true), StructField(make,StringType,true), StructField(model,StringType,true), StructField(release_dt,TimestampType,true), StructField(dev_type,StringType,true))\n"}]},"apps":[],"jobName":"paragraph_1591819965136_50185144","id":"20200428-225645_364347750","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:32+0000","dateFinished":"2020-06-10T20:31:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186589"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%spark2\nval devDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")\ndevDF.printSchema","user":"sysadmin","dateUpdated":"2020-06-10T20:31:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\ndevDF: org.apache.spark.sql.DataFrame = [devnum: bigint, make: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1591819965136_-422078547","id":"20200428-225644_1723966751","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:35+0000","dateFinished":"2020-06-10T20:31:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186590"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%spark2\ndevDF.show","user":"sysadmin","dateUpdated":"2020-06-10T20:31:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591819965136_273330422","id":"20200428-230856_1582268955","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:39+0000","dateFinished":"2020-06-10T20:31:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186591"},{"title":"15 - Save the DataFrame in Parquet format","text":"%spark2\ndevDF.write.parquet(\"/user/zeppelin/devices_parquet\") ","user":"sysadmin","dateUpdated":"2020-06-10T20:31:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: path hdfs://cloudair/user/zeppelin/devices_parquet already exists.;\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:664)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:664)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:664)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n  ... 49 elided\n"}]},"apps":[],"jobName":"paragraph_1591819965137_-615128276","id":"20200428-230854_1798105788","dateCreated":"2020-06-10T20:12:45+0000","dateStarted":"2020-06-10T20:31:45+0000","dateFinished":"2020-06-10T20:31:46+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:186592"},{"title":"16 - View the schema of the Parquet file","text":"%sh\nhdfs dfs -get /user/zeppelin/devices_parquet /tmp/\nparquet-tools schema /tmp/devices_parquet","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"bash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1591819965137_-64380770","id":"20200428-230853_1175884827","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186593"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%spark2\nspark.read.parquet(\"/user/zeppelin/devices_parquet\").printSchema","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819965137_-1740341379","id":"20200428-230853_1544838296","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186594"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T20:12:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591819965138_-520970686","id":"20181126-133017_244739700","dateCreated":"2020-06-10T20:12:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186595"}],"name":"ScalaSpark/05-WorkingWithDataFramesAndSchemas","id":"2F9Z6XS6B","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}