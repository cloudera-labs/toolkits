{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with Apache Kafka Streaming Messages\n**Objective:** Read streaming data from a Kafka topic and send data to a Kafka topic.\n**File locations:**\n    Test data (local): /home/devuser/data/telco/activations_stream/\n    Test script: /home/devuser/bin/devsh/scripts/streamtest-kafka.sh\n    Solution files: /home/devuser/bin/devsh/scripts/streaming-kafka-source.pyspark\n                    /home/devuser/bin/devsh/scripts/streaming-kafka-sink.pyspark\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with Apache Kafka Streaming Messages\n<br  /><strong>Objective:</strong> Read streaming data from a Kafka topic and send data to a Kafka topic.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Test data (local): $DEVDATA/activations_stream/\nTest script: $DEVSH/scripts/streamtest-kafka.sh\nSolution files: streaming-kafka-source.pyspark\n                streaming-kafka-source.scalaspark\n                streaming-kafka-sink.pyspark\n                streaming-kafka-sink.scalaspark\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815480523_735098103","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:153415"},{"text":"%md\n# Setup\nNone","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>None</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480524_976963099","id":"20181201-044336_178705192","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153416"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591815480524_-1360399924","id":"20181126-093358_358613711","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153417"},{"text":"%md\n### Read Messages from a Kafka Source\n\nIn this section, you will create a streaming Kafka DataFrame based on device activation Kafka messages in the `activations` topic. The message content is in JSON format. You will extract the required data and calculate the number of activations by model name.\n\nThe Kafka messages will be generated by a test script using the files in `/home/devuser/data/telco/activations_stream/`. You may wish to review the data before proceeding.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Read Messages from a Kafka Source</h3>\n<p>In this section, you will create a streaming Kafka DataFrame based on device activation\n<br  />Kafka messages in the <code>activations</code> topic. The message content is in JSON format.\n<br  />You will extract the required data and calculate the number of activations by model\n<br  />name.</p>\n<p>The Kafka messages will be generated by a test script using the files in\n<br  /><code>$DEVDATA/activations_stream/</code>. You may wish to review the data before proceeding.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480524_-782256094","id":"20200426-203807_1093644001","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153418"},{"title":"1 - Terminate any Spark shells","text":"%md\nIf you currently have a Spark shell running in a terminal session, exit it.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480525_-1186705651","id":"20200426-203923_163702109","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153419"},{"title":"2 - Create a topic","text":"%md\nCreate the `activations` topic for the streaming messages.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>    $ kafka-topics --create --zookeeper localhost:2181 \\\n    --partitions 2 --replication-factor 1 --topic activations\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815480525_-455372279","id":"20200426-203953_665503242","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153420"},{"text":"%sh\n$ kafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 --topic activations","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480525_1407584371","id":"20200429-225447_1230942060","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153421"},{"title":"3 - Start a new local Spark shell","text":"%md\n\n    $ pyspark --master local[2]","user":"sysadmin","dateUpdated":"2020-06-14T03:55:44+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The course exercise environment's resources are not sufficient to run a streaming\n<br  />application. Start a new Python or Scala Spark shell running locally instead of on the\n<br  />cluster.</p>\n<pre><code>$ pyspark --master local[2]\n</code></pre>\n<!-- -->\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815480525_928022397","id":"20200426-203952_996510190","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153422"},{"title":"4 - Create a streaming DataFrame","text":"%md\nCreate a streaming DataFrame called `kafkaDF` using the following settings:\n\n- Format: `kafka`\n- `kafka.bootstrap.servers` option: `localhost:9092`\n- `subscribe` option: `activations` (topic name)","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a streaming DataFrame called <code>kafkaDF</code> using the following settings:</p>\n<ul>\n<li>Format: <code>kafka</code></li>\n<li><code>kafka.bootstrap.servers</code> option: <code>localhost:9092</code></li>\n<li><code>subscribe</code> option: <code>activations</code> (topic name)</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591815480526_478329456","id":"20200426-203952_1826451491","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153423"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480526_-849518559","id":"20200426-203951_1151868381","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153424"},{"title":"5 - Display the schema of the DataFrame","text":"%md\nNote that the `value` column contains binary values.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that the <code>value</code> column contains binary values.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480526_-1493530661","id":"20200426-203950_166689905","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153425"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480526_1161725719","id":"20200426-203949_1312335789","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153426"},{"title":"6 - Create a new DataFrame","text":"%md\nCreate a new DataFrame called `stringValueDF` containing only the value column from `kafkaDF`. The new `value` should be string type.\n\nHint: use the `cast(\"string\")` function on the value column reference.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>stringValueDF</code> containing only the value\n<br  />column from <code>kafkaDF</code>. The new <code>value</code> should be string type.</p>\n<p>Hint: use the <code>cast(\"string\")</code> function on the value column reference.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480527_64402644","id":"20200426-203948_477490279","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153427"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480527_-1310613336","id":"20200426-203948_1280825326","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153428"},{"title":"7 - Confirm the column type was changed","text":"%md\nDisplay the schema to confirm that the column type was changed correctly.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480527_1043432119","id":"20200426-203947_1010888942","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153429"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480528_1194508180","id":"20200426-203947_900544353","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153430"},{"title":"8 - Create a new DataFrame","text":"%md\nCreate a new DataFrame called `activationsDF` with a single column called `activation`. The column should contain sub-columns for the four values in the activation JSON records: `acct_num`, `dev_id`, `phone`, and `model`.\n\n**a.** Create a schema to map the JSON values to columns.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame called <code>activationsDF</code> with a single column called\n<br  /><code>activation</code>. The column should contain sub-columns for the four values in the\n<br  />activation JSON records: <code>acct_num</code>, <code>dev_id</code>, <code>phone</code>, and <code>model</code>.</p>\n<p><strong>a.</strong> Create a schema to map the JSON values to columns.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480528_1505896318","id":"20200426-203946_1670977750","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153431"},{"text":"%pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480528_883530043","id":"20200426-203945_2047118956","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153432"},{"title":"8 b - Parse the value column","text":"%md\nUse the `from_json` function to parse the values in the `value` column to the schema above.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>b.</strong> Use the <code>from_json</code> function to parse teh values in the <code>value</code> column to the schema above.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480528_1876949517","id":"20200426-203944_240993201","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153433"},{"text":"%pyspark\nfrom pyspark.sql.functions import *\n\nactivationsDF = stringValueDF. \\\nselect(from_json(stringValueDF.value,\nactivationsSchema).\nalias(\"activation\"))","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480529_-809288894","id":"20200426-203944_1147612167","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153434"},{"title":"9 - Confirm the new DataFrame's schema is correct","text":"%md\nView the new DataFrames's schema to confirm that it is correct.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480529_948278735","id":"20200426-203943_532597384","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153435"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480529_1924602668","id":"20200426-203943_1223752146","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153436"},{"title":"10 - Create and start a streaming query","text":"%md\nCreate and start a streaming query based on `activationsDF`. Display the output to the console using output mode `append`. The elements in the DataFrame are longer than the default output, so set the `truncate` option to `false`.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create and start a streaming query based on <code>activationsDF</code>. Display the output\n<br  />to the console using output mode <code>append</code>. The elements in the DataFrame are\n<br  />longer than the default output, so set the <code>truncate</code> option to <code>false</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480529_1678684450","id":"20200426-203928_558847417","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153437"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480529_1862843953","id":"20200426-203928_240614274","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153438"},{"title":"11 - Test the query output","text":"%md\nTest the query output using the provided test script. Run the script in a separate terminal window (not running the Spark shell).\n\n```\n    /home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations \\\n    localhost:9092 10 /user/zeppelin/activations_stream\n```\n\nThis command produces Kafka messages based on the JSON lines in `/user/zeppelin/activations_stream/`. It sends the messages to the Kafka broker running on `localhost` on the `activations` topic, 10 messages per second.\n\nWhen prompted, confirm that the script settings are correct.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Test the query output using the provided test script. Run the script in a separate\n<br  />terminal window (not running the Spark shell).</p>\n<pre><code>$ $DEVSH/scripts/streamtest-kafka.sh activations \\\nlocalhost:9092 10 $DEVDATA/activations_stream\n</code></pre>\n<p>This command produces Kafka messages based on the JSON lines in <code>$DEVDATA/activations_stream/</code>.\n<br  />It sends the messages to the Kafka broker running on <code>localhost</code> on the <code>activations</code> topic, 10 messages per second.</p>\n<p>When prompted, confirm that the script settings are correct.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480530_1571394734","id":"20200426-203927_353325578","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153439"},{"title":"12 - Run the test script for several seconds","text":"%md\nThe script will display each line from the file as it sends the corresponding Kafka message. Let the script run for several seconds, then stop it using Ctrl+C.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The script will display each line from the file as it sends the corresponding Kafka\n<br  />message. Let the script run for several seconds, then stop it using Ctrl+C.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480530_-259754561","id":"20200426-210541_2098834615","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153440"},{"title":"13 - Review the output displayed by the streaming query","text":"%md\nReturn to the Spark shell and review the output displayed by the streaming query. Note that each element in the result DataFrame is an array of strings -- that is, the value of the `activation` column.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to the Spark shell and review the output displayed by the streaming query.\n<br  />Note that each element in the result DataFrame is an array of strings &ndash; that is, the\n<br  />value of the <code>activation</code> column.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480530_-1649502573","id":"20200426-210541_1596990","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153441"},{"title":"14 - Stopping the query","text":"%md\nWhen you are done testing, stop the query by calling the `stop` function on the `StreamingQuery` object.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480530_-1581063456","id":"20200426-210541_1127240569","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153442"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480531_-999186615","id":"20200429-231148_1561617789","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153443"},{"title":"15 - Optional: Perform additional transformations","text":"%md\n*Optional:* Try performing additional transformations on the device activation data in the Kafka message stream. For instance, try counting activations by model.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try performing additional transformations on the device activation data\n<br  />in the Kafka message stream. For instance, try counting activations by model.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480531_-2012182061","id":"20200426-210541_1555786742","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153444"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480531_-1664592905","id":"20200429-231250_1028042742","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153445"},{"text":"%md\n### Send a Stream of Messages to a Kafka Sink\n\nIn this section, you will send Kafka messages containing information about device activations to the `activations-out` topic.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Send a Stream of Messages to a Kafka Sink</h3>\n<p>In this section, you will send Kafka messages containing information about device\n<br  />activations to the <code>activations-out</code> topic.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480532_-1658289393","id":"20200426-210541_1938942316","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153446"},{"title":"16 - Exit the Spark shell you started in the section above","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480532_-1029172393","id":"20200426-210541_34201750","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153447"},{"title":"17 - Create the topic for the output messages","text":"","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>$ kafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815480532_-241036405","id":"20200426-210540_728715783","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153448"},{"text":"%sh\nkafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480532_1888571044","id":"20200429-231606_367276239","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153449"},{"title":"18 - Start a Kafka consumer","text":"%md\nStart a Kafka consumer running on the command line to test the output you will produce below.\n\nLeave the consumer application running. You will return later to confirm your Kafka message output.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a Kafka consumer running on the command line to test the output you will produce below.</p>\n<pre><code>$ kafka-console-consumer --topic activations-out \\\n--bootstrap-server localhost:9092\n</code></pre>\n<p>Leave the consumer application running. You will return later to confirm your Kafka message output.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480532_2139149938","id":"20200426-210540_732200048","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153450"},{"text":"%sh\n$ kafka-console-consumer --topic activations-out \\\n--bootstrap-server localhost:9092","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480533_-302810075","id":"20200429-231708_994413478","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153451"},{"title":"19 - In a separate terminal window, restart the shell","text":"%md\n```\n$ pyspark --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>$ pyspark --master local[2]\n</code></pre>\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815480533_-716725570","id":"20200426-210540_1848021454","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153452"},{"title":"20 - Create the schema for the streaming DataFrame","text":"%md\nThe test data files for this section are the same ones you use in the previous section, containing JSON records with device activation data. You will simulate incoming streaming data by reading the existing JSON files in the data directory, one file per micro-batch trigger.\n\nCreate a schema to map the JSON values to columns.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The test data files for this section are the same ones you use in the previous section,\n<br  />containing JSON records with device activation data. You will simulate incoming\n<br  />streaming data by reading the existing JSON files in the data directory, one file per\n<br  />micro-batch trigger.</p>\n<p><strong>a.</strong> Create a schema to map the JSON values to columns.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480533_-8531173","id":"20200426-210540_841145454","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153453"},{"text":"%pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480534_76853684","id":"20200426-210540_887017584","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153454"},{"title":"21 - Create a simulated streaming DataFrame","text":"Create a simulated streaming DataFrame based on the JSON activation data, using the schema you created above.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480534_-1822315055","id":"20200426-210539_1086808636","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153455"},{"text":"%pyspark\nactivationsDF = spark.readStream. \\\nschema(activationsSchema). \\\noption(\"maxFilesPerTrigger\",1). \\\njson(\"file:///home/devuser/data/activations_stream/\")","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480534_1057968928","id":"20200426-210539_183516831","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153456"},{"title":"22 - Transform the input streaming DataFrame","text":"%md\nTransform the input streaming DataFrame into a new DataFrame with the correct schema and content:\n\n- A `key` column for which all rows contain an empty string (\"\")\n    - Hint: use the Spark SQL `lit(`*literal*`)` function to specify a column with a literal value\n- A `value` string column containing the account number and device ID separated by a comma.\n    - Hint: use the Spark SQL `concat_ws(`*separator*`,` *columns...*`)` function create the comma-delimited string","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Transform the input streaming DataFrame into a new DataFrame with the correct\n<br  />schema and content:</p>\n<ul>\n<li>A <code>key</code> column for which all rows contain an empty string (&ldquo;&ldquo;)<ul>\n<li>Hint: use the Spark SQL <code>lit(</code><em>literal</em><code>)</code> function to specify a column with a\n<br  />literal value</li>\n</ul>\n</li>\n<li>A <code>value</code> string column containing the account number and device ID separated\n<br  />by a comma.<ul>\n<li>Hint: use the Spark SQL <code>concat_ws(</code><em>separator</em><code>,</code> <em>columns&hellip;</em><code>)</code> function\n<br  />create the comma-delimited string</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1591815480534_1322076438","id":"20200426-210538_261794404","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153457"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480534_1835548158","id":"20200426-210537_658320792","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153458"},{"title":"23 - Create and start a query to produces Kafka messages","text":"%md\nThe query should\n- Use format `kafka`\n- Set the `checkpointLocation` option to `/tmp/kafka-checkpoint`\n- Set the `kafka.bootstrap.servers` to `localhost:9092`\n- Set the `topic` option to `activations-out`\n\n**Note:** If you need to run the query above multiple times, be sure to remove thecheckpoint directory above before rerunning it.\n```\n    $ rm -rf /tmp/kafka-checkpoint\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should</p>\n<ul>\n<li>Use format <code>kafka</code></li>\n<li>Set the <code>checkpointLocation</code> option to <code>/tmp/kafka-checkpoint</code></li>\n<li>Set the <code>kafka.bootstrap.servers</code> to <code>localhost:9092</code></li>\n<li>Set the <code>topic</code> option to <code>activations-out</code></li>\n</ul>\n<p><strong>Note:</strong> If you need to run the query above multiple times, be sure to remove the\n<br  />checkpoint directory above before rerunning it.</p>\n<pre><code>$ rm -rf /tmp/kafka-checkpoint\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815480535_-34523509","id":"20200426-210537_37861484","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153459"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480535_-666535981","id":"20200426-210536_1372487501","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153460"},{"title":"24 - Confirm that the messages are being received correctly","text":"%md\nThe query should start generating Kafka messages immediately. Return to the window where you started the console consumer to confirm that the messages are being received and have the correct format.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The query should start generating Kafka messages immediately. Return to the\n<br  />window where you started the console consumer to confirm that the messages are\n<br  />being received and have the correct format.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480535_-233174533","id":"20200426-210535_991493700","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153461"},{"title":"25 - Stopping the query","text":"%md\nWhen you are done testing, exit the consumer application using `Ctrl+C`. Then stop the query using the `stop` function and exit the Spark shell.","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done testing, exit the consumer application using <code>Ctrl+C</code>. Then stop the query using the <code>stop</code> function and exit the Spark shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815480536_641777429","id":"20200426-203926_711291629","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153462"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480536_-829936925","id":"20200426-213337_581099564","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153463"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815480536_462707362","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153464"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815480536_1261541384","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153465"},{"text":"%md\n### Read Messages from a Kafka Source","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480537_1016558100","id":"20200429-224937_1880357458","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153466"},{"title":"1 - Create a topic","text":"%sh\nkafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 --topic activations\n# /home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"bash: kafka-topics: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1591815480537_-654894245","id":"20200429-225016_359372993","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153467"},{"title":"3 - Start a new local Spark shell","text":"%sh\n# pyspark --master local[2]","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480537_702376824","id":"20200429-225015_615463066","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153468"},{"title":"4 - Create a streaming DataFrame","text":"%pyspark\nkafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"activations\").load()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480537_-81088538","id":"20200429-225013_613497445","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153469"},{"title":"5 - Display the schema of the DataFrame","text":"%pyspark\nkafkaDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480538_-1822183981","id":"20200429-225010_1354703023","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153470"},{"title":"6 - Create a new DataFrame","text":"%pyspark\nstringValueDF = kafkaDF.select(kafkaDF.value.cast(\"string\"))","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480538_-1977062571","id":"20200429-225009_10700973","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153471"},{"title":"7 - Confirm the column type was changed","text":"%pyspark\n#*** !!! This line was missing from both solution files !!! ***\nstringValueDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480538_-1953302134","id":"20200429-225008_660411338","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153472"},{"title":"8 - Create a new DataFrame","text":"%pyspark\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480538_1913828117","id":"20200429-225007_367971138","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153473"},{"title":"8 b - Parse the value column","text":"%pyspark\n# from pyspark.sql.functions import *\nactivationsDF = stringValueDF.select(from_json(stringValueDF.value, activationsSchema).alias(\"activation\"))","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480539_-1340714204","id":"20200429-225006_1594579468","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153474"},{"title":"9 - Confirm the new DataFrame's schema is correct","text":"%pyspark\n#*** !!! parentheses missing in Scala code !!! ***\nactivationsDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480539_314015805","id":"20200429-225005_1171313932","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153475"},{"title":"10 - Create and start a streaming query","text":"%pyspark\nactivationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480539_-1461952054","id":"20200429-225003_190569295","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153476"},{"title":"11 - Test the query outptut","text":"%sh\n/home/devuser/bin/devsh/scripts/streamtest-kafka.sh activations localhost:9092 10 /home/devuser/data/telco/activations_stream","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480539_-108459691","id":"20200429-225002_38570415","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153477"},{"title":"12 - Run the test script for several seconds","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480540_1647984081","id":"20200429-225002_1467321867","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153478"},{"title":"13 - Review the output displayed by the streaming query","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480540_-802605763","id":"20200429-225001_1997974606","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153479"},{"title":"14 - Stopping the query","text":"%pyspark\n#*** !!! parentheses missing in Scala code !!! ***\nactivationsQuery.stop()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480540_1938552222","id":"20200429-225001_1085492957","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153480"},{"title":"15 - Optional: Perform additional transformations","text":"%pyspark\n# Count activations by model\ncountByModelDF = activationsDF.groupBy(\"activation.model\").count()\n# count aggregation only supported with complete output mode\ncountByModelQuery = countByModelDF.writeStream.outputMode(\"complete\").format(\"console\").start()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480540_2014879096","id":"20200429-225000_1716593565","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153481"},{"title":"Stop the query","text":"%pyspark\n#*** !!! parentheses missing in Scala code !!! ***\ncountByModelQuery.stop()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480541_1151138301","id":"20200429-224958_1282131172","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153482"},{"text":"%md\n### Send a Stream of Messages to a Kafka Sink","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Send a Stream of Messages to a Kafka Sink</h3>\n"}]},"apps":[],"jobName":"paragraph_1591815480541_-1319438414","id":"20200429-224957_1722144467","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153483"},{"title":"16 - Exit the Spark shell you started in the section above","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480541_-1534681475","id":"20200429-224957_429546116","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153484"},{"title":"17 - Create the topic for the output messages","text":"%sh\nkafka-topics --create --zookeeper localhost:2181 \\\n--partitions 2 --replication-factor 1 \\\n--topic activations-out","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480541_731532888","id":"20200429-224956_1476701879","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153485"},{"title":"18 - Start a Kafka consumer","text":"%sh\nkafka-console-consumer --topic activations-out \\\n--bootstrap-server localhost:9092","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480541_503881104","id":"20200429-224954_879965496","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153486"},{"title":"19 - In a separate terminal window, restart the shell","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480542_-1365440303","id":"20200429-231546_372543314","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153487"},{"title":"20 - Create the schema for the streaming DataFrame","text":"%pyspark\n#from pyspark.sql.types import *\n\nactivationsSchema = StructType([\n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480542_-863645600","id":"20200429-231545_221624480","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153488"},{"title":"21 - Create a simulated streaming DataFrame","text":"%pyspark\nactivationsDF = spark.readStream.schema(activationsSchema).option(\"maxFilesPerTrigger\",1).json(\"file:///home/devuser/data/activations_stream/\")","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480542_2139465369","id":"20200429-231543_1403219086","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153489"},{"title":"22 - Transform the input streaming DataFrame","text":"%pyspark\nfrom pyspark.sql.functions import *\n\nactivationsValueDF = activationsDF.select(lit(\"\").alias(\"key\"),concat_ws(\",\",activationsDF.acct_num,activationsDF.dev_id).alias(\"value\"))\nactivationsValueDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480542_-1731836618","id":"20200429-231541_612551067","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153490"},{"title":"23 - Create and start a query that produces Kafka messages","text":"%pyspark\n# Note: if you rerun query be sure to clear out checkpoint location directory\nactivationsKafkaQuery = activationsValueDF.writeStream.format(\"kafka\").option(\"checkpointLocation\",\"/tmp/kafka-checkpoint\").option(\"topic\",\"activations-out\").option(\"kafka.bootstrap.servers\",\"localhost:9092\").start()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480543_-483054371","id":"20200429-231945_1868607638","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153491"},{"title":"24 - Confirm that the messages are being received correctly","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480543_-1648416223","id":"20200429-232115_809190101","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153492"},{"title":"25 - Stopping the query","text":"%pyspark\nactivationsKafkaQuery.stop()","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815480543_1168147060","id":"20200429-232112_1285256751","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153493"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T18:58:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591815480544_-1761515355","id":"20181126-133017_244739700","dateCreated":"2020-06-10T18:58:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153494"}],"name":"PYSPARK/17-WorkingWithApacheKafkaStreamingMessages","id":"2FBJKE33E","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}