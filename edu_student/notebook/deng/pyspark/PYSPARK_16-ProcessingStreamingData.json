{"paragraphs":[{"text":"ac%md\n# About\n**Lab:** Processing Streaming Data\n**Objective:** Read and process streaming data from a set of files.\n**File locations:**\n    Test data (local): /home/devuser/data/telco/activations_stream/\n    Test script: /home/devuser/bin/devsh/scripts/streamtest-file.sh\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-13T21:52:37+0000","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Processing Streaming Data\n<br  /><strong>Objective:</strong> Read and process streaming data from a set of files.\n<br  /><strong>File locations:</strong></p>\n<pre><code>Test data (local): $DEVDATA/activations_stream/\nTest script: $DEVSH/scripts/streamtest-file.sh\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815474587_588011010","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:147987"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815474588_-2008340202","id":"20181201-044336_178705192","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147988"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591815474588_-1169002301","id":"20181126-093358_358613711","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147989"},{"text":"%md\n### Display Streaming Data to the Console\n\nIn this section, you will read data from a file-based stream and display the results to the console. The query in this section is very simple -- it does not transform the data, and simply outputs the data it receives \"as-is.\"\"","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Streaming Data to the Console</h3>\n<p>In this section, you will read data from a file-based stream and display the results to the\n<br  />console. The query in this section is very simple &ndash; it does not transform the data, and\n<br  />simply outputs the data it receives &ldquo;as-is.&ldquo;&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474589_-671056013","id":"20200426-194011_597040821","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147990"},{"title":"1 - Review the test data you will be using in this exercise","text":"%md\nIt contains information about device activations on Loudacre's cellular network in JSON format.\nThe data is in `/home/devuser/data/telco/activations_stream/`.\nYou will use these files later to simulate a stream of JSON data by running a script that copies the files in one at a time.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>It contains information about device activations on Loudacre's cellular network in JSON format.\n<br  />The data is in <code>$DEVDATA/activations_stream/</code>.\n<br  />You will use these files later to simulate a stream of JSON data by running a script\n<br  />that copies the files in one at a time.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474589_116356802","id":"20200426-194056_111072930","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147991"},{"title":"2 - In a terminal, set up a directory to contain the data files that Spark will read","text":"%md\nSet the file permissions to allow your application to access the files.\nDo not copy any data into the directory yet.\n\n```\nmkdir -p /tmp/telco-streaming\nchmod go+wr /tmp/telco-streaming\n```\n\n**Note:** The directory from which Spark will load data must exist before the you create the DataFrame based on the data.","user":"sysadmin","dateUpdated":"2020-06-10T19:50:43+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474589_1412594640","id":"20200426-194111_1243893916","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147992"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\nIf you currenty have a Spark shell running in a terminal session, exit it.\n\nStart a new Python or Scala Spark shell running on the cluster.\n\n```\npyspark --master local[2]\n```\n\nEnter the code in the following paragraphs into your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you currently have a Spark shell running in a terminal session, exit it.\n<br  />The course exercise environment's resources are not sufficient to run a streaming\n<br  />application, so start a new Python or Scala Spark shell running locally instead of on\n<br  />the cluster.</p>\n<pre><code>$ pyspark --master local[2]\n</code></pre>\n<!-- -->\n<pre><code>$ spark-shell --master local[2]\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815474589_2017648865","id":"20200426-194110_1702903882","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147993"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\ninputDir = \"file:/tmp/telco-streaming/\"\n\nfrom pyspark.sql.types import *\n\nactivationsSchema = StructType([\nStructField(\"acct_num\", IntegerType()),\nStructField(\"dev_id\", StringType()),\nStructField(\"phone\", StringType()),\nStructField(\"model\", StringType())])\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474590_569671993","id":"20200426-194109_271714022","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147994"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream. \\\nschema(activationsSchema). \\\njson(\"file:/tmp/telco-streaming/\")\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474590_116557925","id":"20200426-194108_1321483966","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147995"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474590_-1789503432","id":"20200426-194106_1144996716","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147996"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474590_-846758012","id":"20200426-194105_131508129","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147997"},{"title":"8 - Start a streaming query","text":"%md\nStart a streaming query that displays results to the console. Use append mode to display the first several records in each new input stream micro-batch.\n\nSet the `truncate` option so that you will be able to see all the data in each record.\n\n```\nactivationsQuery = activationsDF.writeStream. \\\noutputMode(\"append\"). \\\nformat(\"console\").option(\"truncate\",\"false\"). \\\nstart()\n```\n\nThe query will not display any output yet, because no files are available to read yet.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a streaming query that displays results to the console. Use append mode to\n<br  />display the first several records in each new input stream micro-batch.</p>\n<p>Set the <code>truncate</code> option so that you will be able to see all the data in each record.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474591_-1571849902","id":"20200426-194103_454623960","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147998"},{"title":"9 - Begin streaming data","text":"%md\nOpen a new terminal window. Run the test script to copy the test data files into the streaming directory at a rate of one per second.\n\n```\n/home/devuser/bin/spark/streaming/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```\n\nThe script will display the names of the files as it copies them.\n\n**Note:** Spark keeps track of files that have been previously read by each query. If you need to re-run the script later to test the same query, Spark will ignore any files that were previously copied.","user":"sysadmin","dateUpdated":"2020-06-14T03:42:24+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new terminal window. Run a test script to copy the test data files into the\n<br  />streaming directory at a rate of one per second.</p>\n<pre><code>$ $DEVSH/scripts/streamtest-file.sh \\\n$DEVDATA/activations_stream/ /tmp/devsh-streaming\n</code></pre>\n<p>The script will display the names of the files as it copies them.</p>\n<p><strong>Note:</strong> Spark keeps track of files that have been previously read by each query. If you\n<br  />need to re-run the script later to test the same query, Spark will ignore any files that\n<br  />were previously copied.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474591_-315351870","id":"20200426-194101_1798043560","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147999"},{"title":"10 - Confirm that the query is displaying data","text":"%md\nReturn to your Spark shell and confirm that the query is displaying data from each batch.\nNote that the first batch processed and displayed will always be empty.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>!!! Edit needed - &ldquo;displaying console output displaying data from each batch&rdquo; !!!</h4>\n<p>Return to your Spark shell and confirm that the query is displaying console output\n<br  />displaying data from each batch.\n<br  />Note that the first batch processed and displayed will always be empty.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474591_-1366050304","id":"20200426-195419_757368827","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148000"},{"title":"11 - Stopping the stream","text":"%md\nWhen you are done, stop the stream by entering \n\n```\nactivationsQuery.stop()\n```\n\n**Note:** You will not see a prompt while the shell is displaying output, but you can enter commands in the shell anyway.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>When you are done, stop the stream by entering activationsQuery.stop().</p>\n<p><strong>Note:</strong> You will not see a prompt while the shell is displaying output, but you can\n<br  />enter commands in the shell anyway.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474591_-1990980633","id":"20200426-195419_1156531890","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148001"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474592_-218700722","id":"20200426-195418_1358698244","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148002"},{"text":"%md\n### Display Aggregated Streaming Data to the Console\n\nIn this section, you will perform a simple aggregation -- counting devices by model -- and display the results to the console.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display Aggregated Streaming Data to the Console</h3>\n<p>In this section, you will perform a simple aggregation &ndash; counting devices by model &ndash; and display the results to the console.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474592_400316263","id":"20200426-195418_1158851408","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148003"},{"title":"13 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data that was copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In your test script terminal window (not the Spark shell), remove data from the\n<br  />streaming directory copied in the last section.</p>\n<pre><code>$ rm -rf /tmp/devsh-streaming/*\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591815474592_951643000","id":"20200426-195417_1588990866","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148004"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created above, create a second DataFrame containing the count of each device model.\n\n```\nactivationCountDF = activationsDF. \\\ngroupBy(\"model\").count()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474592_879400196","id":"20200426-195417_926762270","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148005"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\nStart a streaming query that displays the model name and count for activated devices. Use `complete` mode so that the data will be aggregated across all the data in all the batches received so far for each interval, rather than just each individual batch.\n\n```\nactivationCountQuery = activationCountDF. \\\nwriteStream.outputMode(\"complete\"). \\\nformat(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474593_-1088360066","id":"20200426-195415_1492878269","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148006"},{"title":"16 - Run the streamtest-file.sh script again","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` script you ran in the previous section.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474593_25016264","id":"20200426-195414_562992384","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148007"},{"title":"17 - Confirm that complete output mode is activated","text":"%md\nIn your Spark shell, make sure that the models and their counts are being displayed. Confirm that the counts are going up in each batch because you used `complete` output mode.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474593_-2145989465","id":"20200426-195413_572889012","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148008"},{"title":"18 - Stop the query and terminate the test script","text":"%md\nWhen you are done, stop the `activationCountQuery` query and terminate the test script.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474593_1622475697","id":"20200426-195412_2056098062","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148009"},{"text":"%md\n### Output Data to Files\n\nIn this section, youw ill run a query that outputs to a set of files.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Output Data to Files</h3>\n<p>In this section, youw ill run a query that outputs to a set of files.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474593_-1734413171","id":"20200426-194100_1694382703","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148010"},{"title":"19 - Clean the streaming directory","text":"%md\nIn your test script terminal window (not the Spark shell), remove the data copied into the streaming directory in the last section.\n\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474594_728223957","id":"20200426-201404_1653661450","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148011"},{"title":"20 - Create a new streaming DataFrame","text":"%md\nGo to your Spark shell. Starting with the `activationsDF` DataFrame you created previously, create a new streaming DataFrame with just the rows for the \"Titanic 1000\" devices and the `dev_id` and `acct_num` columns.\n\n```\ntitanic1000DF = activationsDF. \\\nwhere(\"model = 'Titanic 1000'\"). \\\nselect(\"dev_id\",\"acct_num\")\n\ntitanic1000DF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474594_2010049666","id":"20200426-201404_1292067866","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148012"},{"title":"21 - Start a query that saves to HDFS","text":"%md\nStart a query that saves the data in the streaming DataFrame you just created to the `/user/zeppelin/titanic1000/` directory in HDFS. Set the query to trigger every three seconds.\n\n**a.** Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.\n\n```\nspark.conf.set(\n\"spark.sql.streaming.checkpointLocation\",\n\"/tmp/streaming-checkpoint\")\n```\n\n**b.** Start the query\n\n```\ntitanic1000Query = titanic1000DF. \\\nwriteStream.trigger(processingTime=\"3 seconds\"). \\\noutputMode(\"append\").format(\"csv\"). \\\noption(\"path\",\"/user/zeppelin/titanic1000/\"). \\\nstart()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Start a query that saves the data in the streaming DataFrame you just created to the <code>/user/zeppelin/titanic1000/`</code> directory in HDFS. Set the query to trigger every three seconds.</p>\n<p><strong>a.</strong> Checkpointing must be enabled to ensure fault tolerance when saving files. Set the checkpoint HDFS directory for the Spark session.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474594_-1315335660","id":"20200426-201402_528992491","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148013"},{"title":"22 - Re-run the stream test script","text":"%md\nReturn to your second terminal window and re-run the `streamtest-file.sh` test script.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Return to your second terminal window and re-run the <code>streamtest-file.sh</code>\n<br  />test script.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474594_1817495980","id":"20200426-202548_611263496","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148014"},{"title":"23 - Confirm the target directory is receiving the files","text":"%md\nOpen a new (third) terminal window and list the contents of the HDFS target directory: `/user/zeppelin/titanic1000`. Take note of the number of files.\n\nList the contents again after a few seconds and notice that the number of files is growing as Spark adds new data files to the directory for each incoming microbatch received.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Open a new (third) terminal window and list the contents of the HDFS target\n<br  />directory: <code>/devsh_loudacre/titanic1000</code>. Take note of the number of files.</p>\n<p>List the contents again after a few seconds and notice that the number of files is\n<br  />growing as Spark adds new data files to the directory for each incoming microbatch\n<br  />received.</p>\n"}]},"apps":[],"jobName":"paragraph_1591815474595_596065473","id":"20200426-202547_2080286423","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148015"},{"title":"24 - Stopping the test","text":"%md\nWhen you are done, stop the query, terminate the test script, and exit your Spark shell.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474595_-970390401","id":"20200426-202547_1010479452","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148016"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815474595_-1351075090","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148017"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591815474595_-157943626","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148018"},{"text":"%md\n### Display Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474596_245504542","id":"20200429-213425_223954687","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148019"},{"title":"1   - Review the test data you will be using in this exercise","text":"%sh\nhead /home/devuser/data/telco/activations_stream/part-00000-ad954932-b2d7-426d-a7c1-429cfc1ebb33-c000.json","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"acct_num\":128931,\"dev_id\":\"247abaa7-6c85-4f2c-a96f-c11450250506\",\"phone\":\"6190399439\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":97620,\"dev_id\":\"16512c4a-b3d3-4f92-97b7-80f9df6a487e\",\"phone\":\"7072821950\",\"model\":\"Sorrento F24L\"}\n{\"acct_num\":74940,\"dev_id\":\"361cbf31-cc6c-4233-ad4e-28d10ab09568\",\"phone\":\"5623783063\",\"model\":\"MeeToo 3.0\"}\n{\"acct_num\":125327,\"dev_id\":\"6db1bbf9-3903-47f8-b150-388dcfd8d0d3\",\"phone\":\"9168710424\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":32782,\"dev_id\":\"e97f4769-13f7-4a28-9843-827f49e767dd\",\"phone\":\"8056881980\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":94440,\"dev_id\":\"1512a38e-721b-4712-afd4-79fd94ec2b56\",\"phone\":\"9513506516\",\"model\":\"MeeToo 2.0\"}\n{\"acct_num\":4337,\"dev_id\":\"6f495bc4-d6a9-43bb-90fc-650cbd6da073\",\"phone\":\"8056442972\",\"model\":\"Titanic 2200\"}\n{\"acct_num\":28646,\"dev_id\":\"eee8add4-1740-4a39-9f23-c9ee8b4e04ac\",\"phone\":\"5039316268\",\"model\":\"Titanic 2000\"}\n{\"acct_num\":44835,\"dev_id\":\"aed40d19-d5fc-4e11-a04a-f68094b38d73\",\"phone\":\"9289444545\",\"model\":\"Sorrento F41L\"}\n{\"acct_num\":121026,\"dev_id\":\"e67cd2c4-7358-4796-81ea-bc8a7bf91212\",\"phone\":\"5419395477\",\"model\":\"Sorrento F32L\"}\n"}]},"apps":[],"jobName":"paragraph_1591815474596_-129578756","id":"20200429-213439_92529216","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148020"},{"title":"2 - Open a terminal and set up a directory to contain the data files that Spark will read","text":"%md\n```\nmkdir -p /tmp/telco-streaming\nchmod +wr /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591815474596_-874210090","id":"20200429-213508_983964336","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148021"},{"title":"3 - Prepare a shell to run a streaming application","text":"%md\n```\npyspark --master local[2]\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474596_1883135890","id":"20200429-213505_44445732","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148022"},{"title":"4 - Define a schema for the structure of the input data","text":"%md\n```\ninputDir = \"file:/tmp/telco-streaming/\"\n\nfrom pyspark.sql.types import *\nactivationsSchema = StructType([ \n  StructField(\"acct_num\", IntegerType()),\n  StructField(\"dev_id\", StringType()),\n  StructField(\"phone\", StringType()),\n  StructField(\"model\", StringType())])\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474597_900619331","id":"20200601-192211_252144301","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148023"},{"title":"5 - Create a streaming DataFrame by reading the data you reviewed above","text":"%md\n```\nactivationsDF = spark.readStream.schema(activationsSchema).json(inputDir)\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474597_-41305233","id":"20200429-213501_2000175821","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148024"},{"title":"6 - Display the streaming DataFrame's schema to confirm that it is set up correctly","text":"%md\n```\nactivationsDF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474597_65665292","id":"20200429-213459_1552306503","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148025"},{"title":"7 - Confirm that the DataFrame's isStreaming property is set","text":"%md\n```\nactivationsDF.isStreaming\n```","user":"sysadmin","dateUpdated":"2020-06-10T19:58:38+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474597_308340106","id":"20200429-213458_677198705","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148026"},{"title":"8 - Start a streaming query","text":"%md\n```\nactivationsQuery = activationsDF.writeStream.outputMode(\"append\").option(\"truncate\",\"false\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474598_1664045054","id":"20200429-213458_820871973","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148027"},{"title":"9 - Begin streaming data","text":"%md\n```\n/home/devuser/bin/spark/streamingrm -spar/streamtest-file.sh /home/devuser/data/telco/activations_stream/ /tmp/telco-streaming\n```","user":"sysadmin","dateUpdated":"2020-06-14T03:49:06+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474598_-631590568","id":"20200429-213456_1259144389","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148028"},{"title":"10 - Confirm that the query is displaying data","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474598_1515690592","id":"20200429-213456_813615217","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148029"},{"title":"11 - Stopping the stream","text":"%md\n```\nactivationsQuery.stop()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474598_456349498","id":"20200429-213455_307289701","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148030"},{"title":"12 - Stopping the test script","text":"%md\nTerminate the test script in the second terminal window using `Ctrl+C`.","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474598_498066988","id":"20200429-213454_100119688","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148031"},{"text":"%md\n### Display Aggregated Streaming Data to the Console","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474599_925677175","id":"20200429-213454_1477948613","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148032"},{"title":"13 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474599_-655029787","id":"20200429-213454_63154555","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148033"},{"title":"14 - Create a DataFrame to count each device model","text":"%md\n```\nactivationCountDF = activationsDF.groupBy(\"model\").count()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474599_-908688031","id":"20200429-213453_573955489","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148034"},{"title":"15 - Start a streaming query that aggregates the data","text":"%md\n```\nactivationCountQuery = activationCountDF.writeStream.outputMode(\"complete\").format(\"console\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474599_1844203657","id":"20200429-213452_810770797","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148035"},{"title":"16 - Run the streamtest-file.sh script again","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474600_2052108341","id":"20200429-213450_1359119850","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148036"},{"title":"17 - Confirm that complete output mode is activated","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474600_-2060446431","id":"20200429-213450_472997628","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148037"},{"title":"18 - Stop the query and terminate the test script","text":"%md\n```\nactivationCountQuery.stop()\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474600_-613745692","id":"20200429-213450_463682321","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148038"},{"text":"%md\n### Output Data to Files","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474600_-1067346373","id":"20200429-213449_842091323","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148039"},{"title":"19 - Clean the streaming directory","text":"%md\n```\nrm -rf /tmp/telco-streaming/*\n````","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474600_-828439957","id":"20200429-213449_1526293893","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148040"},{"title":"20 - Create a new streaming DataFrame","text":"%md\n```\ntitanic1000DF = activationsDF.where(\"model = 'Titanic 1000'\").select(\"dev_id\",\"acct_num\")\ntitanic1000DF.printSchema()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474601_-810442718","id":"20200429-213448_1192486338","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148041"},{"title":"21 - Start a query that saves to HDFS","text":"%md\n```\nspark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-checkpoint\")\n```\n\n```\ntitanic1000Query = titanic1000DF.writeStream.trigger(processingTime=\"3 seconds\").outputMode(\"append\").format(\"csv\").option(\"path\",\"/user/zeppelin/titanic1000/\").start()\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474601_-1509799450","id":"20200429-213447_402657581","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148042"},{"title":"22 - Re-run the stream test script","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474601_1205630916","id":"20200429-213445_1660540485","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148043"},{"title":"23 - Confirm the target directory is receiving the files","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474601_-2119365082","id":"20200429-213445_371430499","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148044"},{"title":"24 - Stopping the test","text":"%md\n```\ntitanic1000Query.stop()\n```\n\n```\nCtrl+C\n```","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591815474602_-215370804","id":"20200429-213445_2052407867","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148045"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T18:57:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591815474602_-1839070729","id":"20181126-133017_244739700","dateCreated":"2020-06-10T18:57:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148046"}],"name":"PYSPARK/16-ProcessingStreamingData","id":"2FA2FKQNU","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}