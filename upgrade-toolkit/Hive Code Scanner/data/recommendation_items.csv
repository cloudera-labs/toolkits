mapred.map.tasks,"mapred.map.tasks is deprecated, use mapreduce.job.maps instead",me,"\w+\s+mapred.map.tasks(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.exec.mode.local.auto,"hive.exec.mode.local.auto is deprecated, use hive.exec.mode.local.auto.inputbytes.max instead",me,"\w+\s+hive.exec.mode.local.auto(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.merge.mapfiles,"hive.merge.mapfiles is deprecated, use hive.merge.smallfiles.avgsize instead",me,"\w+\s+hive.merge.mapfiles(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.map.aggr,"hive.map.aggr is deprecated, use mapreduce.combine.class instead",me,"\w+\s+hive.map.aggr(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.optimize.index.filter,"hive.optimize.index.filter is deprecated, use hive.optimize.index.autotune instead",me,"\w+\s+hive.optimize.index.filter(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
application,"the word application cannot be used in queries in CDP, Change applications. Enclose queries in backticks",se,"\s+(application)(\s+|\,)"
time,"the word time cannot be used in queries in CDP, Change time. Enclose queries in backticks",se,"\s+(time)(\s+|\,)"
numeric,"the word numeric cannot be used in queries in CDP, Change numeric. Enclose queries in backticks",se,"\s+(numeric)(\s+|\,)"
sync,"the word sync cannot be used in queries in CDP, Change sync. Enclose queries in backticks",se,"\s+(sync)(\s+|\,)"
hive.limit.query.max.table.partition,The parameter hive.limit.query.max.table.partition has been deprecated and must be removed from your code,me,"\w+\s+hive.limit.query.max.table.partition(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.warehouse.subdir.inherit.perms,The parameter hive.warehouse.subdir.inherit.perms has been deprecated and must be removed from your code,me,"\w+\s+hive.warehouse.subdir.inherit.perms(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.stats.fetch.partition.stats,The parameter hive.stats.fetch.partition.stats has been deprecated and must be removed from your code,me,"\w+\s+hive.stats.fetch.partition.stats(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.cache.ttl,The parameter hive.metastore.hbase.cache.ttl has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.cache.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.catalog.cache.size,The parameter hive.metastore.hbase.catalog.cache.size has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.catalog.cache.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggregate.stats.cache.size,The parameter hive.metastore.hbase.aggregate.stats.cache.size has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggregate.stats.cache.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggregate.stats.max.partitions,The parameter hive.metastore.hbase.aggregate.stats.max.partitions has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggregate.stats.max.partitions(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggregate.stats.false.positive.probability,The parameter hive.metastore.hbase.aggregate.stats.false.positive.probability has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggregate.stats.false.positive.probability(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.cache.max.writer.wait,The parameter hive.metastore.hbase.cache.max.writer.wait has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.cache.max.writer.wait(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.cache.max.reader.wait,The parameter hive.metastore.hbase.cache.max.reader.wait has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.cache.max.reader.wait(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.cache.max.full,The parameter hive.metastore.hbase.cache.max.full has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.cache.max.full(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggregate.stats.max.variance,The parameter hive.metastore.hbase.aggregate.stats.max.variance has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggregate.stats.max.variance(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.cache.clean.until,The parameter hive.metastore.hbase.cache.clean.until has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.cache.clean.until(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.connection.class,The parameter hive.metastore.hbase.connection.class has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.connection.class(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggr.stats.cache.entries,The parameter hive.metastore.hbase.aggr.stats.cache.entries has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggr.stats.cache.entries(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggr.stats.memory.ttl,The parameter hive.metastore.hbase.aggr.stats.memory.ttl has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggr.stats.memory.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggr.stats.invalidator.frequency,The parameter hive.metastore.hbase.aggr.stats.invalidator.frequency has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggr.stats.invalidator.frequency(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
hive.metastore.hbase.aggr.stats.hbase.ttl,The parameter hive.metastore.hbase.aggr.stats.hbase.ttl has been deprecated and must be removed from your code,me,"\w+\s+hive.metastore.hbase.aggr.stats.hbase.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
datanucleus.connectionPool.maxPoolSize,"datanucleus.connectionPool.maxPoolSize used to be 30, after upgrade this changes to 10",me,"\w+\s+datanucleus.connectionPool.maxPoolSize(\s{0,1}|\s+)\=(\s{0,1}|\s+)30"
datanucleus.connectionPoolingType,"datanucleus.connectionPoolingType used to be BONECP, after upgrade this changes to HikariCP",me,"\w+\s+datanucleus.connectionPoolingType(\s{0,1}|\s+)\=(\s{0,1}|\s+)BONECP"
hive.auto.convert.join.noconditionaltask.size,"hive.auto.convert.join.noconditionaltask.size used to be 20971520, after upgrade this changes to 52428800",me,"\w+\s+hive.auto.convert.join.noconditionaltask.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)20971520"
hive.auto.convert.sortmerge.join,"hive.auto.convert.sortmerge.join used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.auto.convert.sortmerge.join(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.auto.convert.sortmerge.join.to.mapjoin,"hive.auto.convert.sortmerge.join used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.auto.convert.sortmerge.join.to.mapjoin(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.cbo.enable,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.cbo.enable(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.cbo.show.warnings,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.cbo.show.warnings(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.compactor.worker.threads,"property used to be 0, after upgrade this changes to 5",me,"\w+\s+hive.compactor.worker.threads(\s{0,1}|\s+)\=(\s{0,1}|\s+)0"
hive.compute.query.using.stats,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.compute.query.using.stats(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.default.fileformat.managed,"property used to be NONE, after upgrade this changes to ORC",me,"\w+\s+hive.default.fileformat.managed(\s{0,1}|\s+)\=(\s{0,1}|\s+)NONE"
hive.driver.parallel.compilation,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.driver.parallel.compilation(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.exec.dynamic.partition.mode,"property used to be strict, after upgrade this changes to nonstrict",me,"\w+\s+hive.exec.dynamic.partition.mode(\s{0,1}|\s+)\=(\s{0,1}|\s+)strict"
hive.exec.max.dynamic.partitions,"property used to be 1000, after upgrade this changes to 5000",me,"\w+\s+hive.exec.max.dynamic.partitions(\s{0,1}|\s+)\=(\s{0,1}|\s+)1000"
hive.exec.max.dynamic.partitions.pernode,"property used to be 100, after upgrade this changes to 2000",me,"\w+\s+hive.exec.max.dynamic.partitions.pernode(\s{0,1}|\s+)\=(\s{0,1}|\s+)100"
hive.exec.reducers.max,"property used to be 1099, after upgrade this changes to 1009",me,"\w+\s+hive.exec.reducers.max(\s{0,1}|\s+)\=(\s{0,1}|\s+)1099"
hive.execution.engine,"property used to be mr, after upgrade this changes to tez",me,"\w+\s+hive.execution.engine(\s{0,1}|\s+)\=(\s{0,1}|\s+)mr"
hive.fetch.task.conversion,"property used to be minimal, after upgrade this changes to more",me,"\w+\s+hive.fetch.task.conversion(\s{0,1}|\s+)\=(\s{0,1}|\s+)minimal"
hive.fetch.task.conversion.threshold,"property used to be 256MB, after upgrade this changes to 1GB",me,"\w+\s+hive.fetch.task.conversion.threshold(\s{0,1}|\s+)\=(\s{0,1}|\s+)256MB"
hive.hashtable.key.count.adjustment,"property used to be 1, after upgrade this changes to 0.99",me,"\w+\s+hive.hashtable.key.count.adjustment(\s{0,1}|\s+)\=(\s{0,1}|\s+)1"
hive.limit.optimize.enable,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.limit.optimize.enable(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.limit.pushdown.memory.usage,"property used to be 0.1, after upgrade this changes to 0.04",me,"\w+\s+hive.limit.pushdown.memory.usage(\s{0,1}|\s+)\=(\s{0,1}|\s+)0.1"
hive.mapjoin.hybridgrace.hashtable,"property used to be TRUE, after upgrade this changes to FALSE",me,"\w+\s+hive.mapjoin.hybridgrace.hashtable(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
hive.mapred.reduce.tasks.speculative.execution,"property used to be TRUE, after upgrade this changes to FALSE",me,"\w+\s+hive.mapred.reduce.tasks.speculative.execution(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
hive.metastore.aggregate.stats.cache.enabled,"property used to be TRUE, after upgrade this changes to FALSE",me,"\w+\s+hive.metastore.aggregate.stats.cache.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
hive.metastore.disallow.incompatible.col.type.changes,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.metastore.disallow.incompatible.col.type.changes(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.metastore.dml.events,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.metastore.dml.events(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.metastore.event.message.factory,"property used to be org.apache.hadoop.hive.metastore.messaging.json.ExtendedJSONMessageFactory, after upgrade this changes to org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder",me,"\w+\s+hive.metastore.event.message.factory(\s{0,1}|\s+)\=(\s{0,1}|\s+)org.apache.hadoop.hive.metastore.messaging.json.ExtendedJSONMessageFactory"
hive.metastore.uri.selection,"property used to be SEQUENTIAL, after upgrade this changes to RANDOM",me,"\w+\s+hive.metastore.uri.selection(\s{0,1}|\s+)\=(\s{0,1}|\s+)SEQUENTIAL"
hive.metastore.warehouse.dir,"property used to be /user/hive/warehouse, after upgrade this changes to /warehouse/tablespace/managed/hive",me,"\w+\s+hive.metastore.warehouse.dir(\s{0,1}|\s+)\=(\s{0,1}|\s+)/user/hive/warehouse"
hive.optimize.metadataonly,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.optimize.metadataonly(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.optimize.point.lookup.min,"property used to be 31, after upgrade this changes to 2",me,"\w+\s+hive.optimize.point.lookup.min(\s{0,1}|\s+)\=(\s{0,1}|\s+)31"
hive.prewarm.numcontainers,"property used to be 10, after upgrade this changes to 3",me,"\w+\s+hive.prewarm.numcontainers(\s{0,1}|\s+)\=(\s{0,1}|\s+)10"
hive.script.operator.env.blacklist,"property used to be hive.txn.valid.txns,hive.script.operator.env.blacklist, after upgrade this changes to hive.txn.valid.txns,hive.txn.tables.valid.writeids,hive.txn.valid.writeids,hive.script.operator.env.blacklist",me,"\w+\s+hive.script.operator.env.blacklist(\s{0,1}|\s+)\=(\s{0,1}|\s+)hive.txn.valid.txns,hive.script.operator.env.blacklist(\s{0,1}|\s+)\;"
hive.security.command.whitelist,"property used to be set,reset,dfs,add,list,delete,reload,compile, after upgrade this changes to set,reset,dfs,add,list,delete,reload,compile,llap",me,"\w+\s+hive.security.command.whitelist(\s{0,1}|\s+)\=(\s{0,1}|\s+)set,reset,dfs,add,list,delete,reload,compile(\s{0,1}|\s+)\;"
hive.server2.enable.doAs,"property used to be TRUE, after upgrade this changes to FALSE",me,"\w+\s+hive.server2.enable.doAs(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
hive.server2.idle.session.timeout,"property used to be 12, after upgrade this changes to 24",me,"\w+\s+hive.server2.idle.session.timeout(\s{0,1}|\s+)\=(\s{0,1}|\s+)12"
hive.server2.max.start.attempts,"property used to be 30, after upgrade this changes to 5",me,"\w+\s+hive.server2.max.start.attempts(\s{0,1}|\s+)\=(\s{0,1}|\s+)30"
hive.server2.parallel.ops.in.session,"property used to be TRUE, after upgrade this changes to FALSE",me,"\w+\s+hive.server2.parallel.ops.in.session(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
hive.server2.support.dynamic.service.discovery,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.server2.support.dynamic.service.discovery(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.server2.tez.initialize.default.sessions,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.server2.tez.initialize.default.sessions(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.server2.thrift.max.worker.threads,"property used to be 100, after upgrade this changes to 500",me,"\w+\s+hive.server2.thrift.max.worker.threads(\s{0,1}|\s+)\=(\s{0,1}|\s+)100"
hive.server2.thrift.resultset.max.fetch.size,"property used to be 1000, after upgrade this changes to 10000",me,"\w+\s+hive.server2.thrift.resultset.max.fetch.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)1000(\s{0,1}|\s+)\;"
hive.service.metrics.file.location,"property used to be /var/log/hive/metrics-hiveserver2/metrics.log, after upgrade this changes to /var/log/hive/metrics-hiveserver2-hiveontez/metrics.log",me,"\w+\s+hive.service.metrics.file.location(\s{0,1}|\s+)\=(\s{0,1}|\s+)/var/log/hive/metrics-hiveserver2/metrics.log"
hive.stats.column.autogather,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.stats.column.autogather(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.stats.deserialization.factor,"property used to be 1, after upgrade this changes to 10",me,"\w+\s+hive.stats.deserialization.factor(\s{0,1}|\s+)\=(\s{0,1}|\s+)1(\s{0,1}|\s+)\;"
hive.support.special.characters.tablename,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.support.special.characters.tablename(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.tez.auto.reducer.parallelism,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.tez.auto.reducer.parallelism(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.tez.bucket.pruning,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.tez.bucket.pruning(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.tez.container.size,"property used to be -1, after upgrade this changes to 4096",me,"\w+\s+hive.tez.container.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)-1"
hive.tez.exec.print.summary,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.tez.exec.print.summary(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.txn.manager,"property used to be org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager, after upgrade this changes to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager",me,"\w+\s+hive.txn.manager(\s{0,1}|\s+)\=(\s{0,1}|\s+)org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager"
hive.vectorized.execution.mapjoin.minmax.enabled,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.vectorized.execution.mapjoin.minmax.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
hive.vectorized.use.row.serde.deserialize,"property used to be FALSE, after upgrade this changes to TRUE",me,"\w+\s+hive.vectorized.use.row.serde.deserialize(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
cast timestamp,"By Default, casting a numeric type value into a timestamp produces a result that reflects the UTC instead of the time zone of the cluster.You can set hive.strict.timestamp.conversion to false for legacy behavior.",se,CAST(\s+|\s{0})\(\d+\s+AS\s+TIMESTAMP
cast zeros as null,"By Default, casting of an invalid date returns a result. Please ensure cast is as desired.",se,CAST(\s+|\s{0})\((\s+|\s{0})('0000-00-00'|'000-00-00 00:00:00')
acos,"ACOS(2) function returend NAN, after upgrade the function returns null consistent with SQL standards",se,acos\s*\(\s*[^)]+\s*\)
asin,"ASIN(2) function returend NAN, after upgrade the function returns null consistent with SQL standards",se,asin\s*\(\s*[^)]+\s*\)
corr,"CORR returned 'NaN' for N*SUM(x*x) = SUM(x)*SUM(x) and N*SUM(y*y) = SUM(y)*SUM(y). However, the function is expected to return NULL per the standards.",se,corr\(.*\)
covar_samp,COVAR_SAMP returns '0' when the function is applied to a set with a single element. The function is expected to return NULL.,se,covar_samp\s*\(\s*[\w_]+\s*\)
cast decimal value,"In CDH, fractional digits appear in the output of a cast of a decimal value that has only zeros in the fractional part, after upgrade fractional decimal digits are dropped in the output of a cast of a decimal value that has only zeros in the fractional part, workaround is to cast the value as string",se,cast\(\d+\.000\s+AS\s+.*\(\d+\)
casting with leading or trailing spaces,"The CAST function resulted in incorrect results while casting string values with leading or trailing spaces to numeric data types, after upgrade the behavior is consistent across datatypes.",se,cast\s*\(\s*'\\s*(\d+)\\s*'\s+as\s+(tinyint|smallint|int|bigint)\s*\)
length,"By default,the LENGTH function supports double in addition to string, char, varchar or binary, after upfgrade LENGTH supports columns of data type string, char, varchar or binary. Double is not supported by length() in CDP, workaround is to cast the double value to char",se,length.*(\d+.\d+)
stddev_samp,"By default,aggregate functions were not in compliance with SQL:2011 standards and returns '0' when the function is applied to a set with a single element, after upgrade it returns null ",se,stddev_samp\(.*\)
var_samp,"By default,aggregate functions were not in compliance with SQL:2011 standards and returns '0' when the function is applied to a set with a single element, after upgrade it returns null ",se,var_samp\(.*\)
order by,"By default,hive.default.nulls.last is false. In ascending (ASC) order, NULL appears first. In descending (DESC) order, NULL appears last, after upgrade hive.default.nulls.last is true. In ascending (ASC) order, NULL appears last. In descending (DESC) order, NULL appears first, workaround is set hive.default.nulls.last to false",se,order by (.*?)\s+desc
not null,"By default,You could enable/enforce a NOT NULL constraint on an external table, after upgrade enabling or enforcing a NOT NULL constraint on an external table causes an error. Workaround is removing the NOT NULL constraint in CREATE TABLE or ALTER TABLE statements.",se,"(,.* not null enable,)"
add_months includes time,"By default, the add months query incorrectly omitted the time, after upgrade the query output includes the time",se,"add_months\([^,]+,\s*\d+\)"
add_months date validation,"By default ADD_MONTHS function would execute on invalid dates, after upgrade CDP performs validation of the date before executing the ADD_MONTHS function.",se,"add_months\('\d{4}-\d{2}-\d{2}',\s*\d+\)"
casting invalid dates,"Casting of invalid date (zero value in one or more of the 3 fields of date, month, year) returns a NULL value, after upgrade Casting of an invalid date returns a result.",se,CAST\s*\(\s*('0000-00-00'|'000-00-00 00:00:00')\s*AS\s*(DATE|TIMESTAMP)\s*\)
from_unixtime,"By default, The functions supported the system time zone , The functions now support the user session time zone ",se,from_unixtime\(\d+\.\d+\)
current_timestamp,"CURRENT_TIMESTAMP returns the result in different output formats, after upgrade CDP CDP gives output format for CURRENT_TIMESTAMP YYYY-MM-DD HH:MM:SS.fff.",se,current_timestamp\(\)
to_date,"Some of the original date functions, such as TO_DATE, DATE_ADD, and DATE_SUB returned string values instead of date values. This was because the date return type was not available in Hive when these functions were introduced after upgrade the return type of to_date, date_add, and date_sub functions from string to date.",se,to_date\s*\(\s*[^)]+\s*\)
date_add,"Some of the original date functions, such as TO_DATE, DATE_ADD, and DATE_SUB returned string values instead of date values. This was because the date return type was not available in Hive when these functions were introduced after upgrade the return type of to_date, date_add, and date_sub functions from string to date.",se,date_add\s*\(\s*[^)]+\s*\)
date_sub,"Some of the original date functions, such as TO_DATE, DATE_ADD, and DATE_SUB returned string values instead of date values. This was because the date return type was not available in Hive when these functions were introduced after upgrade the return type of to_date, date_add, and date_sub functions from string to date.",se,date_sub\s*\(\s*[^)]+\s*\)
unix_timestamp for hours,"In CDH, you can use lowercase hh to represent hours of a timestamp in the format specification, after upgrade Hive in CDP requires uppercase HH",se,"unix_timestamp\('\d+-\d+-\d+\s+\d+:\d+:\d+',\s+""yyyy-MM-dd\s+hh:mm:ss""\)"
unix_timestamp for microseconds,"In CDH you do not have to include microseconds in the format specification, after upgrade microseconds SSS are required in the format specification.For example, 'MM-dd-yyyy HH:mm:ss.SSS",se,"unix_timestamp\('\d+-\d+-\d+\s+\d+:\d+:\d+',\s+""yyyy-MM-dd\s+HH:mm:ss""\)"
cast unix_timestamp to string,"In CDH Casting a unix timestamp as a string without defining the proper date and time format works. In CDP it provides null. Workaround is to provide full date,time format yyyy-MM-dd:HH:mm:ss",se,"cast\(unix_timestamp\('\d+-\d+-\d+:\d+:\d+:\d+',\s+'yyyyMM'\)\s+as\s+string\)"
coalesce,"In CDH the NVL and COALESCE function were both used for the same purpose - to replace a NULL value with a non-NULL value. Both the functions were compiled separately, after upgrade the NVL function is compiled to COALESCE, which is reflected in the execution plan generated using the EXPLAIN command. The optimizations that are performed on the COALESCE function are also performed on the NVL calls.",se,coalesce\(.*\)
hive.txn.xlock.mergeinsert,"MERGE INSERT operation is treated as regular INSERT and acquires a SHARED_READ lock, which does not prevent other INSERT operations. By default the property is set to false in cdp.",me,"\w+\s+hive.txn.xlock.mergeinsert(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
trailing zeros of decimal constants,"Hive removes trailing zeros of decimal constants in some cases. Padding decimal values with trailing zeros is not consistent, after upgrade CDP provides the fix that pads constant decimal values with trailing zeros up to the specified scale",se,"cast\s*\(\s*cast\s*\(\s*1\.1\s*as\s*decimal\s*\(\s*22\s*,\s*2\s*\)\s*\)\s*as\s*string\s*\)"
Julian dates,"In CDH Julian calendar (before Oct 15, 1582) dates are handled improperly by date/timestamp UDFs.In CDP, Hive uses a proleptic Gregorian calendar used in SQL standard to handle Julian dates",se,date_format\s*\((.*?)\)