"mapred.map.tasks","mapred.map.tasks is deprecated, use mapreduce.job.maps instead","me","\w+\s+mapred.map.tasks(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.exec.mode.local.auto","hive.exec.mode.local.auto is deprecated, use hive.exec.mode.local.auto.inputbytes.max instead","me","\w+\s+hive.exec.mode.local.auto(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.merge.mapfiles","hive.merge.mapfiles is deprecated, use hive.merge.smallfiles.avgsize instead","me","\w+\s+hive.merge.mapfiles(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.map.aggr","hive.map.aggr is deprecated, use mapreduce.combine.class instead","me","\w+\s+hive.map.aggr(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.optimize.index.filter","hive.optimize.index.filter is deprecated, use hive.optimize.index.autotune instead","me","\w+\s+hive.optimize.index.filter(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"application","the word application cannot be used in queries in CDP, Change applications. Enclose queries in backticks","se","\s+(application)(\s+|\,)"
"time","the word time cannot be used in queries in CDP, Change time. Enclose queries in backticks","se","\s+(time)(\s+|\,)"
"numeric","the word numeric cannot be used in queries in CDP, Change numeric. Enclose queries in backticks","se","\s+(numeric)(\s+|\,)"
"sync","the word sync cannot be used in queries in CDP, Change sync. Enclose queries in backticks","se","\s+(sync)(\s+|\,)"
"hive.limit.query.max.table.partition","The parameter hive.limit.query.max.table.partition has been deprecated and must be removed from your code","me","\w+\s+hive.limit.query.max.table.partition(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.warehouse.subdir.inherit.perms","The parameter hive.warehouse.subdir.inherit.perms has been deprecated and must be removed from your code","me","\w+\s+hive.warehouse.subdir.inherit.perms(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.stats.fetch.partition.stats","The parameter hive.stats.fetch.partition.stats has been deprecated and must be removed from your code","me","\w+\s+hive.stats.fetch.partition.stats(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.cache.ttl","The parameter hive.metastore.hbase.cache.ttl has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.cache.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.catalog.cache.size","The parameter hive.metastore.hbase.catalog.cache.size has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.catalog.cache.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggregate.stats.cache.size","The parameter hive.metastore.hbase.aggregate.stats.cache.size has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggregate.stats.cache.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggregate.stats.max.partitions","The parameter hive.metastore.hbase.aggregate.stats.max.partitions has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggregate.stats.max.partitions(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggregate.stats.false.positive.probability","The parameter hive.metastore.hbase.aggregate.stats.false.positive.probability has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggregate.stats.false.positive.probability(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.cache.max.writer.wait","The parameter hive.metastore.hbase.cache.max.writer.wait has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.cache.max.writer.wait(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.cache.max.reader.wait","The parameter hive.metastore.hbase.cache.max.reader.wait has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.cache.max.reader.wait(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.cache.max.full","The parameter hive.metastore.hbase.cache.max.full has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.cache.max.full(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggregate.stats.max.variance","The parameter hive.metastore.hbase.aggregate.stats.max.variance has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggregate.stats.max.variance(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.cache.clean.until","The parameter hive.metastore.hbase.cache.clean.until has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.cache.clean.until(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.connection.class","The parameter hive.metastore.hbase.connection.class has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.connection.class(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggr.stats.cache.entries","The parameter hive.metastore.hbase.aggr.stats.cache.entries has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggr.stats.cache.entries(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggr.stats.memory.ttl","The parameter hive.metastore.hbase.aggr.stats.memory.ttl has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggr.stats.memory.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggr.stats.invalidator.frequency","The parameter hive.metastore.hbase.aggr.stats.invalidator.frequency has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggr.stats.invalidator.frequency(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"hive.metastore.hbase.aggr.stats.hbase.ttl","The parameter hive.metastore.hbase.aggr.stats.hbase.ttl has been deprecated and must be removed from your code","me","\w+\s+hive.metastore.hbase.aggr.stats.hbase.ttl(\s{0,1}|\s+)\=(\s{0,1}|\s+)"
"datanucleus.connectionPool.maxPoolSize","datanucleus.connectionPool.maxPoolSize used to be 30, after upgrade this changes to 10","me","\w+\s+datanucleus.connectionPool.maxPoolSize(\s{0,1}|\s+)\=(\s{0,1}|\s+)30"
"datanucleus.connectionPoolingType","datanucleus.connectionPoolingType used to be BONECP, after upgrade this changes to HikariCP","me","\w+\s+datanucleus.connectionPoolingType(\s{0,1}|\s+)\=(\s{0,1}|\s+)BONECP"
"hive.auto.convert.join.noconditionaltask.size","hive.auto.convert.join.noconditionaltask.size used to be 20971520, after upgrade this changes to 52428800","me","\w+\s+hive.auto.convert.join.noconditionaltask.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)20971520"
"hive.auto.convert.sortmerge.join","hive.auto.convert.sortmerge.join used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.auto.convert.sortmerge.join(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.auto.convert.sortmerge.join.to.mapjoin","hive.auto.convert.sortmerge.join used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.auto.convert.sortmerge.join.to.mapjoin(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.cbo.enable","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.cbo.enable(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.cbo.show.warnings","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.cbo.show.warnings(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.compactor.worker.threads","property used to be 0, after upgrade this changes to 5","me","\w+\s+hive.compactor.worker.threads(\s{0,1}|\s+)\=(\s{0,1}|\s+)0"
"hive.compute.query.using.stats","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.compute.query.using.stats(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.default.fileformat.managed","property used to be NONE, after upgrade this changes to ORC","me","\w+\s+hive.default.fileformat.managed(\s{0,1}|\s+)\=(\s{0,1}|\s+)NONE"
"hive.driver.parallel.compilation","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.driver.parallel.compilation(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.exec.dynamic.partition.mode","property used to be strict, after upgrade this changes to nonstrict","me","\w+\s+hive.exec.dynamic.partition.mode(\s{0,1}|\s+)\=(\s{0,1}|\s+)strict"
"hive.exec.max.dynamic.partitions","property used to be 1000, after upgrade this changes to 5000","me","\w+\s+hive.exec.max.dynamic.partitions(\s{0,1}|\s+)\=(\s{0,1}|\s+)1000"
"hive.exec.max.dynamic.partitions.pernode","property used to be 100, after upgrade this changes to 2000","me","\w+\s+hive.exec.max.dynamic.partitions.pernode(\s{0,1}|\s+)\=(\s{0,1}|\s+)100"
"hive.exec.reducers.max","property used to be 1099, after upgrade this changes to 1009","me","\w+\s+hive.exec.reducers.max(\s{0,1}|\s+)\=(\s{0,1}|\s+)1099"
"hive.execution.engine","property used to be mr, after upgrade this changes to tez","me","\w+\s+hive.execution.engine(\s{0,1}|\s+)\=(\s{0,1}|\s+)mr"
"hive.fetch.task.conversion","property used to be minimal, after upgrade this changes to more","me","\w+\s+hive.fetch.task.conversion(\s{0,1}|\s+)\=(\s{0,1}|\s+)minimal"
"hive.fetch.task.conversion.threshold","property used to be 256MB, after upgrade this changes to 1GB","me","\w+\s+hive.fetch.task.conversion.threshold(\s{0,1}|\s+)\=(\s{0,1}|\s+)256MB"
"hive.hashtable.key.count.adjustment","property used to be 1, after upgrade this changes to 0.99","me","\w+\s+hive.hashtable.key.count.adjustment(\s{0,1}|\s+)\=(\s{0,1}|\s+)1"
"hive.limit.optimize.enable","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.limit.optimize.enable(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.limit.pushdown.memory.usage","property used to be 0.1, after upgrade this changes to 0.04","me","\w+\s+hive.limit.pushdown.memory.usage(\s{0,1}|\s+)\=(\s{0,1}|\s+)0.1"
"hive.mapjoin.hybridgrace.hashtable","property used to be TRUE, after upgrade this changes to FALSE","me","\w+\s+hive.mapjoin.hybridgrace.hashtable(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
"hive.mapred.reduce.tasks.speculative.execution","property used to be TRUE, after upgrade this changes to FALSE","me","\w+\s+hive.mapred.reduce.tasks.speculative.execution(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
"hive.metastore.aggregate.stats.cache.enabled","property used to be TRUE, after upgrade this changes to FALSE","me","\w+\s+hive.metastore.aggregate.stats.cache.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
"hive.metastore.disallow.incompatible.col.type.changes","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.metastore.disallow.incompatible.col.type.changes(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.metastore.dml.events","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.metastore.dml.events(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.metastore.event.message.factory","property used to be org.apache.hadoop.hive.metastore.messaging.json.ExtendedJSONMessageFactory, after upgrade this changes to org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder","me","\w+\s+hive.metastore.event.message.factory(\s{0,1}|\s+)\=(\s{0,1}|\s+)org.apache.hadoop.hive.metastore.messaging.json.ExtendedJSONMessageFactory"
"hive.metastore.uri.selection","property used to be SEQUENTIAL, after upgrade this changes to RANDOM","me","\w+\s+hive.metastore.uri.selection(\s{0,1}|\s+)\=(\s{0,1}|\s+)SEQUENTIAL"
"hive.metastore.warehouse.dir","property used to be /user/hive/warehouse, after upgrade this changes to /warehouse/tablespace/managed/hive","me","\w+\s+hive.metastore.warehouse.dir(\s{0,1}|\s+)\=(\s{0,1}|\s+)/user/hive/warehouse"
"hive.optimize.metadataonly","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.optimize.metadataonly(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.optimize.point.lookup.min","property used to be 31, after upgrade this changes to 2","me","\w+\s+hive.optimize.point.lookup.min(\s{0,1}|\s+)\=(\s{0,1}|\s+)31"
"hive.prewarm.numcontainers","property used to be 10, after upgrade this changes to 3","me","\w+\s+hive.prewarm.numcontainers(\s{0,1}|\s+)\=(\s{0,1}|\s+)10"
"hive.script.operator.env.blacklist","property used to be hive.txn.valid.txns,hive.script.operator.env.blacklist, after upgrade this changes to hive.txn.valid.txns,hive.txn.tables.valid.writeids,hive.txn.valid.writeids,hive.script.operator.env.blacklist","me","\w+\s+hive.script.operator.env.blacklist(\s{0,1}|\s+)\=(\s{0,1}|\s+)hive.txn.valid.txns,hive.script.operator.env.blacklist(\s{0,1}|\s+)\;"
"hive.security.command.whitelist","property used to be set,reset,dfs,add,list,delete,reload,compile, after upgrade this changes to set,reset,dfs,add,list,delete,reload,compile,llap","me","\w+\s+hive.security.command.whitelist(\s{0,1}|\s+)\=(\s{0,1}|\s+)set,reset,dfs,add,list,delete,reload,compile(\s{0,1}|\s+)\;"
"hive.server2.enable.doAs","property used to be TRUE, after upgrade this changes to FALSE","me","\w+\s+hive.server2.enable.doAs(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
"hive.server2.idle.session.timeout","property used to be 12, after upgrade this changes to 24","me","\w+\s+hive.server2.idle.session.timeout(\s{0,1}|\s+)\=(\s{0,1}|\s+)12"
"hive.server2.max.start.attempts","property used to be 30, after upgrade this changes to 5","me","\w+\s+hive.server2.max.start.attempts(\s{0,1}|\s+)\=(\s{0,1}|\s+)30"
"hive.server2.parallel.ops.in.session","property used to be TRUE, after upgrade this changes to FALSE","me","\w+\s+hive.server2.parallel.ops.in.session(\s{0,1}|\s+)\=(\s{0,1}|\s+)true"
"hive.server2.support.dynamic.service.discovery","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.server2.support.dynamic.service.discovery(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.server2.tez.initialize.default.sessions","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.server2.tez.initialize.default.sessions(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.server2.thrift.max.worker.threads","property used to be 100, after upgrade this changes to 500","me","\w+\s+hive.server2.thrift.max.worker.threads(\s{0,1}|\s+)\=(\s{0,1}|\s+)100"
"hive.server2.thrift.resultset.max.fetch.size","property used to be 1000, after upgrade this changes to 10000","me","\w+\s+hive.server2.thrift.resultset.max.fetch.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)1000(\s{0,1}|\s+)\;"
"hive.service.metrics.file.location","property used to be /var/log/hive/metrics-hiveserver2/metrics.log, after upgrade this changes to /var/log/hive/metrics-hiveserver2-hiveontez/metrics.log","me","\w+\s+hive.service.metrics.file.location(\s{0,1}|\s+)\=(\s{0,1}|\s+)/var/log/hive/metrics-hiveserver2/metrics.log"
"hive.stats.column.autogather","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.stats.column.autogather(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.stats.deserialization.factor","property used to be 1, after upgrade this changes to 10","me","\w+\s+hive.stats.deserialization.factor(\s{0,1}|\s+)\=(\s{0,1}|\s+)1(\s{0,1}|\s+)\;"
"hive.support.special.characters.tablename","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.support.special.characters.tablename(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.tez.auto.reducer.parallelism","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.tez.auto.reducer.parallelism(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.tez.bucket.pruning","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.tez.bucket.pruning(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.tez.container.size","property used to be -1, after upgrade this changes to 4096","me","\w+\s+hive.tez.container.size(\s{0,1}|\s+)\=(\s{0,1}|\s+)-1"
"hive.tez.exec.print.summary","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.tez.exec.print.summary(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.txn.manager","property used to be org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager, after upgrade this changes to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager","me","\w+\s+hive.txn.manager(\s{0,1}|\s+)\=(\s{0,1}|\s+)org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager"
"hive.vectorized.execution.mapjoin.minmax.enabled","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.vectorized.execution.mapjoin.minmax.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"hive.vectorized.use.row.serde.deserialize","property used to be FALSE, after upgrade this changes to TRUE","me","\w+\s+hive.vectorized.use.row.serde.deserialize(\s{0,1}|\s+)\=(\s{0,1}|\s+)false"
"cast timestamp","By Default, casting a numeric type value into a timestamp produces a result that reflects the UTC instead of the time zone of the cluster.","se","CAST(\s+|\s{0})\(\d+\s+AS\s+TIMESTAMP"
"cast zeros as null","By Default, casting of an invalid date returns a result. Please ensure cast is as desired.","se","CAST(\s+|\s{0})\((\s+|\s{0})('0000-00-00'|'000-00-00 00:00:00')"
"greatest","If NULL is found while using greatest in CDP you could return the incorrect result, use NVL to set a default","me","\w+\s+greatest\("
"least","If NULL is found while using least in CDP you could return the incorrect result, use NVL to set a default","me","\w+\s+least\("