{"paragraphs":[{"text":"%md\n# About\n**Lab:** Using Datasets in Scala\n**Objective:** Explore Datasets using web log data\n**File locations:**\n    /user/zeppelin/weblogs\n\n**Successful outcome:** \n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Using Datasets in Scala\n<br  /><strong>Objective:</strong> Explore Datasets using web log data\n<br  /><strong>File locations:</strong></p>\n<pre><code>/devsh_loudacre/weblogs\n</code></pre>\n<p><strong>Successful outcome:</strong>\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819999554_847719252","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:53"},{"text":"%md\n# Setup","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p><strong>Important:</strong> This exercise depends on <strong><em> ***Insert previous exercise title here (with link?)*** </em></strong>. If you did not complete that exercise, run the course catch-up script and advance to the current exercise:</p>\n<pre><code>$ $DEVSH/scripts/catchup.sh\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591819999555_-2104146001","id":"20181201-044336_178705192","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591819999556_998703607","id":"20181126-093358_358613711","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%md\n### Explore Datasets Using Web Log Data\n\nFind all the account IDs and the IP addresses from which those accounts logged in to the web site from Loudacre's web log data.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Explore Datasets Using Web Log Data</h3>\n<p>Find all the account IDs and the IP addresses from which those accounts logged in to the web site from Loudacre's web log data.</p>\n"}]},"apps":[],"jobName":"paragraph_1591819999556_-1856741619","id":"20200425-214556_1601909015","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"title":"1 - Create a case class for account ID/IP address pairs","text":"%spark2\ncase class AccountIP (id: Int, ip: String)","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999557_-2034720202","id":"20200425-214738_1778456643","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"2 - Create an RDD of AccountIP objects","text":"%md\nCreate an RDD of `AccountIP` objects by using the web log data in `/devsh_loudacre/weblogs`. Split the data by spaces and use the first field as IP address and the third as account ID.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999557_-265738630","id":"20200429-045932_1953923456","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"","text":"%spark2\nval accountIPRDD = sc.\ntextFile(\"/user/zeppelin/weblogs\").\nmap(line => line.split(' ')).\nmap(fields => new AccountIP(fields(2).toInt,fields(0)))","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:28: error: not found: type AccountIP\n       map(fields => new AccountIP(fields(2).toInt,fields(0)))\n                         ^\n"}]},"apps":[],"jobName":"paragraph_1591819999557_-1617119479","id":"20200425-214850_1932539533","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"3 - Create a Dataset of AccountIP objects using the new RDD","text":"%spark2\nval accountIPDS = spark.createDataset(accountIPRDD)","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999558_-1112756868","id":"20200425-215002_646404360","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"title":"4 - View the schema and the data in the new Dataset","text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999558_1509758912","id":"20200425-215045_629388297","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"5 - Compare the result types of a typed transformation and an untyped transformation","text":"%md\nCompare the result types of a typed transformation -- `distinct` -- and an untyped transformation -- `groupBy/count`.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999559_-1773766604","id":"20200425-215147_758473712","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%spark2\nval distinctIPDS = accountIPDS.distinct\nval accountIPCountDS = distinctIPDS.groupBy(\"id\",\"ip\").count","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999559_-504362812","id":"20200429-050210_544844192","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"6 - Save the new Dataset as a Parquet file, then read it back into a DataFrame","text":"%md\nSave the `accountIPDS` Dataset as a Parquet file, then read the file back into a DataFrame. Note that the type of the original Dataset (`AccountIP`) is not preserved, but the types of the columns are.","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999560_-1509401328","id":"20200425-215146_2094043180","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999560_1469208690","id":"20200429-050458_1142980792","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%md\n### Bonus Exercise","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Bonus Exercise</h3>\n"}]},"apps":[],"jobName":"paragraph_1591819999560_761022313","id":"20200425-215146_126681074","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"title":"1 - Perform a SQL query on a view","text":"%md\nCreate a view on the `AccountIPDS` Dataset, and perform a SQL query on the view. What is the return type of the SQL query? Were column types preserved?","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999561_-2110530561","id":"20200425-215145_1722351189","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%spark2\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999561_1723375311","id":"20200429-050600_1572101890","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819999562_1039669720","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591819999562_1066258372","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"%md\n### Explore Datasets Using Web Log Data\n\nIf not previously uploaded: \n```\n$ hdfs dfs -put /home/devuser/data/telco/weblogs /user/zeppelin/\n```","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999562_-415749703","id":"20200429-045508_1792730451","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"1 - Create a case class for account ID/IP address pairs","text":"%spark2\ncase class AccountIP (id: Int, ip: String)","user":"sysadmin","dateUpdated":"2020-06-10T23:50:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class AccountIP\n"}]},"apps":[],"jobName":"paragraph_1591819999563_450347985","id":"20200429-045519_1771957595","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:30+0000","dateFinished":"2020-06-10T23:50:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"2 - Create an RDD of AccountIP objects","text":"%spark2\nval accountIPRDD=sc.textFile(\"/user/zeppelin/weblogs\").map(line => line.split(' ')).map(fields => new AccountIP(fields(2).toInt,fields(0)))","user":"sysadmin","dateUpdated":"2020-06-10T23:50:33+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"accountIPRDD: org.apache.spark.rdd.RDD[AccountIP] = MapPartitionsRDD[914] at map at <console>:33\n"}]},"apps":[],"jobName":"paragraph_1591819999563_934945046","id":"20200429-045517_898375787","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:33+0000","dateFinished":"2020-06-10T23:50:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"title":"3 - Create a Dataset of AccountIP objects using the new RDD","text":"%spark2\nval accountIPDS = spark.createDataset(accountIPRDD)","user":"sysadmin","dateUpdated":"2020-06-10T23:50:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"accountIPDS: org.apache.spark.sql.Dataset[AccountIP] = [id: int, ip: string]\n"}]},"apps":[],"jobName":"paragraph_1591819999564_1535201131","id":"20200429-045515_737119741","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:35+0000","dateFinished":"2020-06-10T23:50:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"title":"4 - View the schema and the data in the new Dataset","text":"%spark2\naccountIPDS.printSchema\naccountIPDS.show","user":"sysadmin","dateUpdated":"2020-06-10T23:50:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: integer (nullable = false)\n |-- ip: string (nullable = true)\n\n+-----+---------------+\n|   id|             ip|\n+-----+---------------+\n|69827|      3.94.78.5|\n|69827|      3.94.78.5|\n|21475|   19.38.140.62|\n|21475|   19.38.140.62|\n| 2489| 129.133.56.105|\n| 2489| 129.133.56.105|\n| 4712|217.150.149.167|\n| 4712|217.150.149.167|\n| 4712|217.150.149.167|\n| 4712|217.150.149.167|\n|45922|  209.151.12.34|\n|45922|  209.151.12.34|\n|  144|  184.97.84.245|\n|  144|  184.97.84.245|\n|33908|   233.60.251.2|\n|33908|   233.60.251.2|\n|51340|160.134.139.204|\n|51340|160.134.139.204|\n|13392|  19.209.18.222|\n|13392|  19.209.18.222|\n+-----+---------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591819999564_-374612224","id":"20200429-045514_971111712","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:39+0000","dateFinished":"2020-06-10T23:50:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"title":"5 - Compare the result types of a typed transformation and an untyped transformation","text":"%spark2\nval distinctIPDS = accountIPDS.distinct\nval accountIPCountDS = distinctIPDS.groupBy(\"id\",\"ip\").count","user":"sysadmin","dateUpdated":"2020-06-10T23:50:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"distinctIPDS: org.apache.spark.sql.Dataset[AccountIP] = [id: int, ip: string]\naccountIPCountDS: org.apache.spark.sql.DataFrame = [id: int, ip: string ... 1 more field]\n"}]},"apps":[],"jobName":"paragraph_1591819999564_-719116794","id":"20200429-045514_1400658563","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:50+0000","dateFinished":"2020-06-10T23:50:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"title":"6 - Save the new Dataset as a Parquet file, then read it back into a DataFrame","text":"%spark2\naccountIPDS.write.save(\"/user/zeppelin/accountIPs\")\n\nval accountIPDF = spark.read.load(\"/user/zeppelin/accountIPs\")","user":"sysadmin","dateUpdated":"2020-06-10T23:50:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"accountIPDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]\n"}]},"apps":[],"jobName":"paragraph_1591819999565_1312202935","id":"20200429-045513_134581158","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:50:53+0000","dateFinished":"2020-06-10T23:50:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"%md\n### Bonus Exercise","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591819999565_928221938","id":"20200429-045512_272015402","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"1 - Perform a SQL query on a view","text":"%spark2\n//case class AccountIP (id: Int, ip: String)\n\n//val accountIPRDD=sc.textFile(\"/devsh_loudacre/weblogs\").map(line => line.split(' ')).map(fields => new AccountIP(fields(2).toInt,fields(0)))\n\n//val accountIPDS = spark.createDataset(accountIPRDD)\naccountIPDS.createOrReplaceTempView(\"account_ip\")\nval queryDF = spark.sql(\"SELECT DISTINCT *  FROM account_ip WHERE id < 200\")\nqueryDF.printSchema\nqueryDF.show","user":"sysadmin","dateUpdated":"2020-06-10T23:51:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- id: integer (nullable = false)\n |-- ip: string (nullable = true)\n\n+---+---------------+\n| id|             ip|\n+---+---------------+\n|193|  24.236.212.29|\n| 68|     30.79.81.4|\n| 62|  144.28.77.181|\n|164|  125.204.88.88|\n| 71|  125.237.22.79|\n| 73|242.144.195.130|\n|179|  190.158.14.43|\n| 34|   176.54.2.155|\n| 50|134.153.231.103|\n| 85|  15.122.12.230|\n| 93|133.136.172.229|\n|190|  61.106.56.145|\n|105|      8.7.64.28|\n| 30| 226.62.201.108|\n|192|      5.1.79.46|\n| 89| 128.22.254.129|\n| 49|  178.124.97.30|\n|117|   59.108.46.89|\n| 95|  98.48.220.213|\n|138|   236.24.67.21|\n+---+---------------+\nonly showing top 20 rows\n\nqueryDF: org.apache.spark.sql.DataFrame = [id: int, ip: string]\n"}]},"apps":[],"jobName":"paragraph_1591819999565_-2137221425","id":"20200429-045511_206929784","dateCreated":"2020-06-10T20:13:19+0000","dateStarted":"2020-06-10T23:51:01+0000","dateFinished":"2020-06-10T23:51:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T20:13:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591819999566_-69668556","id":"20181126-133017_244739700","dateCreated":"2020-06-10T20:13:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"}],"name":"ScalaSpark/11-UsingDatasetsInScala","id":"2FDGGQ49W","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}