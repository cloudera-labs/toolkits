{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with DataFrames and Schemas\n**Objective:** Practice working with structured account data and mobile device data using DataFrames\n**File locations:**\n- Data files (HDFS): /user/zeppelin/device.json\n- Hive Tables: telco.account\n\n**Successful outcome:** Create and save DataFrames using different types of data sources, and infer and define schemas\n**Before you begin:** \n**Related lessons:** \n\n---","user":"devuser","dateUpdated":"2020-11-19T03:47:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with DataFrames and Schemas\n<br  /><strong>Objective:</strong> Practice working with structured account data and mobile device data using DataFrames\n<br  /><strong>File locations:</strong></p>\n<ul>\n<li>Data files (HDFS): /user/zeppelin/device.json</li>\n<li>Hive Tables: telco.account</li>\n</ul>\n<p><strong>Successful outcome:</strong> Create and save DataFrames using different types of data sources, and infer and define schemas\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1603769326078_-1689195600","id":"20181126-092644_1457476546","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-19T03:47:39+0000","dateFinished":"2020-11-19T03:47:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:37655"},{"text":"%md\n# Setup\nThis exercise requires the `telco.account` table, check that it exists and containts data by running the `SELECT` statement below. If the table has not already been created and loaded you can do so by completing exercise `SETUP: O2-Telco_Hive`.\n\n**Important:** This assume you are familiar with DataFrames, you can practice by completing exercise `01 - DataFrames`.","user":"devuser","dateUpdated":"2020-11-30T04:07:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>This exercise requires the <code>telco.account</code> table, check that it exists and containts data by running the <code>SELECT</code> statement below. If the table has not already been created and loaded you can do so by completing exercise <code>SETUP: O2-Telco_Hive</code>.</p>\n<p><strong>Important:</strong> This assume you are familiar with DataFrames, you can practice by completing exercise <code>01 - DataFrames</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326079_-848514974","id":"20181201-044336_178705192","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-30T04:07:23+0000","dateFinished":"2020-11-30T04:07:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37656"},{"title":"Query the Hive table","text":"%jdbc(hive)\nSELECT * FROM telco.account LIMIT 5;","user":"devuser","dateUpdated":"2020-12-10T00:11:12+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"account.acct_num":"string","account.acct_create_dt":"string","account.acct_close_dt":"string","account.first_name":"string","account.last_name":"string","account.address":"string","account.city":"string","account.state":"string","account.zipcode":"string","account.phone_number":"string","account.created":"string","account.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"account.acct_num\taccount.acct_create_dt\taccount.acct_close_dt\taccount.first_name\taccount.last_name\taccount.address\taccount.city\taccount.state\taccount.zipcode\taccount.phone_number\taccount.created\taccount.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1603769326079_-494822107","id":"20200602-184825_95278537","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:11:12+0000","dateFinished":"2020-12-10T00:11:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37657"},{"text":"%sh\nhdfs dfs -rm -r -skipTrash /user/zeppelin/account_zip94913\n\nhdfs dfs -rm -r -skipTrash /user/zeppelin/device.json\nhdfs dfs -put /var/data/telco/device.json /user/zeppelin/\n\nhdfs dfs -rm -r -skipTrash /user/zeppelin/device_parquet","user":"devuser","dateUpdated":"2020-12-10T00:33:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /user/zeppelin/account_zip94913\nDeleted /user/zeppelin/device.json\n"}]},"apps":[],"jobName":"paragraph_1606710124109_-2027202143","id":"20201130-042204_487011500","dateCreated":"2020-11-30T04:22:04+0000","dateStarted":"2020-12-10T00:11:31+0000","dateFinished":"2020-12-10T00:11:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37658"},{"text":"%md\n# Lab\n","user":"devuser","dateUpdated":"2020-12-10T00:11:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1603769326079_1485409001","id":"20181126-093358_358613711","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:11:21+0000","dateFinished":"2020-12-10T00:11:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37659"},{"text":"%md\n#### Create a DataFrame Based on a Hive Table","user":"devuser","dateUpdated":"2020-11-09T02:50:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Create a DataFrame Based on a Hive Table</h4>\n"}]},"apps":[],"jobName":"paragraph_1603769326080_-574102718","id":"20200427-233238_1449553071","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-09T02:50:51+0000","dateFinished":"2020-11-09T02:50:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37660"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.account;","user":"devuser","dateUpdated":"2020-12-10T00:24:04+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1603769326080_11469198","id":"20200521-194130_648124297","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:04+0000","dateFinished":"2020-12-10T00:24:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37661"},{"title":"","text":"%md\nThis exercise uses a DataFrame based on the `account` table in the `telco` Hive database. You can review \nthe schema using the `jdbc` interpreter to access Hive.","user":"devuser","dateUpdated":"2020-11-09T02:51:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This exercise uses a DataFrame based on the <code>account</code> table in the <code>telco</code> Hive database. You can review\n<br  />the schema using the <code>jdbc</code> interpreter to access Hive.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326080_855088953","id":"20200424-193917_1691179324","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-09T02:51:24+0000","dateFinished":"2020-11-09T02:51:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37662"},{"title":"2 - Create a new DataFrame","text":"%pyspark\naccountDF = spark.read.table(\"telco.account\")","user":"devuser","dateUpdated":"2020-12-10T00:24:09+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326080_-1333837721","id":"20200424-194301_536696070","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:10+0000","dateFinished":"2020-12-10T00:24:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37663"},{"title":"","text":"%md\nCreate a new DataFrame using the Hive `telco.account` table.","user":"devuser","dateUpdated":"2020-11-09T02:51:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Hive <code>telco.account</code> table.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326080_-993588526","id":"20200424-194206_1083041321","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-09T02:51:59+0000","dateFinished":"2020-11-09T02:51:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37664"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%pyspark\naccountDF.printSchema()\naccountDF.show(5)","user":"devuser","dateUpdated":"2020-12-10T00:24:12+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|acct_num|     acct_create_dt|acct_close_dt|first_name|last_name|             address|         city|state|zipcode|phone_number|            created|           modified|\n+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\n|       1|2008-10-23 16:05:05|         null|    Donald|   Becton|2275 Washburn Street|      Oakland|   CA|  94660|  5100032418|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       2|2008-11-12 03:00:01|         null|     Donna|    Jones| 3885 Elliott Street|San Francisco|   CA|  94171|  4150835799|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       3|2008-12-21 09:19:50|         null|    Dorthy| Chalmers|    4073 Whaley Lane|    San Mateo|   CA|  94479|  6506877757|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       4|2008-11-28 00:08:09|         null|     Leila|  Spencer|    1447 Ross Street|    San Mateo|   CA|  94444|  6503198619|2014-03-18 13:29:47|2014-03-18 13:29:47|\n|       5|2008-11-15 23:06:06|         null|     Anita| Laughlin|    2767 Hill Street|     Richmond|   CA|  94872|  5107754354|2014-03-18 13:29:47|2014-03-18 13:29:47|\n+--------+-------------------+-------------+----------+---------+--------------------+-------------+-----+-------+------------+-------------------+-------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1603769326081_24617240","id":"20200522-201531_943828593","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:12+0000","dateFinished":"2020-12-10T00:24:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37665"},{"title":"","text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with \nthat of the Hive table.","user":"devuser","dateUpdated":"2020-11-09T02:52:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first few rows of the DataFrame, and note that the schema aligns with\n<br  />that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326081_1014122889","id":"20200424-194700_1657078805","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-09T02:52:39+0000","dateFinished":"2020-11-09T02:52:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37666"},{"text":"%md\n#### Write a csv file to HDFS","user":"devuser","dateUpdated":"2020-11-30T04:28:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Write a csv file to HDFS</h4>\n"}]},"apps":[],"jobName":"paragraph_1606710485302_380614736","id":"20201130-042805_1206892987","dateCreated":"2020-11-30T04:28:05+0000","dateStarted":"2020-11-30T04:28:14+0000","dateFinished":"2020-11-30T04:28:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37667"},{"title":"4 - Create a new DataFrame based on a condition","text":"%pyspark\naccountDF.where(\"zipcode = 94913\"). \\\nwrite.option(\"header\",\"true\"). \\\ncsv(\"/user/zeppelin/account_zip94913\")","user":"devuser","dateUpdated":"2020-12-10T00:24:18+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326081_-691732493","id":"20200424-195112_623415846","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:18+0000","dateFinished":"2020-12-10T00:24:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37668"},{"title":"","text":"%md\nCreate a new DataFrame with rows from the account data where the zip code is 94913, and save \nthe result to CSV files in the `/user/zeppelin/account_zip94913` HDFS directory. You can do \nthis in a single command, as shown here, or with multiple commands.","user":"devuser","dateUpdated":"2020-11-30T04:17:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{"0":{"graph":{"mode":"table","height":60,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the account data where the zip code is 94913, and save\n<br  />the result to CSV files in the <code>/user/zeppelin/account_zip94913</code> HDFS directory. You can do\n<br  />this in a single command, as shown here, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326081_-352565584","id":"20200424-194924_522335325","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-30T04:16:54+0000","dateFinished":"2020-11-30T04:16:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37669"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\nhdfs dfs -ls /user/zeppelin/account_zip94913","user":"devuser","dateUpdated":"2020-12-10T00:24:22+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-12-10 00:24 /user/zeppelin/account_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-12-10 00:24 /user/zeppelin/account_zip94913/part-00000-04ba8d6b-06cd-4376-bb0c-652908a48c63-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-12-10 00:24 /user/zeppelin/account_zip94913/part-00001-04ba8d6b-06cd-4376-bb0c-652908a48c63-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-12-10 00:24 /user/zeppelin/account_zip94913/part-00002-04ba8d6b-06cd-4376-bb0c-652908a48c63-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-12-10 00:24 /user/zeppelin/account_zip94913/part-00003-04ba8d6b-06cd-4376-bb0c-652908a48c63-c000.csv\n"}]},"apps":[],"jobName":"paragraph_1603769326081_-951498523","id":"20200522-201740_1472082247","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:22+0000","dateFinished":"2020-12-10T00:24:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37670"},{"title":"6 - Inspect the contents of one of the part files","text":"%sh\nhdfs dfs -head /user/zeppelin/account_zip94913/part-00000-673e4ce6-19d8-4787-a635-7e586cab4354-c000.csv","user":"devuser","dateUpdated":"2020-12-10T00:27:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"acct_num,acct_create_dt,acct_close_dt,first_name,last_name,address,city,state,zipcode,phone_number,created,modified\n19,2008-11-13T23:40:53.000Z,2014-02-26T18:44:26.000Z,Leona,Bray,364 Romrog Way,Santa Rosa,CA,94913,7070013038,2014-03-18T13:29:47.000Z,2014-03-18T13:29:47.000Z\n3156,2009-07-03T11:04:35.000Z,,Stephen,Bisson,1116 Daylene Drive,Santa Rosa,CA,94913,7075647641,2014-03-18T13:29:53.000Z,2014-03-18T13:29:53.000Z\n5536,2010-05-15T14:09:32.000Z,,Steven,Jones,4818 Copperhead Road,Santa Rosa,CA,94913,7079211079,2014-03-18T13:29:57.000Z,2014-03-18T13:29:57.000Z\n6320,2010-09-22T01:35:25.000Z,,Victor,Cheatham,2564 Center Avenue,Santa Rosa,CA,94913,7078711417,2014-03-18T13:29:59.000Z,2014-03-18T13:29:59.000Z\n19938,2011-06-22T02:18:05.000Z,2014-01-27T22:10:49.000Z,David,Reeves,2971 Biddie Lane,Santa Rosa,CA,94913,7071048775,2014-03-18T13:30:23.000Z,2014-03-18T13:30:23.000Z\n"}]},"apps":[],"jobName":"paragraph_1607396604336_1023597832","id":"20201208-030324_1156500396","dateCreated":"2020-12-08T03:03:24+0000","dateStarted":"2020-12-10T00:14:34+0000","dateFinished":"2020-12-10T00:14:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37671"},{"title":"","text":"%md\nCopy and paste one of the files from the `account_zip94913` directory into the `hdfs dfs -head` command to inspect the data.  \n\nConfirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"devuser","dateUpdated":"2020-12-10T00:19:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Copy and paste one of the files from the <code>account_zip94913</code> directory into the <code>hdfs dfs -head</code> command to inspect the data.</p>\n<p>Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326081_1196064960","id":"20200424-195251_938833734","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:19:22+0000","dateFinished":"2020-12-10T00:19:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37672"},{"text":"%md\n#### Manage the data types of a DataFrame","user":"devuser","dateUpdated":"2020-11-30T04:40:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Manage the data types of a DataFrame</h4>\n"}]},"apps":[],"jobName":"paragraph_1606711200876_-1135384012","id":"20201130-044000_1662195485","dateCreated":"2020-11-30T04:40:00+0000","dateStarted":"2020-11-30T04:40:27+0000","dateFinished":"2020-11-30T04:40:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37673"},{"title":"7 - Create a DataFrame from a CSV","text":"%pyspark\ntest1DF = spark.read.option(\"header\",\"true\").csv(\"/user/zeppelin/account_zip94913\")\ntest1DF.printSchema()","user":"devuser","dateUpdated":"2020-12-10T00:28:27+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326082_493277137","id":"20200522-201757_1455279920","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:36+0000","dateFinished":"2020-12-10T00:24:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37674"},{"title":"8 - Compare the data types when using the inferSchema option","text":"%pyspark\ntest2DF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/account_zip94913\")\ntest2DF.printSchema()","user":"devuser","dateUpdated":"2020-12-10T00:28:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1606711364129_-427931765","id":"20201130-044244_807704475","dateCreated":"2020-11-30T04:42:44+0000","dateStarted":"2020-12-10T00:24:40+0000","dateFinished":"2020-12-10T00:24:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37675"},{"text":"%md\n#### Define a Schema for a DataFrame","user":"devuser","dateUpdated":"2020-11-09T03:20:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Define a Schema for a DataFrame</h4>\n"}]},"apps":[],"jobName":"paragraph_1603769326082_1455900036","id":"20200427-234619_2069213136","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-09T03:20:20+0000","dateFinished":"2020-11-09T03:20:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37676"},{"title":"9 - Review the data file","text":"%sh\nhdfs dfs -head /user/zeppelin/device.json","user":"devuser","dateUpdated":"2020-12-10T00:29:01+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1603769326082_696402084","id":"20200430-003645_261089763","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:45+0000","dateFinished":"2020-12-10T00:24:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37677"},{"title":"10 - Create a DataFrame based on the device.json file","text":"%pyspark\ndevDF = spark.read.json(\"/user/zeppelin/device.json\")","user":"devuser","dateUpdated":"2020-12-10T00:29:41+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326083_-535824353","id":"20200424-201433_776130777","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:51+0000","dateFinished":"2020-12-10T00:24:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37678"},{"title":"","text":"%md\nCreate a new DataFrame based on the `device.json` file. (This command could take several seconds while \nit infers the schema.)","user":"devuser","dateUpdated":"2020-11-19T05:39:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the <code>device.json</code> file. (This command could take several seconds while\n<br  />it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326083_-1008993484","id":"20200424-201503_1618492364","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-19T05:39:21+0000","dateFinished":"2020-11-19T05:39:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37679"},{"title":"11 - Review the schema of the devDF DataFrame","text":"%pyspark\ndevDF.printSchema()","user":"devuser","dateUpdated":"2020-12-10T00:29:54+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326083_1522964804","id":"20200522-201923_1097540138","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:55+0000","dateFinished":"2020-12-10T00:24:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37680"},{"title":"","text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. \nIn particular, note that the `release_dt` column is of type `string`, whereas the data in the column actually \nrepresents a timestamp.","user":"devuser","dateUpdated":"2020-11-08T21:41:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that Spark inferred from the JSON file.\n<br  />In particular, note that the <code>release_dt</code> column is of type <code>string</code>, whereas the data in the column actually\n<br  />represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326083_-853595149","id":"20200424-201330_1854493694","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:41:53+0000","dateFinished":"2020-11-08T21:41:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37681"},{"title":"12 - Import the column types for the DataFrame","text":"%pyspark\nfrom pyspark.sql.types import *","user":"devuser","dateUpdated":"2020-12-10T00:30:07+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326084_715529498","id":"20200424-201240_160502927","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:24:59+0000","dateFinished":"2020-12-10T00:24:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37682"},{"title":"","text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package \nwith the definitions of necessary classes and types.","user":"devuser","dateUpdated":"2020-11-08T21:42:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start by importing the package\n<br  />with the definitions of necessary classes and types.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326083_512199003","id":"20200424-201310_819844900","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:42:35+0000","dateFinished":"2020-11-08T21:42:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37683"},{"title":"13 - Represent the column definitions with StructField objects","text":"%pyspark\ndevColumn = [\nStructField(\"devnum\",LongType()),\nStructField(\"make\",StringType()),\nStructField(\"model\",StringType()),\nStructField(\"release_dt\",TimestampType()),\nStructField(\"dev_type\",StringType())]","user":"devuser","dateUpdated":"2020-12-10T00:30:12+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326084_-1476550563","id":"20200424-201128_583503462","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:25:01+0000","dateFinished":"2020-12-10T00:25:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37684"},{"title":"","text":"%md\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column \nshould be a timestamp.","user":"devuser","dateUpdated":"2020-11-08T21:43:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, create a collection of <code>StructField</code> objects, which represent column definitions. The <code>release_dt</code> column\n<br  />should be a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326084_218891189","id":"20200424-201153_1739325405","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:43:13+0000","dateFinished":"2020-11-08T21:43:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37685"},{"title":"14 - Create a schema using the column definition","text":"%pyspark\ndevSchema = StructType(devColumn)","user":"devuser","dateUpdated":"2020-12-10T00:30:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326084_-1776358472","id":"20200424-200751_1167632895","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:25:06+0000","dateFinished":"2020-12-10T00:25:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37686"},{"title":"","text":"%md\nCreate a schema (a `StructType` object) using the column definition list.","user":"devuser","dateUpdated":"2020-11-08T21:43:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326084_104843203","id":"20200424-201020_2045594003","dateCreated":"2020-10-27T03:28:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37687"},{"title":"15 - Recreate the devDF DataFrame using the new schema","text":"%pyspark\ndevDF = spark.read.schema(devSchema).json(\"/user/zeppelin/device.json\")","user":"devuser","dateUpdated":"2020-12-10T00:30:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326085_1538883105","id":"20200424-200134_290658193","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:25:08+0000","dateFinished":"2020-12-10T00:25:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37688"},{"title":"16 - Confirm that the release_dt column is now of type timestamp","text":"%pyspark\ndevDF.printSchema()\ndevDF.show()","user":"devuser","dateUpdated":"2020-12-10T00:30:59+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1603769326085_-1916658301","id":"20200522-202136_1188742309","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:25:10+0000","dateFinished":"2020-12-10T00:25:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37689"},{"title":"","text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now `timestamp`.","user":"devuser","dateUpdated":"2020-11-08T21:45:10+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the <code>release_dt</code> column type is now <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326085_44147406","id":"20200424-200027_404284113","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:45:10+0000","dateFinished":"2020-11-08T21:45:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37690"},{"text":"%md\n#### Create DataFrames from Parquet files","user":"devuser","dateUpdated":"2020-11-30T04:53:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Create DataFrames from Parquet files</h4>\n"}]},"apps":[],"jobName":"paragraph_1606711963551_-1872792394","id":"20201130-045243_2085434388","dateCreated":"2020-11-30T04:52:43+0000","dateStarted":"2020-11-30T04:52:54+0000","dateFinished":"2020-11-30T04:52:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37691"},{"title":"17 - Save the DataFrame in Parquet format","text":"%pyspark\ndevDF.write.parquet(\"/user/zeppelin/device_parquet\") ","user":"devuser","dateUpdated":"2020-12-10T00:34:20+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326086_827109350","id":"20200522-202142_98727568","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:25:39+0000","dateFinished":"2020-12-10T00:25:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37692"},{"title":"","text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the \nschema. Save the Parquet data files into an HDFS directory called `/user/zeppelin/devices_parquet`.","user":"devuser","dateUpdated":"2020-11-08T21:45:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the\n<br  />schema. Save the Parquet data files into an HDFS directory called <code>/user/zeppelin/devices_parquet</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326085_1777235784","id":"20200424-195942_1783798900","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:45:44+0000","dateFinished":"2020-11-08T21:45:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37693"},{"title":"18 - View the schema of the Parquet file (Note tool is not loaded","text":"%sh\nhdfs dfs -get /user/zeppelin/device_parquet /tmp/\nparquet-tools schema /tmp/device_parquet","user":"devuser","dateUpdated":"2020-12-10T00:34:56+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"get: `/tmp/device_parquet/_SUCCESS': File exists\nbash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1603769326086_1573950134","id":"20200522-202202_1659863288","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-12-10T00:34:53+0000","dateFinished":"2020-12-10T00:34:54+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:37694"},{"title":"","text":"%md\n*Optional:* Use `parquet-tools` to view the schema of the saved files. First download the HDFS directory \n(or an individual file), then run the command.\n\n```\n    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in \nParquet.\n\nFor more information about `parquet-tools`, run `parquet-tools --help`.","user":"devuser","dateUpdated":"2020-11-08T21:46:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Use <code>parquet-tools</code> to view the schema of the saved files. First download the HDFS directory\n<br  />(or an individual file), then run the command.</p>\n<pre><code>    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark denotes a timestamp type in\n<br  />Parquet.</p>\n<p>For more information about <code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326086_-1145669107","id":"20200424-195815_1058807478","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:46:26+0000","dateFinished":"2020-11-08T21:46:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37695"},{"title":"19 - Create a DataFrame from the Parquet files","text":"%pyspark\nspark.read.parquet(\"/user/zeppelin/device_parquet\").printSchema()","user":"devuser","dateUpdated":"2020-12-10T00:35:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1603769326086_1540131937","id":"20200522-202219_851657177","dateCreated":"2020-10-27T03:28:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37696"},{"title":"","text":"%md\nCreate a new DataFrame using the Parquet files you saved in `devices_parquet` and view its schema. Note that \nSpark is able to correctly infer the timestamp type of the `release_dt` column from Parquet's embedded schema.","user":"devuser","dateUpdated":"2020-11-08T21:47:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in <code>devices_parquet</code> and view its schema. Note that\n<br  />Spark is able to correctly infer the timestamp type of the <code>release_dt</code> column from Parquet's embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1603769326086_1339947089","id":"20200424-195648_983273468","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:47:08+0000","dateFinished":"2020-11-08T21:47:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37697"},{"text":"%md\n# Reset","user":"devuser","dateUpdated":"2020-11-30T04:30:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Reset</h1>\n"}]},"apps":[],"jobName":"paragraph_1606710639698_1023207820","id":"20201130-043039_1240308842","dateCreated":"2020-11-30T04:30:39+0000","dateStarted":"2020-11-30T04:30:47+0000","dateFinished":"2020-11-30T04:30:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37698"},{"title":"Ensure Hive table telco.account is accessible","text":"%jdbc(hive)\nSELECT * FROM telco.account LIMIT 5;","user":"devuser","dateUpdated":"2020-12-10T00:23:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"account.acct_num":"string","account.acct_create_dt":"string","account.acct_close_dt":"string","account.first_name":"string","account.last_name":"string","account.address":"string","account.city":"string","account.state":"string","account.zipcode":"string","account.phone_number":"string","account.created":"string","account.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"account.acct_num\taccount.acct_create_dt\taccount.acct_close_dt\taccount.first_name\taccount.last_name\taccount.address\taccount.city\taccount.state\taccount.zipcode\taccount.phone_number\taccount.created\taccount.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1606712739247_1192464722","id":"20201130-050539_1592985772","dateCreated":"2020-11-30T05:05:39+0000","dateStarted":"2020-12-10T00:23:44+0000","dateFinished":"2020-12-10T00:23:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37699"},{"text":"%sh\nhdfs dfs -rm -r -skipTrash /user/zeppelin/account_zip94913\n\nhdfs dfs -rm -r -skipTrash /user/zeppelin/device.json\nhdfs dfs -put /var/data/telco/device.json /user/zeppelin/\n\nhdfs dfs -rm -r -skipTrash /user/zeppelin/device_parquet","user":"devuser","dateUpdated":"2020-12-10T00:33:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /user/zeppelin/account_zip94913\nDeleted /user/zeppelin/device.json\n"}]},"apps":[],"jobName":"paragraph_1606710649422_854040166","id":"20201130-043049_680050381","dateCreated":"2020-11-30T04:30:49+0000","dateStarted":"2020-12-10T00:23:50+0000","dateFinished":"2020-12-10T00:23:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37700"},{"text":"%md\n# Exercise","user":"devuser","dateUpdated":"2020-11-30T04:54:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Exercise</h1>\n"}]},"apps":[],"jobName":"paragraph_1603769326087_-276480721","id":"20181018-125200_1133281582","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-30T04:54:39+0000","dateFinished":"2020-11-30T04:54:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37701"},{"text":"%md\n#### Create a DataFrame Based on a Hive Table","user":"devuser","dateUpdated":"2020-11-19T05:31:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Create a DataFrame Based on a Hive Table</h4>\n"}]},"apps":[],"jobName":"paragraph_1603769326087_-1579106900","id":"20200428-225106_184286167","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-19T05:31:42+0000","dateFinished":"2020-11-19T05:31:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37702"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.account;","user":"devuser","dateUpdated":"2020-11-20T18:22:50+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1603769326087_-2091183576","id":"20200428-225139_1888690996","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:22:50+0000","dateFinished":"2020-11-20T18:22:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37703"},{"title":"2 - Create a new DataFrame","text":"%pyspark\naccountDF = spark.read.table(\"telco.account\")","user":"devuser","dateUpdated":"2020-11-20T18:25:02+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326087_385734907","id":"20200428-225540_857086049","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:25:02+0000","dateFinished":"2020-11-20T18:25:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37704"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-11-30T04:15:12+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326087_-1237938331","id":"20200428-225533_1026098327","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:25:13+0000","dateFinished":"2020-11-20T18:25:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37705"},{"text":"%md\n#### Write a csv file to HDFS\n","user":"devuser","dateUpdated":"2020-11-19T05:37:45+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Write a csv file to HDFS</h4>\n"}]},"apps":[],"jobName":"paragraph_1605763922693_-1181277345","id":"20201119-053202_1095220351","dateCreated":"2020-11-19T05:32:02+0000","dateStarted":"2020-11-19T05:37:45+0000","dateFinished":"2020-11-19T05:37:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37706"},{"title":"4 - Create a new DataFrame based on a condition","text":"%pyspark\n# Create a DataFrame of all accounts where zipcode = 94913 and write the result as a csv file to /user/zeppelin/account_zip94913 in HDFS\n","user":"devuser","dateUpdated":"2020-11-30T04:30:19+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326088_1218106672","id":"20200428-225650_1329297880","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:30:16+0000","dateFinished":"2020-11-20T18:30:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37707"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\n","user":"devuser","dateUpdated":"2020-11-30T04:30:19+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-11-20 18:30 /user/zeppelin/account_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-11-20 18:30 /user/zeppelin/account_zip94913/part-00000-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-11-20 18:30 /user/zeppelin/account_zip94913/part-00001-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-11-20 18:30 /user/zeppelin/account_zip94913/part-00002-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-11-20 18:30 /user/zeppelin/account_zip94913/part-00003-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv\n"}]},"apps":[],"jobName":"paragraph_1603769326088_-1429410707","id":"20200428-225649_867963440","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:30:52+0000","dateFinished":"2020-11-20T18:30:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37708"},{"title":"6 - Inspect the content of one of the part files","text":"%sh\nhdfs dfs -head /user/zeppelin/account_zip94913/part-00001-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv","user":"devuser","dateUpdated":"2020-12-10T00:28:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"head: `/user/zeppelin/account_zip94913/part-00001-d22d8e20-6a41-4a62-8ecc-f1db3859617a-c000.csv': No such file or directory\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1605763523901_426731105","id":"20201119-052523_1620019586","dateCreated":"2020-11-19T05:25:23+0000","dateStarted":"2020-11-30T04:41:11+0000","dateFinished":"2020-11-30T04:41:13+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:37709"},{"text":"%md\n#### Manage the data types of a DataFrame\n","user":"devuser","dateUpdated":"2020-11-30T04:40:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Manage the data types of a DataFrame</h4>\n"}]},"apps":[],"jobName":"paragraph_1604892384271_821111651","id":"20201109-032624_455236482","dateCreated":"2020-11-09T03:26:24+0000","dateStarted":"2020-11-30T04:40:44+0000","dateFinished":"2020-11-30T04:40:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37710"},{"title":"7 - Create a DataFrame from a CSV","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:28:23+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326088_483392095","id":"20200428-225649_314698363","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:33:28+0000","dateFinished":"2020-11-20T18:33:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37711"},{"title":"8 - Compare the data types when using the inferSchema option","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:28:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1605763645348_-104092118","id":"20201119-052725_1706323516","dateCreated":"2020-11-19T05:27:25+0000","dateStarted":"2020-11-20T18:35:26+0000","dateFinished":"2020-11-20T18:35:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37712"},{"text":"%md\n#### Define a Schema for a DataFrame","user":"devuser","dateUpdated":"2020-11-30T04:38:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Define a Schema for a DataFrame</h4>\n"}]},"apps":[],"jobName":"paragraph_1603769326088_2093800798","id":"20200428-225648_2130864594","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-30T04:37:56+0000","dateFinished":"2020-11-30T04:37:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37713"},{"title":"9 - Review the data file","text":"%sh\n","user":"devuser","dateUpdated":"2020-12-10T00:28:49+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1603769326089_164148299","id":"20200428-225648_313296430","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:38:04+0000","dateFinished":"2020-11-20T18:38:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37714"},{"title":"10 - Create a DataFrame based on the device.json file","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:29:20+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326089_297775892","id":"20200428-225648_1886643259","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:38:22+0000","dateFinished":"2020-11-20T18:38:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37715"},{"title":"11 - Review the schema of the devDF DataFrame","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:29:57+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326089_-1183756440","id":"20200428-225647_1646122487","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:38:35+0000","dateFinished":"2020-11-20T18:38:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37716"},{"title":"12 - Import the column types for the DataFrame","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:30:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326089_1674943601","id":"20200428-225647_1682536791","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:39:38+0000","dateFinished":"2020-11-20T18:39:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37717"},{"title":"13 - Represent the column definitions with StructField objects","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:30:22+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326089_-1736292376","id":"20200428-225646_614334074","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:41:17+0000","dateFinished":"2020-11-20T18:41:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37718"},{"title":"14 - Create a schema using the column definition","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:30:27+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326090_192998153","id":"20200428-225645_364347750","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:42:04+0000","dateFinished":"2020-11-20T18:42:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37719"},{"title":"15 - Recreate the devDF DataFrame using the new schema","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:30:48+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326090_1948582014","id":"20200428-225644_1723966751","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:42:20+0000","dateFinished":"2020-11-20T18:42:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37720"},{"title":"16 - Confirm that the release_dt column is now of type timestamp","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:30:54+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1603769326090_1443009287","id":"20200428-230856_1582268955","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:42:24+0000","dateFinished":"2020-11-20T18:42:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37721"},{"text":"%md\n#### Create DataFrames from Parquet files\n","user":"devuser","dateUpdated":"2020-11-19T05:37:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Create DataFrames from Parquet files</h4>\n"}]},"apps":[],"jobName":"paragraph_1605764209719_176787882","id":"20201119-053649_569490586","dateCreated":"2020-11-19T05:36:49+0000","dateStarted":"2020-11-19T05:37:10+0000","dateFinished":"2020-11-19T05:37:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37722"},{"title":"17 - Save the DataFrame in Parquet format","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:34:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603769326090_-2029205613","id":"20200428-230854_1798105788","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:43:36+0000","dateFinished":"2020-11-20T18:43:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37723"},{"title":"18 - View the schema of the Parquet file (Note tool is not loaded)","text":"%sh\n","user":"devuser","dateUpdated":"2020-12-10T00:34:29+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"get: `/tmp/device_parquet/_SUCCESS': File exists\nbash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1603769326091_768218130","id":"20200428-230853_1175884827","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-19T05:19:33+0000","dateFinished":"2020-11-19T05:19:35+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:37724"},{"title":"19 - Create a DataFrame from the Parquet files","text":"%pyspark\n","user":"devuser","dateUpdated":"2020-12-10T00:35:09+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1603769326091_-1865739729","id":"20200428-230853_1544838296","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-20T18:44:10+0000","dateFinished":"2020-11-20T18:44:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37725"},{"text":"%md\n# Result\n**You have now:** managed DataFrames and Schemas.\n\n---","user":"devuser","dateUpdated":"2020-12-10T00:35:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong> managed DataFrames and Schemas.</p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1606712040473_-701478553","id":"20201130-045400_1110736940","dateCreated":"2020-11-30T04:54:00+0000","dateStarted":"2020-12-10T00:35:12+0000","dateFinished":"2020-12-10T00:35:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37726"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Cloudera Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Cloudera Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Cloudera Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Cloudera Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"devuser","dateUpdated":"2020-11-08T21:51:54+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Cloudera Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Cloudera Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Cloudera Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Cloudera Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1603769326091_-561811166","id":"20181126-133017_244739700","dateCreated":"2020-10-27T03:28:46+0000","dateStarted":"2020-11-08T21:51:54+0000","dateFinished":"2020-11-08T21:51:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:37727"}],"name":"DTRAN/02-Schemas","id":"2FPTQQGGK","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}