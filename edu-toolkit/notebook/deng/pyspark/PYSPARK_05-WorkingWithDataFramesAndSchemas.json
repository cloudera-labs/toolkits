{"paragraphs":[{"text":"%md\n# About\n**Lab:** Working with DataFrames and Schemas\n**Objective:** Practice working with structured account data and mobile device data using DataFrames\n**File locations:**\n- Data files (HDFS): /user/zeppelin/devices.json\n- Hive Tables: telco.accounts\n\n**Successful outcome:** Create and save DataFrames using different types of data sources, and infer and define schemas\n**Before you begin:** \n**Related lessons:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>About</h1>\n<p><strong>Lab:</strong> Working with DataFrames and Schemas\n<br  /><strong>Objective:</strong> Practice working with structured account data and mobile device data using DataFrames\n<br  /><strong>File locations:</strong></p>\n<ul>\n<li>Data files (HDFS): /devsh_loudacre/devices.json</li>\n<li>Hive Tables: devsh.accounts</li>\n</ul>\n<p><strong>Successful outcome:</strong> Create and save DataFrames using different types of data sources, and infer and define schemas\n<br  /><strong>Before you begin:</strong>\n<br  /><strong>Related lessons:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591759205626_-728968894","id":"20181126-092644_1457476546","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:85798"},{"text":"%md\n# Setup\nThis exercise requires the `telco.accounts` table, check that it exists and containts data by running the `SELECT` statement below. If the table has not already been created and loaded you can do so by completing exercise `20 - Supplemental: Creating and Loading Hive Database`.\n\n**Important:** This assume you are familiar with DataFrames, you can practice by completing exercise `04 - Exploring DataFrames Using the Apache Spark Shell`.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Setup</h1>\n<p>This exercise requires the <code>telco.accounts</code> table, check that it exists and containts data by running the <code>SELECT</code> statement below. If the table has not already been created and loaded you can do so by completing exercise <code>20 - Supplemental: Creating and Loading Hive Database</code>.</p>\n<p><strong>Important:</strong> This assume you are familiar with DataFrames, you can practice by completing exercise 04 - Exploring DataFrames Using the Apache Spark Shell.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205627_661112594","id":"20181201-044336_178705192","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85799"},{"text":"%jdbc(hive)\nSELECT * FROM telco.accounts LIMIT 5;","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"accounts.acct_num":"string","accounts.acct_create_dt":"string","accounts.acct_close_dt":"string","accounts.first_name":"string","accounts.last_name":"string","accounts.address":"string","accounts.city":"string","accounts.state":"string","accounts.zipcode":"string","accounts.phone_number":"string","accounts.created":"string","accounts.modified":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"accounts.acct_num\taccounts.acct_create_dt\taccounts.acct_close_dt\taccounts.first_name\taccounts.last_name\taccounts.address\taccounts.city\taccounts.state\taccounts.zipcode\taccounts.phone_number\taccounts.created\taccounts.modified\n1\t2008-10-23 16:05:05.0\tnull\tDonald\tBecton\t2275 Washburn Street\tOakland\tCA\t94660\t5100032418\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n2\t2008-11-12 03:00:01.0\tnull\tDonna\tJones\t3885 Elliott Street\tSan Francisco\tCA\t94171\t4150835799\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n3\t2008-12-21 09:19:50.0\tnull\tDorthy\tChalmers\t4073 Whaley Lane\tSan Mateo\tCA\t94479\t6506877757\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n4\t2008-11-28 00:08:09.0\tnull\tLeila\tSpencer\t1447 Ross Street\tSan Mateo\tCA\t94444\t6503198619\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n5\t2008-11-15 23:06:06.0\tnull\tAnita\tLaughlin\t2767 Hill Street\tRichmond\tCA\t94872\t5107754354\t2014-03-18 13:29:47.0\t2014-03-18 13:29:47.0\n"}]},"apps":[],"jobName":"paragraph_1591759205627_-572709748","id":"20200602-184825_95278537","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85800"},{"text":"%md\n# Lab\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Lab</h1>\n"}]},"apps":[],"jobName":"paragraph_1591759205627_564310770","id":"20181126-093358_358613711","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85801"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591759205627_1499293543","id":"20200427-233238_1449553071","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85802"},{"title":"1 - Review the accounts table in the Hive database","text":"%md\nThis exercise uses a DataFrame based on the `accounts` table in the `telco` Hive database. You can review the schema using the `jdbc` interpreter to access Hive.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This exercise uses a DataFrame based on the <code>accounts</code> table in the <code>devsh</code> Hive\n<br  />database. You can review the schema using the Beeline SQL command line to access\n<br  />Hive.</p>\n<p>In a terminal session (not one that is running the Spark shell), enter the following\n<br  />command:</p>\n<pre><code>$ beeline -u jdbc:hive2://localhost:10000 -e \"DESCRIBE devsh.accounts\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1591759205627_1141818007","id":"20200424-193917_1691179324","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85803"},{"text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591759205628_-438821456","id":"20200521-194130_648124297","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85804"},{"title":"2 - Create a new DataFrame","text":"%md\nCreate a new DataFrame using the Hive `telco.accounts` table.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Hive <code>telco.accounts</code> table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205628_253635845","id":"20200424-194206_1083041321","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85805"},{"text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205628_-937600854","id":"20200424-194301_536696070","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85806"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%md\nPrint the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Print the schema and the first few rows of the DataFrame, and note that the schema aligns with that of the Hive table.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205628_-1557447969","id":"20200424-194700_1657078805","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85807"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205628_1462030518","id":"20200522-201531_943828593","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85808"},{"title":"4 - Create a new DataFrame based on a condition","text":"%md\nCreate a new DataFrame with rows from the accounts data where the zip code is 94913, and save the result to CSV files in the `/user/zeppelin/accounts_zip94913` HDFS directory. You can do this in a single command, as shown below, or with multiple commands.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":60,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame with rows from the accounts data where the zip\n<br  />code is 94913, and save the result to CSV files in the <code>/devsh_loudacre/accounts_zip94913</code> HDFS directory. You can do this in a single command, as shown below, or with multiple commands.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205628_1022549068","id":"20200424-194924_522335325","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85809"},{"text":"%pyspark\naccountsDF.where(\"zipcode = 94913\"). \\\nwrite.option(\"header\",\"true\"). \\\ncsv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205629_1719984274","id":"20200424-195112_623415846","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85810"},{"title":"5 - Confirm the operation was executed correctly","text":"%md\nUse `hdfs` to view the `/user/zeppelin/accounts_zip94913` directory in HDFS and the data in one of the saved files. Confirm that the CSV file includes a header line, and that only records for the selected zip code are included.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Use <code>hdfs</code> in a separate terminal window to view the\n<br  /><code>/devsh_loudacre/accounts_zip94913</code> directory in HDFS and the data in\n<br  />one of the saved files. Confirm that the CSV file includes a header line, and that only\n<br  />records for the selected zip code are included.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205629_2071341636","id":"20200424-195251_938833734","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85811"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205629_-2131913966","id":"20200522-201740_1472082247","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85812"},{"title":"6 - Create a DataFrame from a CSV","text":"%md\n*Optional:* Try creating a new DataFrame based on the CSV files you created above. Compare the schema of the original `accountsDF` and the new DataFrame. What's different? Try again, this time setting the `inferSchema` option to true and compare again.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> Try creating a new DataFrame based on the CSV files you created above.\n<br  />Compare the schema of the original <code>accountsDF</code> and the new DataFrame. What's\n<br  />different? Try again, this time setting the <code>inferSchema</code> option to true and\n<br  />compare again.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205629_-998399242","id":"20200424-195331_1399907429","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85813"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205629_-517321393","id":"20200522-201757_1455279920","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85814"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591759205629_1440136358","id":"20200427-234619_2069213136","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85815"},{"title":"7 - Review the data file","text":"%md\nIf you have not done so yet, review the data in the HDFS file `/user/zeppelin/devices.json`.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you have not done so yet, review the data in the HDFS file\n<br  /><code>/devsh_loudacre/devices.json</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205629_-539615855","id":"20200424-195454_1861585933","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85816"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205630_1418976071","id":"20200430-003645_261089763","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85817"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%md\nCreate a new DataFrame based on the `devices.json` file. (This command could take several seconds while it infers the schema.)","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame based on the <code>devices.json</code> file. (This command could take several seconds while it infers the schema.)</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205630_614613827","id":"20200424-201503_1618492364","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85818"},{"text":"%pyspark\ndevDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205630_-627468366","id":"20200424-201433_776130777","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85819"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%md\nView the schema of the `devDF` DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the `release_dt` column is of type `string`, whereas the data in the column actually represents a timestamp.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema of the <code>devDF</code> DataFrame. Note the column names and types that\n<br  />Spark inferred from the JSON file. In particular, note that the <code>release_dt</code> column\n<br  />is of type <code>string</code>, whereas the data in the column actually represents a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205630_894502305","id":"20200424-201330_1854493694","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85820"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205630_1764347121","id":"20200522-201923_1097540138","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85821"},{"title":"10 - Specify the column types of the DataFrame","text":"%md\nDefine a schema that correctly specifies the column types for this DataFrame. Start by importing the package with the definitions of necessary classes and types.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Define a schema that correctly specifies the column types for this DataFrame. Start\n<br  />by importing the package with the definitions of necessary classes and types.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205630_1643671659","id":"20200424-201310_819844900","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85822"},{"text":"%pyspark\nfrom pyspark.sql.types import *","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205630_1320692148","id":"20200424-201240_160502927","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85823"},{"title":"11 - Represent the column definitions with StructField objects","text":"%md\nNext, create a collection of `StructField` objects, which represent column definitions. The `release_dt` column should be a timestamp.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, create a collection of <code>StructField</code> objects, which represent column\n<br  />definitions. The <code>release_dt</code> column should be a timestamp.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205630_-237776366","id":"20200424-201153_1739325405","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85824"},{"text":"%pyspark\ndevColumns = [\nStructField(\"devnum\",LongType()),\nStructField(\"make\",StringType()),\nStructField(\"model\",StringType()),\nStructField(\"release_dt\",TimestampType()),\nStructField(\"dev_type\",StringType())]","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205631_-544943576","id":"20200424-201128_583503462","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85825"},{"title":"12 - Create a schema using the column definition list","text":"%md\nCreate a schema (a `StructType` object) using the column definition list.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a schema (a <code>StructType</code> object) using the column definition list.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205631_490432329","id":"20200424-201020_2045594003","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85826"},{"text":"%pyspark\ndevSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205631_940883597","id":"20200424-200751_1167632895","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85827"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%md\nRecreate the `devDF` DataFrame, this time using the new schema.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Recreate the <code>devDF</code> DataFrame, this time using the new schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205631_190533378","id":"20200424-200207_703556537","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85828"},{"text":"%pyspark\ndevDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205631_1178725988","id":"20200424-200134_290658193","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85829"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%md\nView the schema and data of the new DataFrame, and confirm that the `release_dt` column type is now `timestamp`.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>View the schema and data of the new DataFrame, and confirm that the\n<br  /><code>release_dt</code> column type is now <code>timestamp</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205631_829042070","id":"20200424-200027_404284113","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85830"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205632_-630375538","id":"20200522-202136_1188742309","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85831"},{"title":"15 - Save the DataFrame in Parquet format","text":"%md\nNow that the device data uses the correct schema, write the data in Parquet format, which automatically embeds the schema. Save the Parquet data files into an HDFS directory called `/user/zeppelin/devices_parquet`.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now that the device data uses the correct schema, write the data in Parquet format,\n<br  />which automatically embeds the schema. Save the Parquet data files into an HDFS\n<br  />directory called <code>/devsh_loudacre/devices_parquet</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205632_-698095427","id":"20200424-195942_1783798900","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85832"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205632_1801564129","id":"20200522-202142_98727568","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85833"},{"title":"16 - View the schema of the Parquet file","text":"%md\n*Optional:* Use `parquet-tools` to view the schema of the saved files. First download the HDFS directory (or an individual file), then run the command.\n\n```\n    $ hdfs dfs -get /user/zeppelin/devices_parquet /tmp/\n    $ parquet-tools schema /tmp/devices_parquet\n```\n\nNote that the type of the `release_dt` column is noted as `int96`; this is how Spark denotes a timestamp type in Parquet.\n\nFor more information about `parquet-tools`, run `parquet-tools --help`.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><em>Optional:</em> In a separate terminal window, use <code>parquet-tools</code> to view the schema\n<br  />of the saved files. First download the HDFS directory (or an individual file), then run\n<br  />the command.</p>\n<pre><code>$ hdfs dfs -get /devsh_loudacre/devices_parquet /tmp/\n$ parquet-tools schema /tmp/devices_parquet/\n</code></pre>\n<p>Note that the type of the <code>release_dt</code> column is noted as <code>int96</code>; this is how Spark\n<br  />denotes a timestamp type in Parquet.</p>\n<p>For more information about <code>parquet-tools</code>, run <code>parquet-tools --help</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205632_909894796","id":"20200424-195815_1058807478","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85834"},{"text":"%sh\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205632_2141053995","id":"20200522-202202_1659863288","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85835"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%md\nCreate a new DataFrame using the Parquet files you saved in `devices_parquet` and view its schema. Note that Spark is able to correctly infer the timestamp type of the `release_dt` column from Parquet's embedded schema.","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create a new DataFrame using the Parquet files you saved in <code>devices_parquet</code>\n<br  />and view its schema. Note that Spark is able to correctly infer the timestamp type\n<br  />of the <code>release_dt</code> column from Parquet's embedded schema.</p>\n"}]},"apps":[],"jobName":"paragraph_1591759205633_-1522374662","id":"20200424-195648_983273468","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85836"},{"text":"%pyspark\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205633_1822857502","id":"20200522-202219_851657177","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85837"},{"text":"%md\n# Result\n**You have now:** \n\n---","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Result</h1>\n<p><strong>You have now:</strong></p>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591759205633_133291592","id":"20181126-133507_1472573213","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85838"},{"text":"%md\n# Solution\n---","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Solution</h1>\n<hr />\n"}]},"apps":[],"jobName":"paragraph_1591759205633_-1280634624","id":"20181018-125200_1133281582","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85839"},{"text":"%md\n### Create a DataFrame Based on a Hive Table","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a DataFrame Based on a Hive Table</h3>\n"}]},"apps":[],"jobName":"paragraph_1591759205633_1035920890","id":"20200428-225106_184286167","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85840"},{"title":"1 - Review the accounts table in the Hive database","text":"%jdbc(hive)\nDESCRIBE telco.accounts;","user":"sysadmin","dateUpdated":"2020-06-10T17:55:23+0000","config":{"tableHide":false,"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"col_name":"string","data_type":"string","comment":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"col_name\tdata_type\tcomment\nacct_num\tint\t\nacct_create_dt\ttimestamp\t\nacct_close_dt\ttimestamp\t\nfirst_name\tvarchar(255)\t\nlast_name\tvarchar(255)\t\naddress\tvarchar(255)\t\ncity\tvarchar(255)\t\nstate\tvarchar(255)\t\nzipcode\tvarchar(255)\t\nphone_number\tvarchar(255)\t\ncreated\ttimestamp\t\nmodified\ttimestamp\t\n"}]},"apps":[],"jobName":"paragraph_1591759205633_-1273263073","id":"20200428-225139_1888690996","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:55:23+0000","dateFinished":"2020-06-10T17:55:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85841"},{"title":"2 - Create a new DataFrame","text":"%pyspark\naccountsDF = spark.read.table(\"telco.accounts\")","user":"sysadmin","dateUpdated":"2020-06-10T17:55:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205633_93921311","id":"20200428-225540_857086049","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:55:29+0000","dateFinished":"2020-06-10T17:55:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85842"},{"title":"3 - Compare the DataFrame and the Hive table","text":"%pyspark\naccountsDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T17:55:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591759205634_1682802453","id":"20200428-225533_1026098327","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:55:31+0000","dateFinished":"2020-06-10T17:55:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85843"},{"title":"4 - Create a new DataFrame based on a condition","text":"%pyspark\naccountsDF.where(\"zipcode = 94913\").write.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")","user":"sysadmin","dateUpdated":"2020-06-10T17:55:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205634_1500278353","id":"20200428-225650_1329297880","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:55:35+0000","dateFinished":"2020-06-10T17:55:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85844"},{"title":"5 - Confirm the operation was executed correctly","text":"%sh\nhdfs dfs -ls /user/zeppelin/accounts_zip94913\n\necho\nhdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>","user":"sysadmin","dateUpdated":"2020-06-10T17:55:40+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Found 5 items\n-rw-r--r--   3 zeppelin hdfs          0 2020-06-10 17:55 /user/zeppelin/accounts_zip94913/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        882 2020-06-10 17:55 /user/zeppelin/accounts_zip94913/part-00000-79950552-0902-4ae7-98c9-bea9f6660de9-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1026 2020-06-10 17:55 /user/zeppelin/accounts_zip94913/part-00001-79950552-0902-4ae7-98c9-bea9f6660de9-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1289 2020-06-10 17:55 /user/zeppelin/accounts_zip94913/part-00002-79950552-0902-4ae7-98c9-bea9f6660de9-c000.csv\n-rw-r--r--   3 zeppelin hdfs       1514 2020-06-10 17:55 /user/zeppelin/accounts_zip94913/part-00003-79950552-0902-4ae7-98c9-bea9f6660de9-c000.csv\n\nbash: -c: line 3: syntax error near unexpected token `newline'\nbash: -c: line 3: `hdfs dfs -head /user/zeppelin/accounts_zip94913/<part-file.csv>'\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1591759205634_-951540503","id":"20200428-225649_867963440","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:55:40+0000","dateFinished":"2020-06-10T17:55:42+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:85845"},{"title":"6 - Create a DataFrame from a CSV","text":"%pyspark\ntest1DF = spark.read.option(\"header\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\ntest2DF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/user/zeppelin/accounts_zip94913\")\n\ntest1DF.printSchema()\ntest2DF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T17:56:04+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- acct_num: string (nullable = true)\n |-- acct_create_dt: string (nullable = true)\n |-- acct_close_dt: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- created: string (nullable = true)\n |-- modified: string (nullable = true)\n\nroot\n |-- acct_num: integer (nullable = true)\n |-- acct_create_dt: timestamp (nullable = true)\n |-- acct_close_dt: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- phone_number: long (nullable = true)\n |-- created: timestamp (nullable = true)\n |-- modified: timestamp (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591759205634_1736507736","id":"20200428-225649_314698363","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:05+0000","dateFinished":"2020-06-10T17:56:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85846"},{"text":"%md\n### Define a Schema for a DataFrame","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Define a Schema for a DataFrame</h3>\n"}]},"apps":[],"jobName":"paragraph_1591759205634_122835777","id":"20200428-225648_2130864594","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85847"},{"title":"7 - Review the data file","text":"%sh\nhdfs dfs -head /user/zeppelin/devices.json","user":"sysadmin","dateUpdated":"2020-06-10T17:56:11+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{\"devnum\":1,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Sorrento\",\"model\":\"F00L\",\"dev_type\":\"phone\"}\n{\"devnum\":2,\"release_dt\":\"2010-04-19T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"2100\",\"dev_type\":\"phone\"}\n{\"devnum\":3,\"release_dt\":\"2011-02-18T00:00:00.000-08:00\",\"make\":\"MeeToo\",\"model\":\"3.0\",\"dev_type\":\"phone\"}\n{\"devnum\":4,\"release_dt\":\"2011-09-21T00:00:00.000-07:00\",\"make\":\"MeeToo\",\"model\":\"3.1\",\"dev_type\":\"phone\"}\n{\"devnum\":5,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"1\",\"dev_type\":\"phone\"}\n{\"devnum\":6,\"release_dt\":\"2011-11-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"3\",\"dev_type\":\"phone\"}\n{\"devnum\":7,\"release_dt\":\"2010-05-20T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"2\",\"dev_type\":\"phone\"}\n{\"devnum\":8,\"release_dt\":\"2013-07-02T00:00:00.000-07:00\",\"make\":\"iFruit\",\"model\":\"5\",\"dev_type\":\"phone\"}\n{\"devnum\":9,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"make\":\"Titanic\",\"model\":\"1000\",\"dev_type\":\"phone\"}\n{\"devnum\":10,\"release_dt\":\"2008-10-21T00:00:00.000-07:00\",\"mak"}]},"apps":[],"jobName":"paragraph_1591759205634_-844865074","id":"20200428-225648_313296430","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:11+0000","dateFinished":"2020-06-10T17:56:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85848"},{"title":"8 - Create a DataFrame based on the devices.json file","text":"%pyspark\ndevDF = spark.read.json(\"/user/zeppelin/devices.json\")","user":"sysadmin","dateUpdated":"2020-06-10T17:56:15+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205635_302228314","id":"20200428-225648_1886643259","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:15+0000","dateFinished":"2020-06-10T17:56:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85849"},{"title":"9 - Review the schema of the devDF DataFrame","text":"%pyspark\ndevDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T17:56:18+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dev_type: string (nullable = true)\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591759205635_2051326775","id":"20200428-225647_1646122487","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:19+0000","dateFinished":"2020-06-10T17:56:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85850"},{"title":"10 - Specify the column types of the DataFrame","text":"%pyspark\nfrom pyspark.sql.types import *","user":"sysadmin","dateUpdated":"2020-06-10T17:56:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205635_978560916","id":"20200428-225647_1682536791","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:24+0000","dateFinished":"2020-06-10T17:56:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85851"},{"title":"11 - Represent the column definitions with StructField objects","text":"%pyspark\ndevColumns = [\nStructField(\"devnum\",LongType()),\nStructField(\"make\",StringType()),\nStructField(\"model\",StringType()),\nStructField(\"release_dt\",TimestampType()),\nStructField(\"dev_type\",StringType())]","user":"sysadmin","dateUpdated":"2020-06-10T17:56:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205635_-2047396753","id":"20200428-225646_614334074","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:56:29+0000","dateFinished":"2020-06-10T17:56:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85852"},{"title":"12 - Create a schema using the column definitions","text":"%pyspark\ndevSchema = StructType(devColumns)","user":"sysadmin","dateUpdated":"2020-06-10T17:59:04+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205635_2126919262","id":"20200428-225645_364347750","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:59:04+0000","dateFinished":"2020-06-10T17:59:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85853"},{"title":"13 - Recreate the devDF DataFrame using the new schema","text":"%pyspark\ndevDF = spark.read.schema(devSchema).json(\"/user/zeppelin/devices.json\")\ndevDF.printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T17:59:07+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- devnum: long (nullable = true)\n |-- make: string (nullable = true)\n |-- model: string (nullable = true)\n |-- release_dt: timestamp (nullable = true)\n |-- dev_type: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1591759205636_226931673","id":"20200428-225644_1723966751","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:59:08+0000","dateFinished":"2020-06-10T17:59:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85854"},{"title":"14 - Confirm that the release_dt column is now of type timestamp","text":"%pyspark\ndevDF.show()","user":"sysadmin","dateUpdated":"2020-06-10T17:59:12+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+--------+--------------+-------------------+--------+\n|devnum|    make|         model|         release_dt|dev_type|\n+------+--------+--------------+-------------------+--------+\n|     1|Sorrento|          F00L|2008-10-21 07:00:00|   phone|\n|     2| Titanic|          2100|2010-04-19 07:00:00|   phone|\n|     3|  MeeToo|           3.0|2011-02-18 08:00:00|   phone|\n|     4|  MeeToo|           3.1|2011-09-21 07:00:00|   phone|\n|     5|  iFruit|             1|2008-10-21 07:00:00|   phone|\n|     6|  iFruit|             3|2011-11-02 07:00:00|   phone|\n|     7|  iFruit|             2|2010-05-20 07:00:00|   phone|\n|     8|  iFruit|             5|2013-07-02 07:00:00|   phone|\n|     9| Titanic|          1000|2008-10-21 07:00:00|   phone|\n|    10|  MeeToo|           1.0|2008-10-21 07:00:00|   phone|\n|    11|Sorrento|          F21L|2011-02-28 08:00:00|   phone|\n|    12|  iFruit|             4|2012-10-25 07:00:00|   phone|\n|    13|Sorrento|          F23L|2011-11-21 08:00:00|   phone|\n|    14| Titanic|          2200|2010-05-25 07:00:00|   phone|\n|    15|   Ronin|Novelty Note 1|2010-06-20 07:00:00|   phone|\n|    16| Titanic|          2500|2012-07-21 07:00:00|   phone|\n|    17|   Ronin|Novelty Note 3|2013-04-11 07:00:00|   phone|\n|    18|   Ronin|Novelty Note 2|2011-10-02 07:00:00|   phone|\n|    19|   Ronin|Novelty Note 4|2013-07-02 07:00:00|   phone|\n|    20|  iFruit|            3A|2012-07-21 07:00:00|   phone|\n+------+--------+--------------+-------------------+--------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1591759205636_1720328856","id":"20200428-230856_1582268955","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:59:12+0000","dateFinished":"2020-06-10T17:59:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85855"},{"title":"15 - Save the DataFrame in Parquet format","text":"%pyspark\ndevDF.write.parquet(\"/user/zeppelin/devices_parquet\") ","user":"sysadmin","dateUpdated":"2020-06-10T17:59:20+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1591759205636_110025777","id":"20200428-230854_1798105788","dateCreated":"2020-06-10T03:20:05+0000","dateStarted":"2020-06-10T17:59:20+0000","dateFinished":"2020-06-10T17:59:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:85856"},{"title":"16 - View the schema of the Parquet file","text":"%sh\nhdfs dfs -get /user/zeppelin/devices_parquet /tmp/\nparquet-tools schema /tmp/devices_parquet","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"bash: line 1: parquet-tools: command not found\n"},{"type":"TEXT","data":"ExitValue: 127"}]},"apps":[],"jobName":"paragraph_1591759205636_1183250521","id":"20200428-230853_1175884827","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85857"},{"title":"17 - Create a DataFrame from the Parquet files","text":"%pyspark\nspark.read.parquet(\"/user/zeppelin/devices_parquet\").printSchema()","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1591759205636_325301927","id":"20200428-230853_1544838296","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85858"},{"text":"%md\nWe hope you've enjoyed this lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html) - official Zeppelin documentation.\n","user":"sysadmin","dateUpdated":"2020-06-10T03:20:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/spark-overview/content/analyzing_data_with_apache_spark.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.1/zeppelin-overview/content/overview.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1591759205636_992852759","id":"20181126-133017_244739700","dateCreated":"2020-06-10T03:20:05+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85859"}],"name":"PYSPARK/05-WorkingWithDataFramesAndSchemas","id":"2FBN432MV","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}